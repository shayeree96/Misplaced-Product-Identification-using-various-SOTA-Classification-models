{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: nvidia-smi: command not found\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### IMPORTING NECESSARY MODULES #########\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torchvision import transforms, datasets, models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloading Scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist = 'training_list_4_departments.txt'\n",
    "validlist ='validation_list_4_departments.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mapping of Unique product Ids to Labels(0 to 31127 classes)\n",
    "# output = dictionary containing mapping of each upc to a label from (0 to 31127)  \n",
    "\n",
    "with open(trainlist, mode = 'r') as f:\n",
    "    \n",
    "    Y=[]\n",
    "    for line in f:\n",
    "        path, UPC = line[:-1].split(',')\n",
    "\n",
    "        Y.append(UPC)\n",
    "        \n",
    "prime_number_list = sorted(set(Y))\n",
    "\n",
    "prime_number_dict = { prime_number_list[i] :i for i in range(0, len(prime_number_list) ) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12828"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prime_number_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mydataset():    \n",
    "\n",
    "    def __init__(self, classification_list, prime_number_dict, name):\n",
    "\n",
    "        super(mydataset).__init__()\n",
    "        \n",
    "        self.X = []\n",
    "        self.Y = []\n",
    "        \n",
    "        with open(classification_list, mode = 'r') as f:\n",
    "            \n",
    "            for line in f:\n",
    "                path, Prime_Number = line[:-1].split(',')\n",
    "\n",
    "                self.X.append(path)\n",
    "                self.Y.append(prime_number_dict[Prime_Number])\n",
    "        \n",
    "\n",
    "        if name == 'valid':\n",
    "            self.transform = transforms.Compose([   \n",
    "#                                                     transforms.RandomResizedCrop(224),\n",
    "                                                    transforms.Resize(256),\n",
    "                                                    transforms.CenterCrop(224),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                    std=[0.229, 0.224, 0.225])\n",
    "                                                ])\n",
    "        else:\n",
    "            self.transform = transforms.Compose([   transforms.RandomResizedCrop(224),\n",
    "                                                transforms.RandomHorizontalFlip(),\n",
    "                                                transforms.ToTensor(),\n",
    "                                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                                    std=[0.229, 0.224, 0.225])\n",
    "                                                                                            ])\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        \n",
    "        image = self.X[index]        \n",
    "        label = float(self.Y[index])\n",
    "        \n",
    "        image = (Image.open(image))\n",
    "               \n",
    "        image = self.transform(image)\n",
    "        \n",
    "        return image, torch.as_tensor(label).long()\n",
    "        \n",
    "  \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### Train Dataloader #### \n",
    "train_dataset = mydataset(trainlist, prime_number_dict, name='train')          \n",
    "train_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 128, num_workers=16,pin_memory=True)\n",
    "\n",
    "\n",
    "#### Validation Dataloader #### \n",
    "validation_dataset = mydataset(validlist, prime_number_dict, name='valid')         \n",
    "validation_dataloader = data.DataLoader(validation_dataset, shuffle=False, batch_size = 128, num_workers=16,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESNET Architecture**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, planes * Bottleneck.expansion, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * Bottleneck.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, bottleneck=False):\n",
    "        super(ResNet, self).__init__()        \n",
    "        \n",
    "        \n",
    "        blocks ={18: BasicBlock, 34: BasicBlock, 50: Bottleneck, 101: Bottleneck, 152: Bottleneck, 200: Bottleneck}\n",
    "        layers ={18: [2, 2, 2, 2], 34: [3, 4, 6, 3], 50: [3, 4, 6, 3], 101: [3, 4, 23, 3], 152: [3, 8, 36, 3], 200: [3, 24, 36, 3]}\n",
    "        assert layers[depth], 'invalid detph for ResNet (depth should be one of 18, 34, 50, 101, 152, and 200)'\n",
    "\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(blocks[depth], 64, layers[depth][0])\n",
    "        self.layer2 = self._make_layer(blocks[depth], 128, layers[depth][1], stride=2)\n",
    "        self.layer3 = self._make_layer(blocks[depth], 256, layers[depth][2], stride=2)\n",
    "        self.layer4 = self._make_layer(blocks[depth], 512, layers[depth][3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7) \n",
    "        self.fc = nn.Linear(512 * blocks[depth].expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "    \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0)\n",
       "    (fc): Linear(in_features=2048, out_features=12828, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ResNet(depth = 50, num_classes = 12828)\n",
    "model = nn.DataParallel(model,device_ids=[6,7]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helper function for Cutmix\n",
    "https://arxiv.org/pdf/1905.04899v2.pdf**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = np.int(W * cut_rat)\n",
    "    cut_h = np.int(H * cut_rat)\n",
    "\n",
    "    # uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training Method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, test_loader,beta, cutmix_prob, epochs):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss = 0.0\n",
    "                \n",
    "        \n",
    "        for batch_num, (feats, target) in enumerate(data_loader):\n",
    "            feats, target = feats.to(device), target.to(device)\n",
    "            \n",
    "            \n",
    "            r = np.random.rand(1)\n",
    "            if beta > 0 and r < cutmix_prob:\n",
    "                # generate mixed sample\n",
    "                lam = np.random.beta(beta, beta)\n",
    "                rand_index = torch.randperm(feats.size()[0]).to(device)\n",
    "                target_a = target\n",
    "                target_b = target[rand_index]\n",
    "                bbx1, bby1, bbx2, bby2 = rand_bbox(feats.size(), lam)\n",
    "                feats[:, :, bbx1:bbx2, bby1:bby2] = feats[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
    "                # adjust lambda to exactly match pixel ratio\n",
    "                lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (feats.size()[-1] * feats.size()[-2]))\n",
    "                # compute output\n",
    "                output = model(feats)\n",
    "                loss = criterion(output, target_a) * lam + criterion(output, target_b) * (1. - lam)\n",
    "            else:\n",
    "                # compute output\n",
    "                output = model(feats)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "\n",
    "                                  \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            avg_loss += loss.item()\n",
    "\n",
    "#             if batch_num % 100 == 99:\n",
    "#                 print('loss', avg_loss/100)\n",
    "\n",
    "            del feats\n",
    "            del target\n",
    "            del loss\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        print('Epoch: ', epoch+1)\n",
    "\n",
    "        print('training loss = ', avg_loss/len(data_loader))\n",
    "        train_loss.append(avg_loss/len(data_loader))\n",
    "\n",
    "        ## Check performance on validation set after an Epoch\n",
    "        valid_loss, valid_acc = test_classify(model, test_loader)\n",
    "        print('Val Loss: {:.4f}\\tVal Accuracy: {:.4f}'.format(valid_loss, valid_acc))\n",
    "        v_loss.append(valid_loss)\n",
    "        v_acc.append(valid_acc)\n",
    "\n",
    "    \n",
    "        \n",
    "        \n",
    "        #########save model checkpoint #########\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'Training_Loss_List':train_loss,\n",
    "            'Validation_Loss_List':v_loss,\n",
    "            'Validation_Accuracy_List': v_acc,\n",
    "            'Epoch':epoch\n",
    "            'lr_scheduler': lr_scheduler.state_dict() \n",
    "\n",
    "            }, 'saved_model_checkpoints/cutmix_2gpu')\n",
    "\n",
    "\n",
    "def test_classify(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    accuracy = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_num, (feats, labels) in enumerate(test_loader):\n",
    "        feats, labels = feats.to(device), labels.to(device)\n",
    "        outputs = model(feats)\n",
    "        \n",
    "        _, pred_labels = torch.max(F.softmax(outputs, dim=1), 1)\n",
    "        pred_labels = pred_labels.view(-1)\n",
    "        \n",
    "        loss = criterion(outputs, labels.long())\n",
    "        \n",
    "        accuracy += torch.sum(torch.eq(pred_labels, labels)).item()\n",
    "        total += len(labels)\n",
    "        test_loss.extend([loss.item()]*feats.size()[0])\n",
    "        del feats\n",
    "        del labels\n",
    "\n",
    "    model.train()\n",
    "    return np.mean(test_loss), accuracy/total\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 30, gamma = 0.1)\n",
    "\n",
    "\n",
    "# Epochs\n",
    "num_Epochs = 120\n",
    "\n",
    "beta=1\n",
    "\n",
    "cutmix_prob = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss= []\n",
    "v_loss = []\n",
    "v_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 4.578716158866882\n",
      "loss 9.170636382102966\n",
      "loss 13.752643392086028\n",
      "loss 18.359760999679565\n",
      "loss 22.991463558673857\n",
      "loss 27.574965839385985\n",
      "loss 32.258319301605226\n",
      "loss 36.791191301345826\n",
      "loss 41.363995401859285\n",
      "loss 46.04338792800903\n",
      "loss 50.645687251091005\n",
      "loss 55.348742160797116\n",
      "loss 60.03664400100708\n",
      "loss 64.68959849834442\n",
      "loss 69.30490703344346\n",
      "loss 73.9310449051857\n",
      "loss 78.64238798618317\n",
      "loss 83.36281627178192\n",
      "loss 87.9769247865677\n",
      "loss 92.64733883857727\n",
      "loss 97.38966991901398\n",
      "loss 102.0286577630043\n",
      "loss 106.7366502571106\n",
      "loss 111.27371577024459\n",
      "loss 115.98025035142899\n",
      "loss 120.68550924062728\n",
      "loss 125.39539215564727\n",
      "loss 130.08090263605118\n",
      "loss 134.7830699777603\n",
      "loss 139.4397251677513\n",
      "loss 144.13632500648498\n",
      "loss 148.8577701473236\n",
      "loss 153.53952512025833\n",
      "loss 158.23203296661376\n",
      "loss 162.98658477306367\n",
      "loss 167.6701317501068\n",
      "loss 172.47873880147935\n",
      "loss 177.10921412467957\n",
      "loss 181.84475066661835\n",
      "loss 186.39121801137924\n",
      "Epoch:  1\n",
      "training loss =  4.660319218762856\n",
      "Val Loss: 3.3550\tVal Accuracy: 0.4032\n",
      "loss 4.539549121856689\n",
      "loss 9.113470921516418\n",
      "loss 13.71453796863556\n",
      "loss 18.219911205768586\n",
      "loss 22.905169703960418\n",
      "loss 27.598373749256133\n",
      "loss 32.23931370258331\n",
      "loss 36.8081944322586\n",
      "loss 41.46529946565628\n",
      "loss 46.102017328739166\n",
      "loss 50.76737518072128\n",
      "loss 55.39982886552811\n",
      "loss 59.9221564078331\n",
      "loss 64.53005788803101\n",
      "loss 69.14693279981613\n",
      "loss 73.81238532304764\n",
      "loss 78.50633150339127\n",
      "loss 83.18425707817077\n",
      "loss 87.83576899766922\n",
      "loss 92.46299908876419\n",
      "loss 97.10956351041794\n",
      "loss 101.72440053939819\n",
      "loss 106.3909848189354\n",
      "loss 111.04649848937989\n",
      "loss 115.68328045606613\n",
      "loss 120.38179253101349\n",
      "loss 124.97144612550736\n",
      "loss 129.60873701810837\n",
      "loss 134.29437797546387\n",
      "loss 138.93942858934403\n",
      "loss 143.6511310482025\n",
      "loss 148.3092479968071\n",
      "loss 152.99109110355377\n",
      "loss 157.63606016874314\n",
      "loss 162.33314586877822\n",
      "loss 166.9540119957924\n",
      "loss 171.55906148195265\n",
      "loss 176.2560448241234\n",
      "loss 180.8406422281265\n",
      "loss 185.49774632692336\n",
      "Epoch:  2\n",
      "training loss =  4.637973593170667\n",
      "Val Loss: 3.1360\tVal Accuracy: 0.4318\n",
      "loss 4.426916942596436\n",
      "loss 8.879593920707702\n",
      "loss 13.457359945774078\n",
      "loss 17.93257368326187\n",
      "loss 22.49053841114044\n",
      "loss 27.12179168462753\n",
      "loss 31.64108916282654\n",
      "loss 36.34226311683655\n",
      "loss 40.92278683423996\n",
      "loss 45.501058185100554\n",
      "loss 50.01263261795044\n",
      "loss 54.63917459249497\n",
      "loss 59.268388011455535\n",
      "loss 63.85815049648285\n",
      "loss 68.50616356611252\n",
      "loss 73.17920263290405\n",
      "loss 77.76296127557754\n",
      "loss 82.36194947242737\n",
      "loss 86.97823872327804\n",
      "loss 91.62840528011321\n",
      "loss 96.20654607534408\n",
      "loss 100.85162952184677\n",
      "loss 105.50367564439773\n",
      "loss 110.20775717496872\n",
      "loss 114.77016968727112\n",
      "loss 119.4327564573288\n",
      "loss 124.0503935623169\n",
      "loss 128.7059458398819\n",
      "loss 133.31229226827622\n",
      "loss 137.95759222269058\n",
      "loss 142.5018346095085\n",
      "loss 147.18360613822938\n",
      "loss 151.86700387239455\n",
      "loss 156.50205491065978\n",
      "loss 161.12550065279007\n",
      "loss 165.73443502902984\n",
      "loss 170.3539350104332\n",
      "loss 175.02992708444594\n",
      "loss 179.62133425951004\n",
      "loss 184.2477570772171\n",
      "Epoch:  3\n",
      "training loss =  4.607549722539774\n",
      "Val Loss: 3.1494\tVal Accuracy: 0.4303\n",
      "loss 4.472013368606567\n",
      "loss 8.920629119873047\n",
      "loss 13.497560200691224\n",
      "loss 18.047134568691252\n",
      "loss 22.549195396900178\n",
      "loss 27.098562602996825\n",
      "loss 31.68316568851471\n",
      "loss 36.22589476823807\n",
      "loss 40.87717423439026\n",
      "loss 45.473925864696504\n",
      "loss 50.08066890001297\n",
      "loss 54.698968329429626\n",
      "loss 59.28128818511963\n",
      "loss 63.7837420463562\n",
      "loss 68.40768138885498\n",
      "loss 73.0001766705513\n",
      "loss 77.60619185209275\n",
      "loss 82.21155511379241\n",
      "loss 86.84069145202636\n",
      "loss 91.48247723579406\n",
      "loss 96.13880380153655\n",
      "loss 100.75163289546967\n",
      "loss 105.30583072423934\n",
      "loss 109.96157381296157\n",
      "loss 114.61667890071868\n",
      "loss 119.18589065551758\n",
      "loss 123.80410201311112\n",
      "loss 128.33309745311738\n",
      "loss 132.96088435173036\n",
      "loss 137.6005809235573\n",
      "loss 142.24631111145018\n",
      "loss 146.9176823759079\n",
      "loss 151.53035315036774\n",
      "loss 156.14458932876587\n",
      "loss 160.76113464593888\n",
      "loss 165.32083186388016\n",
      "loss 169.99168494701385\n",
      "loss 174.62925612211228\n",
      "loss 179.24573458671568\n",
      "loss 183.81972494602203\n",
      "Epoch:  4\n",
      "training loss =  4.595246499933041\n",
      "Val Loss: 3.0782\tVal Accuracy: 0.4366\n",
      "loss 4.412361025810242\n",
      "loss 8.917735438346863\n",
      "loss 13.479803814888001\n",
      "loss 18.058457436561586\n",
      "loss 22.58325423002243\n",
      "loss 27.20183810710907\n",
      "loss 31.739915044307708\n",
      "loss 36.364783835411075\n",
      "loss 40.9442174577713\n",
      "loss 45.506468334198\n",
      "loss 50.127520730495455\n",
      "loss 54.70428285598755\n",
      "loss 59.26878354549408\n",
      "loss 63.76675345420838\n",
      "loss 68.28667707204819\n",
      "loss 72.89983342647552\n",
      "loss 77.50259666681289\n",
      "loss 82.12285526275635\n",
      "loss 86.70372321128845\n",
      "loss 91.24031552553177\n",
      "loss 95.8802890586853\n",
      "loss 100.43474501132965\n",
      "loss 105.07005015611648\n",
      "loss 109.68781037569046\n",
      "loss 114.2592057466507\n",
      "loss 118.78614488124848\n",
      "loss 123.39592709302902\n",
      "loss 128.0010935330391\n",
      "loss 132.58103060483933\n",
      "loss 137.19182084083556\n",
      "loss 141.8379559469223\n",
      "loss 146.4627505636215\n",
      "loss 151.10048369407653\n",
      "loss 155.6200408911705\n",
      "loss 160.2813406777382\n",
      "loss 164.86307064056396\n",
      "loss 169.4226763176918\n",
      "loss 174.00606649160386\n",
      "loss 178.63199110507966\n",
      "loss 183.34767548084258\n",
      "Epoch:  5\n",
      "training loss =  4.584109676488614\n",
      "Val Loss: 3.1415\tVal Accuracy: 0.4297\n",
      "loss 4.498116858005524\n",
      "loss 8.966012623310089\n",
      "loss 13.46011640548706\n",
      "loss 17.990070283412933\n",
      "loss 22.486809911727907\n",
      "loss 27.02884203672409\n",
      "loss 31.520454037189484\n",
      "loss 36.021991846561434\n",
      "loss 40.57053494215012\n",
      "loss 45.116937568187716\n",
      "loss 49.62454996109009\n",
      "loss 54.15538583278656\n",
      "loss 58.735461583137514\n",
      "loss 63.2431437754631\n",
      "loss 67.83631932497025\n",
      "loss 72.42214791536331\n",
      "loss 76.97728684186936\n",
      "loss 81.51947110176087\n",
      "loss 86.03979864120484\n",
      "loss 90.63204725265503\n",
      "loss 95.16090595960617\n",
      "loss 99.79232652902603\n",
      "loss 104.30090223312378\n",
      "loss 108.88650347232819\n",
      "loss 113.41828892707825\n",
      "loss 118.0107520532608\n",
      "loss 122.5191170167923\n",
      "loss 127.04099182605744\n",
      "loss 131.591884329319\n",
      "loss 136.25452331781386\n",
      "loss 140.83742458343505\n",
      "loss 145.51438482046126\n",
      "loss 150.05135726213456\n",
      "loss 154.65347569704056\n",
      "loss 159.2708378648758\n",
      "loss 163.92189106702804\n",
      "loss 168.5307441210747\n",
      "loss 173.19110320091247\n",
      "loss 177.8846000623703\n",
      "loss 182.48356222867966\n",
      "Epoch:  6\n",
      "training loss =  4.562628256933029\n",
      "Val Loss: 3.2673\tVal Accuracy: 0.4146\n",
      "loss 4.463303415775299\n",
      "loss 8.951521215438843\n",
      "loss 13.477751801013946\n",
      "loss 18.003187284469604\n",
      "loss 22.485659363269807\n",
      "loss 26.97025438785553\n",
      "loss 31.468519937992095\n",
      "loss 36.03499576807022\n",
      "loss 40.6572212600708\n",
      "loss 45.22045460224152\n",
      "loss 49.76415491104126\n",
      "loss 54.226717774868014\n",
      "loss 58.71308261156082\n",
      "loss 63.333512744903565\n",
      "loss 67.95793356418609\n",
      "loss 72.53418061256409\n",
      "loss 77.0881937289238\n",
      "loss 81.65034495353699\n",
      "loss 86.10861668109894\n",
      "loss 90.66123836278915\n",
      "loss 95.25427914857865\n",
      "loss 99.76789193153381\n",
      "loss 104.30899259090424\n",
      "loss 108.80963989973068\n",
      "loss 113.42348206043243\n",
      "loss 117.98677760839462\n",
      "loss 122.61498391628265\n",
      "loss 127.1805755019188\n",
      "loss 131.71032435655593\n",
      "loss 136.30028215408325\n",
      "loss 140.91164923667907\n",
      "loss 145.51529577732086\n",
      "loss 149.99388879776\n",
      "loss 154.64819118976592\n",
      "loss 159.18885563135146\n",
      "loss 163.7501373887062\n",
      "loss 168.32151534557343\n",
      "loss 172.8742401409149\n",
      "loss 177.4415057182312\n",
      "loss 182.06641548871994\n",
      "Epoch:  7\n",
      "training loss =  4.553166490835563\n",
      "Val Loss: 3.1564\tVal Accuracy: 0.4273\n",
      "loss 4.412066621780395\n",
      "loss 8.873273577690124\n",
      "loss 13.293035252094269\n",
      "loss 17.780143411159514\n",
      "loss 22.19558815717697\n",
      "loss 26.652895402908324\n",
      "loss 31.159069738388062\n",
      "loss 35.71283655405045\n",
      "loss 40.20005078315735\n",
      "loss 44.65820567131043\n",
      "loss 49.15422887563705\n",
      "loss 53.61337777853012\n",
      "loss 58.07924857378006\n",
      "loss 62.63836147546768\n",
      "loss 67.08124423503875\n",
      "loss 71.60259026765823\n",
      "loss 76.06892402172089\n",
      "loss 80.65286494016647\n",
      "loss 85.12207795619965\n",
      "loss 89.65457769155502\n",
      "loss 94.19724697113037\n",
      "loss 98.80555831432342\n",
      "loss 103.3772625875473\n",
      "loss 107.94081532239915\n",
      "loss 112.57660530090332\n",
      "loss 117.16239664316177\n",
      "loss 121.66953340291977\n",
      "loss 126.22425230026245\n",
      "loss 130.70733323335648\n",
      "loss 135.27026711225508\n",
      "loss 139.84598918676377\n",
      "loss 144.4914924621582\n",
      "loss 149.03145081281662\n",
      "loss 153.554867272377\n",
      "loss 158.14776951313019\n",
      "loss 162.7020897603035\n",
      "loss 167.24089248895646\n",
      "loss 171.7787041592598\n",
      "loss 176.28413817882537\n",
      "loss 180.84433217525483\n",
      "Epoch:  8\n",
      "training loss =  4.522148848864602\n",
      "Val Loss: 3.0821\tVal Accuracy: 0.4360\n",
      "loss 4.458478929996491\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-f908d54c5405>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcutmix_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_Epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-38bd5c1a6257>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, test_loader, beta, cutmix_prob, epochs)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mavg_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_num\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, validation_dataloader, beta, cutmix_prob, epochs = num_Epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load saved model from checkpoint**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Validation_Loss_List'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-e01769b2e15e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Training_Loss_List'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mv_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Validation_Loss_List'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mv_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Validation_Accuracy_List'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# epoch = checkpoint['epoch']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Validation_Loss_List'"
     ]
    }
   ],
   "source": [
    "checkpoint = torch.load('saved_model_checkpoints/cutmix_2gpu')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "train_loss = checkpoint['Training_Loss_List'] \n",
    "v_loss = checkpoint['Validation_Loss_List']\n",
    "v_acc = checkpoint['Validation_Accuracy_List']\n",
    "epoch = checkpoint['epoch']\n",
    "lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    initial_lr: 0.1\n",
       "    lr: 0.1\n",
       "    momentum: 0.9\n",
       "    nesterov: False\n",
       "    weight_decay: 0.0001\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generate plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(8,8))\n",
    "# x = np.arange(91,121)\n",
    "# plt.plot(x, train_loss)\n",
    "# plt.xlabel('Epochs', fontsize =16)\n",
    "# plt.ylabel('Training Loss', fontsize =16)\n",
    "# plt.title('Training Loss v/s Epochs',fontsize =16)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "x = np.arange(1,128)\n",
    "plt.plot(x, train_loss[:-1], label = 'Training Loss')\n",
    "plt.plot(x, v_loss, label = 'Validation Loss')\n",
    "plt.xlabel('Epochs', fontsize =16)\n",
    "plt.ylabel('Loss', fontsize =16)\n",
    "plt.title('Loss v/s Epochs',fontsize =16)\n",
    "plt.legend(fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
