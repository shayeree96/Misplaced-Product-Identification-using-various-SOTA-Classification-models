{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 20 11:23:58 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 42%   63C    P2   166W / 280W |  20896MiB / 24220MiB |     90%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 80%   87C    P2   226W / 280W |  19108MiB / 24220MiB |     86%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 95%   88C    P2   240W / 280W |  19108MiB / 24220MiB |     86%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN RTX           Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 90%   89C    P2   270W / 280W |  19108MiB / 24220MiB |     84%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN RTX           Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 40%   26C    P8     2W / 280W |     11MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN RTX           Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 41%   32C    P8    33W / 280W |     11MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN RTX           Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 41%   29C    P8    19W / 280W |   2526MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN RTX           Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 41%   26C    P8    13W / 280W |     11MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0      4052      C   python                                     20865MiB |\n",
      "|    1      4052      C   python                                     19077MiB |\n",
      "|    2      4052      C   python                                     19077MiB |\n",
      "|    3      4052      C   python                                     19077MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### IMPORTING NECESSARY MODULES #########\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "sys.path.append('/home/ironman/abhishek/AliProducts/architectures/')\n",
    "sys.path.append('/home/ironman/abhishek/AliProducts/Helper/')\n",
    "from dataloader import mydataset, create_prime_dict \n",
    "from trainer_cutmix import train, test_classify, eval_classify\n",
    "from resnet_models import ResNet,Bottleneck\n",
    "from Load_model import load\n",
    "from plot_curves import plot_loss, plot_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:4' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataloading Scheme**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlist = '/home/ironman/abhishek/AliProducts/Newlist/NEW_train_list1.txt'\n",
    "validlist = '/home/ironman/abhishek/AliProducts/Newlist/NEW_valid_list1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes =  50030\n"
     ]
    }
   ],
   "source": [
    "prime_dict = create_prime_dict(trainlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(trainlist)\n",
    "# s=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# h=[]\n",
    "# print(len(s))\n",
    "# tlist = []\n",
    "# for line in s:\n",
    "    \n",
    "#     path,upc = line[:-1].split(',')\n",
    "#     i = '/media/Duamutef/abhishek/AliProducts/'+path.split('haoc/')[1]  \n",
    "#     image = Image.open(i, mode='r')\n",
    "\n",
    "#     if image.mode !='RGB':\n",
    "#         h.append(image.size)\n",
    "      \n",
    "#     if image.mode =='RGB':\n",
    "#         tlist.append(i+','+upc+'\\n')\n",
    "    \n",
    "#     image.close()\n",
    "\n",
    "# f.close()\n",
    "# len(tlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = open('/home/ironman/abhishek/AliProducts/Newlist/NEW_train_list1.txt','w')\n",
    "# for line in tlist:\n",
    "#     f1.write(line)\n",
    "# f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open(validlist)\n",
    "# s=f.readlines()\n",
    "\n",
    "# h1=[]\n",
    "# print(len(s))\n",
    "# vallist = []\n",
    "# for line in s:\n",
    "    \n",
    "#     path,upc = line[:-1].split(',')\n",
    "#     i = '/media/Duamutef/abhishek/AliProducts/'+path.split('haoc/')[1]  \n",
    "#     image = Image.open(i, mode='r')\n",
    "\n",
    "#     if image.mode !='RGB':\n",
    "#         h.append(image.size)\n",
    "        \n",
    "#     if image.mode =='RGB':\n",
    "#         vallist.append(i+','+upc+'\\n')\n",
    "    \n",
    "#     image.close()\n",
    "\n",
    "# f.close()\n",
    "# len(vallist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f2 = open('/home/ironman/abhishek/AliProducts/Newlist/NEW_valid_list1.txt','w')\n",
    "# for line in vallist:\n",
    "#     f2.write(line)\n",
    "# f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Train Dataloader #### \n",
    "train_dataset = mydataset(trainlist, prime_dict, name='train')          \n",
    "train_dataloader = data.DataLoader(train_dataset, shuffle= True, batch_size = 256, num_workers=16,pin_memory=True)\n",
    "\n",
    "\n",
    "#### Validation Dataloader #### \n",
    "validation_dataset = mydataset(validlist, prime_dict, name='valid')         \n",
    "validation_dataloader = data.DataLoader(validation_dataset, shuffle=False, batch_size = 128, num_workers=16,pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (3): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (4): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (5): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (6): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (7): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (8): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (9): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (10): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (11): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (12): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (13): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (14): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (15): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (16): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (17): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (18): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (19): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (20): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (21): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (22): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): Bottleneck(\n",
       "        (conv1): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "      (2): Bottleneck(\n",
       "        (conv1): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv2): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
       "        (bn2): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv3): Conv2d(2048, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=50030, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = models.resnext101_32x8d(pretrained = False, num_classes = 50030)\n",
    "\n",
    "model = nn.DataParallel(model,device_ids=[4,5,6,7]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 25, gamma = 0.1)\n",
    "\n",
    "# Epochs\n",
    "num_Epochs = 90\n",
    "\n",
    "beta=1\n",
    "\n",
    "cutmix_prob = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelname = 'NewData_Apro_AA_Cutmix_ResNeXt101_32x8d'\n",
    "modelpath = '/home/ironman/abhishek/saved_model_checkpoints/AliProducts/'+modelname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 10.929397249221802\n",
      "loss 21.754922943115233\n",
      "loss 32.574211530685425\n",
      "loss 43.39178409576416\n",
      "loss 54.208316020965576\n",
      "loss 65.02278270721436\n",
      "loss 75.8364190673828\n",
      "loss 86.6492978477478\n",
      "loss 97.46004779815674\n",
      "loss 108.26989413261414\n",
      "loss 119.07832690238952\n",
      "loss 129.88569869041442\n",
      "loss 140.68724915504455\n",
      "loss 151.4813890838623\n",
      "loss 162.26945992469788\n",
      "loss 173.0497418785095\n",
      "loss 183.82717909812928\n",
      "loss 194.59761132240294\n",
      "loss 205.3571516418457\n",
      "loss 216.11430557250978\n",
      "loss 226.86092781066895\n",
      "loss 237.59843144416809\n",
      "loss 248.33528819084168\n",
      "loss 259.0655483150482\n",
      "loss 269.7880341053009\n",
      "loss 280.5086734867096\n",
      "loss 301.92041845321654\n",
      "loss 312.62251637458803\n",
      "loss 323.3181109046936\n",
      "loss 334.00693050384524\n",
      "loss 344.6918183422089\n",
      "loss 355.36865070343015\n",
      "loss 366.035722618103\n",
      "loss 376.6973275756836\n",
      "loss 387.36040234565735\n",
      "loss 398.0061912155151\n",
      "loss 408.6419890880585\n",
      "loss 419.2708703041077\n",
      "loss 429.90141414642335\n",
      "loss 440.52380516052244\n",
      "loss 451.13935965538025\n",
      "loss 461.7413053894043\n",
      "loss 472.34018756866453\n",
      "Epoch:  1\n",
      "Training loss =  10.733527189751864\n",
      "Validation Loss: 10.4942\tTop 1 Validation Accuracy: 0.0003\t Top 5 Validation Accuracy: 0.0011\n",
      "loss 10.533990573883056\n",
      "loss 21.071203260421754\n",
      "loss 31.597585067749023\n",
      "loss 42.1212713432312\n",
      "loss 52.645297574996945\n",
      "loss 63.16458374023438\n",
      "loss 73.6686100578308\n",
      "loss 84.16905913352966\n",
      "loss 94.65916778564453\n",
      "loss 105.14558179855347\n",
      "loss 115.6096994304657\n",
      "loss 126.05605886459351\n",
      "loss 136.51314661026\n",
      "loss 146.94004665374757\n",
      "loss 157.3416298866272\n",
      "loss 167.75271265029906\n",
      "loss 178.14936778068542\n",
      "loss 188.53936436653137\n",
      "loss 198.9245222568512\n",
      "loss 209.2743398284912\n",
      "loss 219.61000641822815\n",
      "loss 229.96845871925353\n",
      "loss 240.27444144248963\n",
      "loss 250.5749819469452\n",
      "loss 260.84192218780515\n",
      "loss 271.11770398139953\n",
      "loss 281.35321034431456\n",
      "loss 291.58728711128236\n",
      "loss 301.8193341732025\n",
      "loss 312.0342288017273\n",
      "loss 322.2355324459076\n",
      "loss 332.41128632545474\n",
      "loss 352.72161963462827\n",
      "loss 362.86279783248904\n",
      "loss 372.9859803390503\n",
      "loss 383.096363401413\n",
      "loss 393.1760808467865\n",
      "loss 403.2500394153595\n",
      "loss 413.30052265167234\n",
      "loss 423.32333967208865\n",
      "loss 433.3418305110931\n",
      "loss 443.34248830795286\n",
      "loss 453.3265411090851\n",
      "Epoch:  2\n",
      "Training loss =  10.299352987283854\n",
      "Validation Loss: 12.3397\tTop 1 Validation Accuracy: 0.0027\t Top 5 Validation Accuracy: 0.0105\n",
      "loss 9.89362509727478\n",
      "loss 19.746918535232545\n",
      "loss 29.56883218765259\n",
      "loss 39.423057556152344\n",
      "loss 49.21731001853943\n",
      "loss 59.05713594436646\n",
      "loss 68.83550041198731\n",
      "loss 78.60003440856934\n",
      "loss 88.34860942840577\n",
      "loss 98.10534465789794\n",
      "loss 107.83636726379395\n",
      "loss 117.49356947898865\n",
      "loss 127.15799233436584\n",
      "loss 136.7038736438751\n",
      "loss 146.3307006072998\n",
      "loss 155.9260253715515\n",
      "loss 165.51230403900146\n",
      "loss 175.0215749168396\n",
      "loss 184.50973637580873\n",
      "loss 194.0614436340332\n",
      "loss 203.52876132965088\n",
      "loss 212.9042465877533\n",
      "loss 222.28982049942016\n",
      "loss 231.67879293441771\n",
      "loss 241.0038770866394\n",
      "loss 250.38544463157655\n",
      "loss 259.73348242759704\n",
      "loss 268.9952551174164\n",
      "loss 278.2142189025879\n",
      "loss 287.4287612056732\n",
      "loss 296.6161610221863\n",
      "loss 305.82056869506835\n",
      "loss 314.98030414581297\n",
      "loss 324.0404060840607\n",
      "loss 333.12802387714385\n",
      "loss 342.1548378324509\n",
      "loss 351.1374010753632\n",
      "loss 360.1609874868393\n",
      "loss 369.1396981430054\n",
      "loss 378.1006755781174\n",
      "loss 386.97661962509153\n",
      "loss 395.8403478384018\n",
      "loss 404.61044297218325\n",
      "loss 413.4194794988632\n",
      "Epoch:  3\n",
      "Training loss =  9.390758491350555\n",
      "Validation Loss: 7.3290\tTop 1 Validation Accuracy: 0.0537\t Top 5 Validation Accuracy: 0.1356\n",
      "loss 8.538194131851196\n",
      "loss 17.123828353881837\n",
      "loss 25.661617212295532\n",
      "loss 34.23786220550537\n",
      "loss 42.747657995223996\n",
      "loss 51.228123736381534\n",
      "loss 59.71070312023163\n",
      "loss 68.03830594539642\n",
      "loss 76.40794326782226\n",
      "loss 84.77329542636872\n",
      "loss 93.08168545246124\n",
      "loss 101.36345286846161\n",
      "loss 109.63408220291137\n",
      "loss 117.77841142177581\n",
      "loss 125.99184903621673\n",
      "loss 134.16844803333282\n",
      "loss 142.29768076896667\n",
      "loss 150.32074582099915\n",
      "loss 158.35280839920043\n",
      "loss 166.37372017860412\n",
      "loss 174.44592519760133\n",
      "loss 182.49364122867584\n",
      "loss 190.4516890144348\n",
      "loss 198.38131900310518\n",
      "loss 206.29729682922363\n",
      "loss 214.15579537868499\n",
      "loss 221.98294993400575\n",
      "loss 229.8151653289795\n",
      "loss 237.60410203933716\n",
      "loss 245.2463180732727\n",
      "loss 252.91511634349823\n",
      "loss 260.67768623828886\n",
      "loss 268.3421130132675\n",
      "loss 275.8448133611679\n",
      "loss 283.48295847415926\n",
      "loss 290.9539736509323\n",
      "loss 298.3920117044449\n",
      "loss 305.92366686344144\n",
      "loss 313.36779597282407\n",
      "loss 320.76857939720156\n",
      "loss 328.1845278787613\n",
      "loss 335.54141896247864\n",
      "loss 342.969097237587\n",
      "loss 350.4768491458893\n",
      "Epoch:  4\n",
      "Training loss =  7.95950216117912\n",
      "Validation Loss: 4.9056\tTop 1 Validation Accuracy: 0.2284\t Top 5 Validation Accuracy: 0.4143\n",
      "loss 7.003441500663757\n",
      "loss 14.048864526748657\n",
      "loss 21.092240390777587\n",
      "loss 28.194439473152162\n",
      "loss 35.21792805671692\n",
      "loss 42.29641398429871\n",
      "loss 49.2601553106308\n",
      "loss 56.301882066726684\n",
      "loss 63.217654609680174\n",
      "loss 70.12746602535248\n",
      "loss 77.10395240306855\n",
      "loss 84.06688628673554\n",
      "loss 91.0954500579834\n",
      "loss 98.06017401218415\n",
      "loss 104.95637386322022\n",
      "loss 111.73734632015228\n",
      "loss 118.6156884098053\n",
      "loss 125.45648170948029\n",
      "loss 132.31522742271423\n",
      "loss 138.9687440443039\n",
      "loss 145.61148435115814\n",
      "loss 152.44749168872832\n",
      "loss 159.21803006649017\n",
      "loss 165.93560793876648\n",
      "loss 172.7416893339157\n",
      "loss 179.38194329738616\n",
      "loss 185.92688246250154\n",
      "loss 192.57388667106628\n",
      "loss 199.11893595218658\n",
      "loss 205.69926316738128\n",
      "loss 212.34501853466034\n",
      "loss 218.94787925720215\n",
      "loss 225.37152069568634\n",
      "loss 231.8251097345352\n",
      "loss 238.10181600809096\n",
      "loss 244.604532058239\n",
      "loss 251.06837450742722\n",
      "loss 257.5271874690056\n",
      "loss 263.9037688136101\n",
      "loss 270.2744056582451\n",
      "loss 276.6043382978439\n",
      "loss 282.9570147228241\n",
      "loss 289.4085345029831\n",
      "loss 295.5816077375412\n",
      "Epoch:  5\n",
      "Training loss =  6.716083044520062\n",
      "Validation Loss: 3.5339\tTop 1 Validation Accuracy: 0.3756\t Top 5 Validation Accuracy: 0.6060\n",
      "loss 6.128463423252105\n",
      "loss 12.302396936416626\n",
      "loss 18.354993844032286\n",
      "loss 24.482693204879762\n",
      "loss 30.650439591407775\n",
      "loss 36.701737592220304\n",
      "loss 42.774901711940764\n",
      "loss 48.86241972446442\n",
      "loss 55.09301815986633\n",
      "loss 61.1817163348198\n",
      "loss 67.21072200298309\n",
      "loss 73.35003514289856\n",
      "loss 79.35239762067795\n",
      "loss 85.32902524471282\n",
      "loss 91.25485123634338\n",
      "loss 97.35671647071838\n",
      "loss 103.36697618246079\n",
      "loss 109.32865111589432\n",
      "loss 115.35184579849243\n",
      "loss 121.28942204236984\n",
      "loss 127.41174013137817\n",
      "loss 133.4221704030037\n",
      "loss 139.56527362346648\n",
      "loss 145.51418648481368\n",
      "loss 151.48364031553268\n",
      "loss 157.3828097820282\n",
      "loss 163.35175960063935\n",
      "loss 169.4159244608879\n",
      "loss 175.4822268676758\n",
      "loss 181.4267144346237\n",
      "loss 187.3826167535782\n",
      "loss 193.26348055124282\n",
      "loss 199.2560704612732\n",
      "loss 205.25167413949967\n",
      "loss 210.97333118200302\n",
      "loss 216.8480612182617\n",
      "loss 222.55543202877044\n",
      "loss 228.44408415317537\n",
      "loss 234.34961916685106\n",
      "loss 240.03937769651412\n",
      "loss 245.9523746275902\n",
      "loss 251.86104657888413\n",
      "loss 257.7562541413307\n",
      "loss 263.6160878634453\n",
      "Epoch:  6\n",
      "Training loss =  5.990380539367621\n",
      "Validation Loss: 3.1048\tTop 1 Validation Accuracy: 0.4386\t Top 5 Validation Accuracy: 0.6789\n",
      "loss 5.6424789714813235\n",
      "loss 11.218273067474366\n",
      "loss 16.608762168884276\n",
      "loss 22.15304595708847\n",
      "loss 27.641835000514984\n",
      "loss 33.341058440208435\n",
      "loss 39.04375084161759\n",
      "loss 44.585516369342805\n",
      "loss 50.04873829841614\n",
      "loss 55.53809901714325\n",
      "loss 61.14881360292435\n",
      "loss 66.80805584669113\n",
      "loss 72.37184005022048\n",
      "loss 77.91778705120086\n",
      "loss 83.67702682495117\n",
      "loss 89.11916008234024\n",
      "loss 94.7970722103119\n",
      "loss 100.37354885578155\n",
      "loss 106.00720252990723\n",
      "loss 111.6836961221695\n",
      "loss 117.31934807300567\n",
      "loss 122.82892408847809\n",
      "loss 128.31106739759446\n",
      "loss 133.91916273117064\n",
      "loss 139.49067591667176\n",
      "loss 145.0654416513443\n",
      "loss 150.54759249687194\n",
      "loss 155.98438235759735\n",
      "loss 161.54914358854293\n",
      "loss 166.92161514520646\n",
      "loss 172.42242411613464\n",
      "loss 177.92255434036255\n",
      "loss 183.44319536209107\n",
      "loss 188.88861132621764\n",
      "loss 194.5060519218445\n",
      "loss 199.9147709608078\n",
      "loss 205.30249008893966\n",
      "loss 210.76651162147522\n",
      "loss 216.333632004261\n",
      "loss 221.78979672908784\n",
      "loss 227.15064458370207\n",
      "loss 232.7282034945488\n",
      "loss 238.08193394899368\n",
      "loss 243.64163294553757\n",
      "Epoch:  7\n",
      "Training loss =  5.5373143601315675\n",
      "Validation Loss: 2.4070\tTop 1 Validation Accuracy: 0.5298\t Top 5 Validation Accuracy: 0.7734\n",
      "loss 5.156103205680847\n",
      "loss 10.408322308063507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 15.497459361553192\n",
      "loss 20.770117037296295\n",
      "loss 26.008161036968232\n",
      "loss 31.222968533039094\n",
      "loss 36.34592840194702\n",
      "loss 41.710502979755404\n",
      "loss 46.95762809514999\n",
      "loss 52.16895679950714\n",
      "loss 57.496606152057645\n",
      "loss 62.566491112709045\n",
      "loss 67.86687086343765\n",
      "loss 73.21528353691102\n",
      "loss 78.44486744642258\n",
      "loss 83.77568132400512\n",
      "loss 89.0597852063179\n",
      "loss 94.33073147535325\n",
      "loss 99.66142205953598\n",
      "loss 104.99925758123398\n",
      "loss 110.23215136766434\n",
      "loss 115.54246727228164\n",
      "loss 120.88569724082947\n",
      "loss 126.07772734642029\n",
      "loss 131.293906621933\n",
      "loss 136.71160546541213\n",
      "loss 142.03923149108886\n",
      "loss 147.29050760746003\n",
      "loss 152.54488394498824\n",
      "loss 157.77047148942947\n",
      "loss 163.10518704175948\n",
      "loss 168.3875400018692\n",
      "loss 173.64060131549834\n",
      "loss 178.91546322107314\n",
      "loss 184.1096124100685\n",
      "loss 189.27245883703233\n",
      "loss 194.48256667613984\n",
      "loss 199.68872567415238\n",
      "loss 204.92214617729186\n",
      "loss 210.16201719284058\n",
      "loss 215.4027048587799\n",
      "loss 220.6067267179489\n",
      "loss 226.0709567642212\n",
      "loss 231.31496007680892\n",
      "Epoch:  8\n",
      "Training loss =  5.257461776887805\n",
      "Validation Loss: 2.0901\tTop 1 Validation Accuracy: 0.5844\t Top 5 Validation Accuracy: 0.8250\n",
      "loss 5.035518157482147\n",
      "loss 10.108874025344848\n",
      "loss 15.11544537782669\n",
      "loss 20.149715654850006\n",
      "loss 25.12028421640396\n",
      "loss 30.149672303199768\n",
      "loss 35.054262762069705\n",
      "loss 40.110429587364195\n",
      "loss 45.169914150238036\n",
      "loss 50.14245709657669\n",
      "loss 55.14691559791565\n",
      "loss 60.38113037347794\n",
      "loss 65.57140144824982\n",
      "loss 70.60522634267807\n",
      "loss 75.7692574262619\n",
      "loss 80.91043374061584\n",
      "loss 85.98088989019394\n",
      "loss 91.09369632959366\n",
      "loss 96.03395382642746\n",
      "loss 101.03714031696319\n",
      "loss 106.19667095184326\n",
      "loss 111.29571724891663\n",
      "loss 116.35170955181121\n",
      "loss 121.49872128009797\n",
      "loss 126.62595647096634\n",
      "loss 131.8054778456688\n",
      "loss 136.94435094118117\n",
      "loss 142.07352895498275\n",
      "loss 147.29544330358505\n",
      "loss 152.49359681606293\n",
      "loss 157.63553358078002\n",
      "loss 162.71692729234695\n",
      "loss 167.78595483779907\n",
      "loss 172.99815229177474\n",
      "loss 178.16192546844482\n",
      "loss 183.3094002199173\n",
      "loss 188.27508265733718\n",
      "loss 193.4485201406479\n",
      "loss 198.61068289995194\n",
      "loss 203.7621904373169\n",
      "loss 208.96044011831285\n",
      "loss 214.08833151340485\n",
      "loss 219.24314309835435\n",
      "loss 224.2713088107109\n",
      "Epoch:  9\n",
      "Training loss =  5.097050802236411\n",
      "Validation Loss: 2.1646\tTop 1 Validation Accuracy: 0.5842\t Top 5 Validation Accuracy: 0.8190\n",
      "loss 4.825190346240998\n",
      "loss 9.716756129264832\n",
      "loss 14.479113163948059\n",
      "loss 19.458205041885375\n",
      "loss 24.26010313987732\n",
      "loss 29.1665679192543\n",
      "loss 33.93033154010773\n",
      "loss 38.806539509296414\n",
      "loss 43.7059618806839\n",
      "loss 48.713252577781674\n",
      "loss 53.53630403041839\n",
      "loss 58.55708038330078\n",
      "loss 63.49046938896179\n",
      "loss 68.28890006542206\n",
      "loss 73.11560395479202\n",
      "loss 78.11915436029435\n",
      "loss 83.09190351724625\n",
      "loss 87.91641986608505\n",
      "loss 92.84591806888581\n",
      "loss 97.87743433237075\n",
      "loss 102.74019187211991\n",
      "loss 107.65682142257691\n",
      "loss 112.64013153553009\n",
      "loss 117.58543835163117\n",
      "loss 122.59254800319671\n",
      "loss 127.57529747486115\n",
      "loss 132.57584476947784\n",
      "loss 137.49287642478942\n",
      "loss 142.49090116024018\n",
      "loss 152.4874594283104\n",
      "loss 157.57745784521103\n",
      "loss 162.53658977270126\n",
      "loss 167.59056772232054\n",
      "loss 172.58541014671326\n",
      "loss 177.53319598436354\n",
      "loss 182.5583352470398\n",
      "loss 187.64842886447906\n",
      "loss 192.5634704089165\n",
      "loss 197.59956084489824\n",
      "loss 202.59986873149873\n",
      "loss 207.4672824215889\n",
      "loss 212.45790394306184\n",
      "loss 217.4731200313568\n",
      "Epoch:  10\n",
      "Training loss =  4.943040684096755\n",
      "Validation Loss: 1.7502\tTop 1 Validation Accuracy: 0.6361\t Top 5 Validation Accuracy: 0.8656\n",
      "loss 4.603974454402923\n",
      "loss 9.132440114021302\n",
      "loss 13.78402610063553\n",
      "loss 18.527176802158355\n",
      "loss 23.352896904945375\n",
      "loss 28.17984232902527\n",
      "loss 32.922321388721464\n",
      "loss 37.67185286998749\n",
      "loss 42.37799899339676\n",
      "loss 47.20704712867737\n",
      "loss 51.955974195003506\n",
      "loss 56.71892707824707\n",
      "loss 61.55411942005158\n",
      "loss 66.28560787439346\n",
      "loss 71.23624083042145\n",
      "loss 75.92653344392777\n",
      "loss 80.7636876821518\n",
      "loss 90.37688756465911\n",
      "loss 95.24937452793121\n",
      "loss 100.01279143810272\n",
      "loss 104.9135985326767\n",
      "loss 109.77651284217835\n",
      "loss 114.5986453294754\n",
      "loss 119.34268606662751\n",
      "loss 124.28977860689163\n",
      "loss 129.19457043647765\n",
      "loss 134.10893383264542\n",
      "loss 139.02441978931427\n",
      "loss 143.9123390340805\n",
      "loss 148.7907753610611\n",
      "loss 153.5002876830101\n",
      "loss 158.35218925714491\n",
      "loss 163.11886269330978\n",
      "loss 168.02923943519593\n",
      "loss 172.92925862073898\n",
      "loss 177.94327090740205\n",
      "loss 182.87258227109908\n",
      "loss 187.6895181131363\n",
      "loss 192.49888211727142\n",
      "loss 197.34668194532395\n",
      "loss 202.28562743902205\n",
      "loss 207.15689952850343\n",
      "loss 211.97302852869035\n",
      "Epoch:  11\n",
      "Training loss =  4.818676215063086\n",
      "Validation Loss: 1.8168\tTop 1 Validation Accuracy: 0.6378\t Top 5 Validation Accuracy: 0.8655\n",
      "loss 4.574497222900391\n",
      "loss 9.265785527229308\n",
      "loss 13.832271234989166\n",
      "loss 18.463179037570953\n",
      "loss 23.037761836051942\n",
      "loss 27.67824884414673\n",
      "loss 32.286957449913025\n",
      "loss 37.09656587600708\n",
      "loss 41.72199448108673\n",
      "loss 46.370426638126375\n",
      "loss 51.111706855297086\n",
      "loss 55.763930044174195\n",
      "loss 60.50556420087814\n",
      "loss 65.22710808753968\n",
      "loss 69.90801263332366\n",
      "loss 74.59239032030105\n",
      "loss 79.29629281759262\n",
      "loss 83.97433650016785\n",
      "loss 88.79676400899888\n",
      "loss 93.60190464019776\n",
      "loss 98.39220168113708\n",
      "loss 103.17079954624177\n",
      "loss 108.01603934526443\n",
      "loss 112.71037889957428\n",
      "loss 117.53465745687485\n",
      "loss 122.35849436521531\n",
      "loss 127.16881252527237\n",
      "loss 132.04390652894975\n",
      "loss 136.88196427345275\n",
      "loss 141.69581558942795\n",
      "loss 146.4699288058281\n",
      "loss 151.39535033226014\n",
      "loss 156.24493048429488\n",
      "loss 161.0004830622673\n",
      "loss 165.77483835458756\n",
      "loss 170.60038221359252\n",
      "loss 175.26251761198043\n",
      "loss 180.00920724868774\n",
      "loss 184.8882123541832\n",
      "loss 189.79981375694274\n",
      "loss 194.64151978731155\n",
      "loss 199.44347972869872\n",
      "loss 204.20831503391267\n",
      "loss 209.08958104133606\n",
      "Epoch:  12\n",
      "Training loss =  4.751899784164051\n",
      "Validation Loss: 1.7581\tTop 1 Validation Accuracy: 0.6515\t Top 5 Validation Accuracy: 0.8793\n",
      "loss 9.019196228981018\n",
      "loss 13.404088265895844\n",
      "loss 17.997798368930816\n",
      "loss 22.543498628139496\n",
      "loss 27.086970710754393\n",
      "loss 31.69042664051056\n",
      "loss 36.345494425296785\n",
      "loss 41.075830025672914\n",
      "loss 45.78495321273804\n",
      "loss 50.3782551240921\n",
      "loss 55.04104523897171\n",
      "loss 59.71048165559769\n",
      "loss 64.41379304885864\n",
      "loss 69.10367079019547\n",
      "loss 73.78679439067841\n",
      "loss 78.55187425136566\n",
      "loss 83.29782692193984\n",
      "loss 87.94775304555893\n",
      "loss 92.68622877120971\n",
      "loss 97.3329253411293\n",
      "loss 102.00080082178116\n",
      "loss 106.65374790906907\n",
      "loss 111.37855798721313\n",
      "loss 116.16376954555511\n",
      "loss 120.98565323114396\n",
      "loss 125.61097000598907\n",
      "loss 130.42414895534515\n",
      "loss 135.07795664548874\n",
      "loss 139.9145771431923\n",
      "loss 144.62593302488327\n",
      "loss 149.35404367446898\n",
      "loss 154.03676827430726\n",
      "loss 158.82278022527694\n",
      "loss 163.5774811553955\n",
      "loss 168.40245349168777\n",
      "loss 173.25539238214492\n",
      "loss 178.0664700078964\n",
      "loss 182.8872694540024\n",
      "loss 187.7134918951988\n",
      "loss 192.38478974342345\n",
      "loss 197.1938216781616\n",
      "loss 201.9172969055176\n",
      "loss 206.58342998743058\n",
      "Epoch:  13\n",
      "Training loss =  4.697818763984679\n",
      "Validation Loss: 1.6650\tTop 1 Validation Accuracy: 0.6600\t Top 5 Validation Accuracy: 0.8823\n",
      "loss 4.552464604377747\n",
      "loss 9.038993411064148\n",
      "loss 13.446642270088196\n",
      "loss 17.95790940284729\n",
      "loss 22.607486703395843\n",
      "loss 27.045880019664764\n",
      "loss 31.622322537899016\n",
      "loss 36.09501852273941\n",
      "loss 40.65911955356598\n",
      "loss 45.31356859445572\n",
      "loss 49.81368576526642\n",
      "loss 54.332570073604586\n",
      "loss 58.978843462467196\n",
      "loss 63.64604598522186\n",
      "loss 68.26464631319045\n",
      "loss 72.96994770288467\n",
      "loss 77.51472989082336\n",
      "loss 82.02805671691894\n",
      "loss 86.68205731630326\n",
      "loss 91.24767724752427\n",
      "loss 95.93246659040452\n",
      "loss 100.60186771392823\n",
      "loss 105.33416172027587\n",
      "loss 110.02790346860886\n",
      "loss 114.74556375741959\n",
      "loss 119.34966015815735\n",
      "loss 128.78471333026886\n",
      "loss 133.3787737774849\n",
      "loss 138.07601076841354\n",
      "loss 142.8939106273651\n",
      "loss 147.5732928943634\n",
      "loss 152.41834904909135\n",
      "loss 157.20753970861435\n",
      "loss 161.8525288939476\n",
      "loss 166.5673077940941\n",
      "loss 171.32933073282243\n",
      "loss 176.11656803131103\n",
      "loss 180.88295449018477\n",
      "loss 185.5982792234421\n",
      "loss 190.33966051578523\n",
      "loss 195.14328414916992\n",
      "loss 199.86348846673965\n",
      "loss 204.4958122587204\n",
      "Epoch:  14\n",
      "Training loss =  4.648229315652455\n",
      "Validation Loss: 1.6709\tTop 1 Validation Accuracy: 0.6642\t Top 5 Validation Accuracy: 0.8875\n",
      "loss 4.452175662517548\n",
      "loss 8.96131646156311\n",
      "loss 13.329594893455505\n",
      "loss 17.65196494102478\n",
      "loss 22.085103862285614\n",
      "loss 26.593920950889586\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 31.209337882995605\n",
      "loss 35.891246590614315\n",
      "loss 40.406964058876035\n",
      "loss 44.936985609531405\n",
      "loss 49.50563185930252\n",
      "loss 54.14712074279785\n",
      "loss 58.66491922855377\n",
      "loss 63.16310787200928\n",
      "loss 67.73422292470931\n",
      "loss 72.34961113214493\n",
      "loss 77.02533833742142\n",
      "loss 81.7070197224617\n",
      "loss 86.3510291481018\n",
      "loss 90.92963228464127\n",
      "loss 95.57065836668015\n",
      "loss 100.1942339682579\n",
      "loss 104.79448829889297\n",
      "loss 109.52940635442734\n",
      "loss 114.12250260829926\n",
      "loss 118.75865366458893\n",
      "loss 123.4761274933815\n",
      "loss 127.97406979799271\n",
      "loss 132.65683878183364\n",
      "loss 137.32205793619156\n",
      "loss 141.83278092861175\n",
      "loss 146.45358692884446\n",
      "loss 151.06675657987594\n",
      "loss 155.75544808149337\n",
      "loss 160.41811363697053\n",
      "loss 165.12246846675873\n",
      "loss 169.8114071202278\n",
      "loss 174.45005273103715\n",
      "loss 179.16253487348555\n",
      "loss 183.89548157453538\n",
      "loss 188.59483250141145\n",
      "loss 193.31634215593337\n",
      "loss 198.00640567302705\n",
      "loss 202.70370172977448\n",
      "Epoch:  15\n",
      "Training loss =  4.606678881751357\n",
      "Validation Loss: 1.5695\tTop 1 Validation Accuracy: 0.6715\t Top 5 Validation Accuracy: 0.8910\n",
      "loss 4.3975340008735655\n",
      "loss 8.858442974090575\n",
      "loss 13.339721956253051\n",
      "loss 17.746458032131194\n",
      "loss 22.093501908779146\n",
      "loss 26.451756319999696\n",
      "loss 31.0243989944458\n",
      "loss 35.54725691318512\n",
      "loss 39.972776246070865\n",
      "loss 44.41316432952881\n",
      "loss 48.98916902542114\n",
      "loss 53.50943359613419\n",
      "loss 58.12541155576706\n",
      "loss 62.719823544025424\n",
      "loss 67.1552973484993\n",
      "loss 71.70239116668701\n",
      "loss 76.12065680265427\n",
      "loss 80.59377119064331\n",
      "loss 85.1660431265831\n",
      "loss 89.82317760944366\n",
      "loss 94.40835451602936\n",
      "loss 98.91436967611313\n",
      "loss 103.62806440353394\n",
      "loss 108.2429165315628\n",
      "loss 112.88619619607925\n",
      "loss 117.49962798118591\n",
      "loss 122.02822299718856\n",
      "loss 126.71822652578354\n",
      "loss 131.29530944347383\n",
      "loss 135.9811950135231\n",
      "loss 140.57251065969467\n",
      "loss 145.18968596458436\n",
      "loss 149.8051010298729\n",
      "loss 154.3925642824173\n",
      "loss 158.96830375671388\n",
      "loss 163.61443264961244\n",
      "loss 168.2400212073326\n",
      "loss 172.9333435988426\n",
      "loss 177.6212199187279\n",
      "loss 182.2538656806946\n",
      "loss 186.80685928583145\n",
      "loss 191.381686668396\n",
      "loss 196.03127120018004\n",
      "loss 200.7572462797165\n",
      "Epoch:  16\n",
      "Training loss =  4.564134470821902\n",
      "Validation Loss: 1.5481\tTop 1 Validation Accuracy: 0.6766\t Top 5 Validation Accuracy: 0.8962\n",
      "loss 4.386822838783264\n",
      "loss 8.880425357818604\n",
      "loss 13.281395590305328\n",
      "loss 17.702483887672425\n",
      "loss 22.0200835442543\n",
      "loss 26.396591024398802\n",
      "loss 30.869478335380553\n",
      "loss 35.36647832036019\n",
      "loss 39.8014965903759\n",
      "loss 44.29073528647423\n",
      "loss 48.8466018807888\n",
      "loss 53.283210407495496\n",
      "loss 57.741405659914015\n",
      "loss 62.2153234064579\n",
      "loss 66.8020505797863\n",
      "loss 71.34191205382348\n",
      "loss 75.75733609318733\n",
      "loss 80.25824517607688\n",
      "loss 84.82680987119674\n",
      "loss 89.52877602815629\n",
      "loss 94.09608140230179\n",
      "loss 98.63357480049133\n",
      "loss 103.16216559410095\n",
      "loss 107.5843052649498\n",
      "loss 112.1047437119484\n",
      "loss 116.79162640571595\n",
      "loss 121.42080908298493\n",
      "loss 125.91923173189163\n",
      "loss 130.5480653142929\n",
      "loss 135.1667514204979\n",
      "loss 139.7610595345497\n",
      "loss 144.42024342536925\n",
      "loss 149.13051044225693\n",
      "loss 153.74938781738283\n",
      "loss 158.4030976319313\n",
      "loss 163.0569888448715\n",
      "loss 167.6730026602745\n",
      "loss 172.32954845190048\n",
      "loss 176.93683349370957\n",
      "loss 181.47613575696946\n",
      "loss 186.18398011922835\n",
      "loss 190.8650117945671\n",
      "loss 195.45141300678253\n",
      "loss 200.03917089223862\n",
      "Epoch:  17\n",
      "Training loss =  4.548153970170241\n",
      "Validation Loss: 1.5209\tTop 1 Validation Accuracy: 0.6858\t Top 5 Validation Accuracy: 0.9027\n",
      "loss 4.323196520805359\n",
      "loss 8.818260722160339\n",
      "loss 13.143781778812409\n",
      "loss 17.51761930704117\n",
      "loss 21.78502654790878\n",
      "loss 26.116446011066436\n",
      "loss 30.514484057426454\n",
      "loss 35.03583410024643\n",
      "loss 39.48620322942734\n",
      "loss 43.85923602342606\n",
      "loss 48.397834928035735\n",
      "loss 52.789312148094176\n",
      "loss 57.24580283403397\n",
      "loss 61.84322263002396\n",
      "loss 66.34304276704788\n",
      "loss 70.80869383096694\n",
      "loss 75.28216440200805\n",
      "loss 79.84416715860367\n",
      "loss 84.3396964097023\n",
      "loss 88.80235023260117\n",
      "loss 93.21185870409012\n",
      "loss 97.8192462015152\n",
      "loss 102.30811738729477\n",
      "loss 106.87226498365402\n",
      "loss 111.45238502025605\n",
      "loss 115.92641154050827\n",
      "loss 120.40222896575928\n",
      "loss 124.83036912083625\n",
      "loss 129.35630666851998\n",
      "loss 134.02214707016944\n",
      "loss 138.65294599413872\n",
      "loss 143.25815921187402\n",
      "loss 147.77173606038093\n",
      "loss 152.2654079043865\n",
      "loss 156.8377266728878\n",
      "loss 161.43919548869133\n",
      "loss 165.98788553595543\n",
      "loss 170.47140413880348\n",
      "loss 175.03734364390374\n",
      "loss 179.65612482190133\n",
      "loss 184.17531200766564\n",
      "loss 188.66842577576637\n",
      "loss 193.30180319190026\n",
      "loss 197.97515459895135\n",
      "Epoch:  18\n",
      "Training loss =  4.5000806028772695\n",
      "Validation Loss: 1.5605\tTop 1 Validation Accuracy: 0.6681\t Top 5 Validation Accuracy: 0.8903\n",
      "loss 4.272772845029831\n",
      "loss 8.507136443853378\n",
      "loss 12.791581519842147\n",
      "loss 17.130560010671616\n",
      "loss 21.46232518315315\n",
      "loss 25.88335547566414\n",
      "loss 30.095240334272386\n",
      "loss 34.48487485766411\n",
      "loss 38.79079511761665\n",
      "loss 43.22940394520759\n",
      "loss 47.58590368628502\n",
      "loss 51.98743702769279\n",
      "loss 56.36453400492668\n",
      "loss 60.831262477636336\n",
      "loss 65.2254248058796\n",
      "loss 69.60612996459007\n",
      "loss 74.0585127723217\n",
      "loss 78.53402782797814\n",
      "loss 82.89871538043022\n",
      "loss 87.38149870991707\n",
      "loss 92.00654776453972\n",
      "loss 96.55207080245017\n",
      "loss 101.0938392484188\n",
      "loss 105.69709877848625\n",
      "loss 110.17589218974113\n",
      "loss 114.69633966565132\n",
      "loss 119.20183502078056\n",
      "loss 123.71460175395012\n",
      "loss 128.12880767464637\n",
      "loss 132.67059114336968\n",
      "loss 137.19674998402596\n",
      "loss 141.68004410624505\n",
      "loss 146.28807716965676\n",
      "loss 150.89367609381677\n",
      "loss 155.46767426371574\n",
      "loss 159.95363399624824\n",
      "loss 164.49210777401925\n",
      "loss 169.19888791441917\n",
      "loss 173.76598017573357\n",
      "loss 178.28010111927986\n",
      "loss 182.82506561636924\n",
      "loss 187.44018904328345\n",
      "loss 192.07420722603797\n",
      "loss 196.57489281058312\n",
      "Epoch:  19\n",
      "Training loss =  4.469728862057114\n",
      "Validation Loss: 1.6037\tTop 1 Validation Accuracy: 0.6744\t Top 5 Validation Accuracy: 0.8946\n",
      "loss 8.36283653974533\n",
      "loss 12.636533749103545\n",
      "loss 17.096566883325576\n",
      "loss 21.53688165307045\n",
      "loss 25.90578431010246\n",
      "loss 30.295360535383224\n",
      "loss 34.652575594186786\n",
      "loss 39.07666026711464\n",
      "loss 43.53100193619728\n",
      "loss 47.82120851874352\n",
      "loss 52.195494183301925\n",
      "loss 56.68758607268333\n",
      "loss 60.9517872607708\n",
      "loss 65.29952148079872\n",
      "loss 69.73473294377327\n",
      "loss 74.15533922791481\n",
      "loss 78.63487720131874\n",
      "loss 82.97237366080284\n",
      "loss 87.38986008763314\n",
      "loss 91.96501124501228\n",
      "loss 96.27546154618263\n",
      "loss 100.75154425501823\n",
      "loss 105.16533520579338\n",
      "loss 109.60922201275825\n",
      "loss 114.07130234122276\n",
      "loss 118.59183648705482\n",
      "loss 123.14217476248741\n",
      "loss 127.68114364027977\n",
      "loss 132.12818826794626\n",
      "loss 136.69249281287193\n",
      "loss 141.19279155373573\n",
      "loss 145.75186151385307\n",
      "loss 150.24850537896157\n",
      "loss 154.6604722750187\n",
      "loss 159.19795946955682\n",
      "loss 163.62599821686746\n",
      "loss 168.14950984835625\n",
      "loss 172.78023856759071\n",
      "loss 177.23020948529245\n",
      "loss 181.77686913371087\n",
      "loss 186.35063497662543\n",
      "loss 190.89822670340538\n",
      "loss 195.42464056134224\n",
      "Epoch:  20\n",
      "Training loss =  4.4441441806857975\n",
      "Validation Loss: 1.5153\tTop 1 Validation Accuracy: 0.6860\t Top 5 Validation Accuracy: 0.9031\n",
      "loss 4.30523579120636\n",
      "loss 8.546681621074676\n",
      "loss 12.728455679416657\n",
      "loss 17.018429641723632\n",
      "loss 21.3123065161705\n",
      "loss 25.554555978775024\n",
      "loss 29.966246507167817\n",
      "loss 34.23168488144874\n",
      "loss 38.465477644205095\n",
      "loss 42.948344579935075\n",
      "loss 51.76263327479362\n",
      "loss 56.132560533285144\n",
      "loss 60.59469977736473\n",
      "loss 65.06494195342064\n",
      "loss 69.463868111372\n",
      "loss 73.78391236186027\n",
      "loss 78.13289598107337\n",
      "loss 82.51815023064613\n",
      "loss 86.94537974238396\n",
      "loss 91.45319080710411\n",
      "loss 95.7889863216877\n",
      "loss 100.21412759184837\n",
      "loss 104.67503003716469\n",
      "loss 109.11227753043175\n",
      "loss 113.48712876915931\n",
      "loss 117.9904137289524\n",
      "loss 122.54846713662147\n",
      "loss 126.91968887925148\n",
      "loss 131.4079742181301\n",
      "loss 136.0186394536495\n",
      "loss 140.5080623447895\n",
      "loss 145.13296784996987\n",
      "loss 149.65013394236564\n",
      "loss 154.14038016676903\n",
      "loss 158.60029683947562\n",
      "loss 163.22238591313362\n",
      "loss 167.7294731962681\n",
      "loss 172.20931284070014\n",
      "loss 176.81058194994927\n",
      "loss 181.38449867367746\n",
      "loss 185.76164881587027\n",
      "loss 190.2815912592411\n",
      "loss 194.88171885371207\n",
      "Epoch:  21\n",
      "Training loss =  4.4331796583980285\n",
      "Validation Loss: 1.5976\tTop 1 Validation Accuracy: 0.6832\t Top 5 Validation Accuracy: 0.9015\n",
      "loss 4.2378719568252565\n",
      "loss 8.455028727054597\n",
      "loss 12.715086493492127\n",
      "loss 16.927524971961976\n",
      "loss 21.123241312503815\n",
      "loss 25.49306180238724\n",
      "loss 29.71840807080269\n",
      "loss 33.93038877367973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 38.26924916148186\n",
      "loss 42.697693907022476\n",
      "loss 47.11066486001015\n",
      "loss 51.52251439929009\n",
      "loss 55.92132270693779\n",
      "loss 60.33581228375435\n",
      "loss 64.66895923018456\n",
      "loss 69.04861928462982\n",
      "loss 73.29504245758056\n",
      "loss 77.73316478729248\n",
      "loss 82.20892670869827\n",
      "loss 86.64542171001435\n",
      "loss 91.10229610681534\n",
      "loss 95.55691867589951\n",
      "loss 100.11491479873658\n",
      "loss 104.54474719762803\n",
      "loss 109.05619847297669\n",
      "loss 113.45229878425599\n",
      "loss 118.01413089752197\n",
      "loss 122.51918726682663\n",
      "loss 126.98522031068802\n",
      "loss 131.58546329259872\n",
      "loss 136.02362464189528\n",
      "loss 140.47943006515504\n",
      "loss 145.0608589911461\n",
      "loss 149.589613468647\n",
      "loss 154.15824872016907\n",
      "loss 158.60558466434478\n",
      "loss 163.11323462724687\n",
      "loss 167.61254764080047\n",
      "loss 172.23379989624024\n",
      "loss 176.7866801595688\n",
      "loss 181.1695425963402\n",
      "loss 185.66557757616044\n",
      "loss 190.16984147310257\n",
      "loss 194.59918643713\n",
      "Epoch:  22\n",
      "Training loss =  4.424755912966908\n",
      "Validation Loss: 1.4498\tTop 1 Validation Accuracy: 0.6978\t Top 5 Validation Accuracy: 0.9110\n",
      "loss 4.244708297252655\n",
      "loss 8.527715978622437\n",
      "loss 12.74825511932373\n",
      "loss 16.967476880550386\n",
      "loss 21.100430337190627\n",
      "loss 25.292695323228838\n",
      "loss 29.540509189367295\n",
      "loss 33.80771240592003\n",
      "loss 38.14511707186699\n",
      "loss 42.42266976237297\n",
      "loss 46.7434031021595\n",
      "loss 51.12325612425804\n",
      "loss 55.41176466584206\n",
      "loss 59.86937623381615\n",
      "loss 64.24831056833267\n",
      "loss 68.64394227266311\n",
      "loss 73.11198040723801\n",
      "loss 77.57234057426453\n",
      "loss 82.09036197662354\n",
      "loss 86.55925176143646\n",
      "loss 90.84064562797546\n",
      "loss 95.11242625713348\n",
      "loss 99.61310339450836\n",
      "loss 104.01617262363433\n",
      "loss 108.35213181734085\n",
      "loss 112.87392289876938\n",
      "loss 117.37765848636627\n",
      "loss 121.81252752065659\n",
      "loss 126.40560176372529\n",
      "loss 130.81252609968186\n",
      "loss 135.43925700902938\n",
      "loss 139.9362535572052\n",
      "loss 144.43281876325608\n",
      "loss 149.01608062028885\n",
      "loss 153.4927393746376\n",
      "loss 157.97185982227325\n",
      "loss 162.47637741804124\n",
      "loss 167.1252373623848\n",
      "loss 171.67429277658462\n",
      "loss 176.08980798244477\n",
      "loss 180.5575914978981\n",
      "loss 185.04783017635344\n",
      "loss 189.60270723581314\n",
      "loss 194.03335811376573\n",
      "Epoch:  23\n",
      "Training loss =  4.411154147129475\n",
      "Validation Loss: 1.4418\tTop 1 Validation Accuracy: 0.6935\t Top 5 Validation Accuracy: 0.9096\n",
      "loss 4.128962554931641\n",
      "loss 8.287978060245514\n",
      "loss 12.4959832072258\n",
      "loss 16.739832482337953\n",
      "loss 21.007249622344972\n",
      "loss 25.253458876609802\n",
      "loss 29.5254435133934\n",
      "loss 33.81944902539253\n",
      "loss 38.0947730243206\n",
      "loss 42.43361194252968\n",
      "loss 46.76941598534584\n",
      "loss 51.12826293587685\n",
      "loss 55.41009226918221\n",
      "loss 59.78840658307075\n",
      "loss 64.25064197659492\n",
      "loss 68.61888592839242\n",
      "loss 73.03275967001915\n",
      "loss 77.26909757256507\n",
      "loss 81.63287021040917\n",
      "loss 86.01234886288643\n",
      "loss 90.49376963973046\n",
      "loss 94.95319103837014\n",
      "loss 99.27977538466453\n",
      "loss 103.80665893673897\n",
      "loss 108.37338652729989\n",
      "loss 112.80964386582374\n",
      "loss 117.25946116566658\n",
      "loss 121.77348764061928\n",
      "loss 126.21443004012107\n",
      "loss 130.63314723610878\n",
      "loss 135.0365760576725\n",
      "loss 139.59660120844842\n",
      "loss 144.15264884591102\n",
      "loss 148.557627876997\n",
      "loss 152.96442837834357\n",
      "loss 157.39405233740806\n",
      "loss 161.88260764956473\n",
      "loss 166.46278300642967\n",
      "loss 170.87639875769617\n",
      "loss 175.4668090593815\n",
      "loss 179.8800134265423\n",
      "loss 184.4084708416462\n",
      "loss 188.88380687832833\n",
      "loss 193.40435288786887\n",
      "Epoch:  24\n",
      "Training loss =  4.395205458255465\n",
      "Validation Loss: 1.5696\tTop 1 Validation Accuracy: 0.6817\t Top 5 Validation Accuracy: 0.8991\n",
      "loss 4.270436353683472\n",
      "loss 8.528638780117035\n",
      "loss 12.550996155738831\n",
      "loss 16.91307814359665\n",
      "loss 21.142628028392792\n",
      "loss 25.40323691368103\n",
      "loss 29.62008065700531\n",
      "loss 33.93343116760254\n",
      "loss 38.31334610104561\n",
      "loss 42.67603915810585\n",
      "loss 46.95350799679756\n",
      "loss 51.18871184229851\n",
      "loss 55.65809691786766\n",
      "loss 60.027823423147204\n",
      "loss 64.30203064322471\n",
      "loss 68.65932944417\n",
      "loss 73.06753131985664\n",
      "loss 77.39202976822853\n",
      "loss 81.69920335650444\n",
      "loss 85.9819750058651\n",
      "loss 90.41773290753365\n",
      "loss 94.82592773079872\n",
      "loss 99.12191066861152\n",
      "loss 103.62581266283989\n",
      "loss 108.10703611969947\n",
      "loss 112.5920985853672\n",
      "loss 116.98352624297142\n",
      "loss 121.33962208509445\n",
      "loss 125.67068973779678\n",
      "loss 130.08727223157882\n",
      "loss 134.53475580453872\n",
      "loss 138.99527054309846\n",
      "loss 143.45845957279207\n",
      "loss 147.80302421092986\n",
      "loss 152.21993857860565\n",
      "loss 156.59625660657883\n",
      "loss 161.01942838668822\n",
      "loss 165.50084306001662\n",
      "loss 169.97947858810426\n",
      "loss 174.45209826231002\n",
      "loss 178.94644347667693\n",
      "loss 183.34085609674455\n",
      "loss 187.7535885500908\n",
      "loss 192.2237148165703\n",
      "Epoch:  25\n",
      "Training loss =  4.370833870478365\n",
      "Validation Loss: 1.4528\tTop 1 Validation Accuracy: 0.6917\t Top 5 Validation Accuracy: 0.9066\n",
      "loss 4.036451516151428\n",
      "loss 7.9616218543052675\n",
      "loss 11.707938319444656\n",
      "loss 15.310530731678009\n",
      "loss 18.978696602582932\n",
      "loss 22.457582606077196\n",
      "loss 26.00989137530327\n",
      "loss 29.49440947175026\n",
      "loss 32.92167776107788\n",
      "loss 36.36754406690598\n",
      "loss 39.827349437475206\n",
      "loss 43.40324916124344\n",
      "loss 46.901152304410935\n",
      "loss 50.368536332845686\n",
      "loss 53.66287569284439\n",
      "loss 57.00443905234337\n",
      "loss 60.38093590497971\n",
      "loss 63.71827159166336\n",
      "loss 67.02196670651436\n",
      "loss 70.36042951703071\n",
      "loss 73.74945211887359\n",
      "loss 77.0059611082077\n",
      "loss 80.29337542176246\n",
      "loss 83.5908041024208\n",
      "loss 86.8038125026226\n",
      "loss 90.05780978083611\n",
      "loss 93.26596533060074\n",
      "loss 96.60402294039726\n",
      "loss 99.86274374485016\n",
      "loss 103.09920238852501\n",
      "loss 106.35397975325584\n",
      "loss 109.48641257166862\n",
      "loss 112.81045084834099\n",
      "loss 116.02230210900306\n",
      "loss 119.32984204173088\n",
      "loss 122.59455716609955\n",
      "loss 125.70769816040993\n",
      "loss 129.01144561767578\n",
      "loss 132.24659910678864\n",
      "loss 135.41269992470743\n",
      "loss 138.56494700431824\n",
      "loss 141.8037967658043\n",
      "loss 145.057118922472\n",
      "loss 148.26236859679221\n",
      "Epoch:  26\n",
      "Training loss =  3.3680813832580037\n",
      "Validation Loss: 0.8630\tTop 1 Validation Accuracy: 0.8156\t Top 5 Validation Accuracy: 0.9676\n",
      "loss 3.18532572388649\n",
      "loss 6.314026942253113\n",
      "loss 9.44382571697235\n",
      "loss 12.458497636318207\n",
      "loss 15.614029219150543\n",
      "loss 18.65436468720436\n",
      "loss 21.750947244167328\n",
      "loss 24.897922941446303\n",
      "loss 27.985369918346404\n",
      "loss 31.181996104717253\n",
      "loss 34.25987197995186\n",
      "loss 40.459645884037016\n",
      "loss 43.648951008319855\n",
      "loss 46.74217646718025\n",
      "loss 49.83398590087891\n",
      "loss 52.851090869903565\n",
      "loss 55.87939609766006\n",
      "loss 58.86603656291962\n",
      "loss 61.98302191734314\n",
      "loss 65.19971705317498\n",
      "loss 68.28552294373512\n",
      "loss 71.35215116977692\n",
      "loss 74.39042370080948\n",
      "loss 77.56191099286079\n",
      "loss 80.62797466039657\n",
      "loss 83.83993038058281\n",
      "loss 86.95761302232742\n",
      "loss 89.97927196264267\n",
      "loss 93.04722921133042\n",
      "loss 96.1176882660389\n",
      "loss 99.26031571984291\n",
      "loss 102.3315007853508\n",
      "loss 105.45200320839882\n",
      "loss 108.44827760457993\n",
      "loss 111.59696367740631\n",
      "loss 114.60451040506364\n",
      "loss 117.72179264903069\n",
      "loss 120.70696907162666\n",
      "loss 123.75828786611557\n",
      "loss 126.80286524891854\n",
      "loss 129.96251947522163\n",
      "loss 133.0404793047905\n",
      "loss 136.0660993516445\n",
      "Epoch:  27\n",
      "Training loss =  3.093053193019432\n",
      "Validation Loss: 0.7922\tTop 1 Validation Accuracy: 0.8243\t Top 5 Validation Accuracy: 0.9719\n",
      "loss 2.9701217544078826\n",
      "loss 5.959537967443466\n",
      "loss 8.906543382406234\n",
      "loss 11.919841058254242\n",
      "loss 15.02783900141716\n",
      "loss 17.990363388061525\n",
      "loss 20.948476126194\n",
      "loss 23.904840527772905\n",
      "loss 26.697169436216356\n",
      "loss 29.644312742948532\n",
      "loss 32.65766096115112\n",
      "loss 35.72984078407288\n",
      "loss 38.7803367626667\n",
      "loss 41.78075159788132\n",
      "loss 44.78913880109787\n",
      "loss 47.82717962503433\n",
      "loss 50.85840865373611\n",
      "loss 53.813171941041944\n",
      "loss 56.882994080781934\n",
      "loss 59.92489816904068\n",
      "loss 62.92500170707703\n",
      "loss 65.9344775724411\n",
      "loss 68.91651540398598\n",
      "loss 71.9860329258442\n",
      "loss 75.00077340841294\n",
      "loss 78.01140231370925\n",
      "loss 80.98837075710297\n",
      "loss 87.00610093355179\n",
      "loss 90.09029204249381\n",
      "loss 93.02172396183013\n",
      "loss 96.05075780153274\n",
      "loss 99.0433327805996\n",
      "loss 102.04577473878861\n",
      "loss 105.0186416053772\n",
      "loss 108.04292434334755\n",
      "loss 111.03882868885994\n",
      "loss 113.94803916096687\n",
      "loss 116.88488828063011\n",
      "loss 122.74198702216148\n",
      "loss 125.72686232209206\n",
      "loss 128.81929923415183\n",
      "loss 131.7312659919262\n",
      "Epoch:  28\n",
      "Training loss =  2.9949042879921253\n",
      "Validation Loss: 0.7319\tTop 1 Validation Accuracy: 0.8311\t Top 5 Validation Accuracy: 0.9751\n",
      "loss 2.8877267742156985\n",
      "loss 5.907319314479828\n",
      "loss 8.824774363040923\n",
      "loss 11.68919878602028\n",
      "loss 14.563272242546082\n",
      "loss 17.43492869734764\n",
      "loss 20.37156089782715\n",
      "loss 23.35060982823372\n",
      "loss 26.261235846281053\n",
      "loss 29.160610947608948\n",
      "loss 32.05863612294197\n",
      "loss 35.00021203517914\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 37.90208057045937\n",
      "loss 40.83367502450943\n",
      "loss 43.70565865278244\n",
      "loss 46.60970752358437\n",
      "loss 49.46948240280151\n",
      "loss 52.3632656621933\n",
      "loss 55.320475754737856\n",
      "loss 58.28649077653885\n",
      "loss 61.28649897694588\n",
      "loss 64.1960612499714\n",
      "loss 67.11724934458732\n",
      "loss 70.06665532827377\n",
      "loss 73.0618589758873\n",
      "loss 76.10833208441734\n",
      "loss 78.98527953267097\n",
      "loss 82.01469027400017\n",
      "loss 85.11183138608932\n",
      "loss 87.8894580066204\n",
      "loss 90.75431931972504\n",
      "loss 93.67554092049599\n",
      "loss 96.60903502583504\n",
      "loss 99.4691258406639\n",
      "loss 102.36995911955833\n",
      "loss 105.3359719991684\n",
      "loss 108.23646250128746\n",
      "loss 111.21786539912223\n",
      "loss 114.16545184135437\n",
      "loss 117.1703672158718\n",
      "loss 120.01424122214317\n",
      "loss 122.96647049307823\n",
      "loss 125.80624537706375\n",
      "loss 128.77355819225312\n",
      "Epoch:  29\n",
      "Training loss =  2.929318895923279\n",
      "Validation Loss: 0.6717\tTop 1 Validation Accuracy: 0.8349\t Top 5 Validation Accuracy: 0.9778\n",
      "loss 2.7644071161746977\n",
      "loss 5.594280009269714\n",
      "loss 8.42893278837204\n",
      "loss 11.219825687408447\n",
      "loss 14.06092323064804\n",
      "loss 16.95239837527275\n",
      "loss 19.86180813550949\n",
      "loss 22.840174157619476\n",
      "loss 25.754434486627577\n",
      "loss 28.62181916832924\n",
      "loss 31.546554839611055\n",
      "loss 34.431216121912\n",
      "loss 37.2399896299839\n",
      "loss 40.127297291755674\n",
      "loss 43.05775270462036\n",
      "loss 45.968107787370684\n",
      "loss 48.94529658436775\n",
      "loss 51.87371024131775\n",
      "loss 54.65969333350658\n",
      "loss 57.62571978628635\n",
      "loss 60.392388367056846\n",
      "loss 63.26371996819973\n",
      "loss 66.1192088097334\n",
      "loss 68.94838358819484\n",
      "loss 71.76908427417278\n",
      "loss 74.64492566645146\n",
      "loss 77.54562076747418\n",
      "loss 80.44345731198787\n",
      "loss 83.2350171750784\n",
      "loss 86.05597521483898\n",
      "loss 88.97887692511081\n",
      "loss 91.77601181805134\n",
      "loss 94.78709150850773\n",
      "loss 97.64125478565693\n",
      "loss 100.53281767427922\n",
      "loss 103.36807384312152\n",
      "loss 106.32838201701641\n",
      "loss 109.20561332762242\n",
      "loss 112.0457521957159\n",
      "loss 114.96216883003711\n",
      "loss 117.92946645200253\n",
      "loss 120.74159444272519\n",
      "loss 123.60699238955975\n",
      "loss 126.48880757510662\n",
      "Epoch:  30\n",
      "Training loss =  2.87574954534987\n",
      "Validation Loss: 0.7361\tTop 1 Validation Accuracy: 0.8337\t Top 5 Validation Accuracy: 0.9762\n",
      "loss 2.7984236490726473\n",
      "loss 5.6936039638519285\n",
      "loss 8.539451001882552\n",
      "loss 11.38223662018776\n",
      "loss 14.227729185819626\n",
      "loss 17.079762831926345\n",
      "loss 19.983927400708197\n",
      "loss 22.703555930256844\n",
      "loss 25.53975050628185\n",
      "loss 28.37046371102333\n",
      "loss 31.178705285787583\n",
      "loss 34.01842477917671\n",
      "loss 36.87602829217911\n",
      "loss 39.85055075764656\n",
      "loss 42.74903918504715\n",
      "loss 45.58842199981213\n",
      "loss 48.441618322730065\n",
      "loss 51.127939143180846\n",
      "loss 53.966906324625015\n",
      "loss 56.69897983431816\n",
      "loss 59.572237941026685\n",
      "loss 62.40224898815155\n",
      "loss 65.28465287566185\n",
      "loss 68.09372683405876\n",
      "loss 70.89125243544578\n",
      "loss 73.75292031764984\n",
      "loss 76.62756635904312\n",
      "loss 79.51624613642693\n",
      "loss 82.43249617099762\n",
      "loss 85.25964237093926\n",
      "loss 88.11118930220604\n",
      "loss 91.06189267158508\n",
      "loss 93.92907739758492\n",
      "loss 96.7901134300232\n",
      "loss 99.67643372654915\n",
      "loss 102.57048935174942\n",
      "loss 105.43701400160789\n",
      "loss 108.2199884903431\n",
      "loss 110.87141191601754\n",
      "loss 113.6778433740139\n",
      "loss 116.5480490386486\n",
      "loss 119.46433895111085\n",
      "loss 122.33583149433136\n",
      "loss 125.11227240383624\n",
      "Epoch:  31\n",
      "Training loss =  2.8430909330265317\n",
      "Validation Loss: 0.6684\tTop 1 Validation Accuracy: 0.8358\t Top 5 Validation Accuracy: 0.9775\n",
      "loss 2.701240659952164\n",
      "loss 5.46655349612236\n",
      "loss 8.207362296581268\n",
      "loss 10.92765057206154\n",
      "loss 13.738334842920302\n",
      "loss 16.652612369060517\n",
      "loss 19.370459922552108\n",
      "loss 22.16464184641838\n",
      "loss 24.927130500078203\n",
      "loss 27.71305329322815\n",
      "loss 30.49669070363045\n",
      "loss 33.279941058158876\n",
      "loss 36.095967378616336\n",
      "loss 38.94537136077881\n",
      "loss 41.7438408267498\n",
      "loss 44.57066006541252\n",
      "loss 47.37227883577347\n",
      "loss 50.209286868572235\n",
      "loss 52.98404166936874\n",
      "loss 55.76239109396934\n",
      "loss 58.60732525467873\n",
      "loss 61.426867446899415\n",
      "loss 64.22624184608459\n",
      "loss 67.09142512202263\n",
      "loss 69.89606034159661\n",
      "loss 72.665328463912\n",
      "loss 75.3848028856516\n",
      "loss 78.18034462749958\n",
      "loss 80.93563624620438\n",
      "loss 83.68187227964401\n",
      "loss 86.62614126563072\n",
      "loss 89.44913593888283\n",
      "loss 92.36127631425857\n",
      "loss 95.20478859305382\n",
      "loss 98.07970621526242\n",
      "loss 100.93029329001904\n",
      "loss 103.77559848248958\n",
      "loss 106.61243611395359\n",
      "loss 109.49952122747898\n",
      "loss 112.29936782062053\n",
      "loss 115.07574319899082\n",
      "loss 117.89942758500575\n",
      "loss 120.72697950661183\n",
      "loss 123.45350902974606\n",
      "Epoch:  32\n",
      "Training loss =  2.8053437458924666\n",
      "Validation Loss: 0.6260\tTop 1 Validation Accuracy: 0.8386\t Top 5 Validation Accuracy: 0.9789\n",
      "loss 2.7733907866477967\n",
      "loss 5.447943698167801\n",
      "loss 8.247119961977004\n",
      "loss 10.862609344720841\n",
      "loss 13.604185672998428\n",
      "loss 16.32554254412651\n",
      "loss 19.053076338768005\n",
      "loss 21.80757719874382\n",
      "loss 24.520863043069838\n",
      "loss 27.276516482830047\n",
      "loss 30.11474324941635\n",
      "loss 32.877044345140455\n",
      "loss 35.617476217746734\n",
      "loss 38.37529201984405\n",
      "loss 41.20691013455391\n",
      "loss 44.04854352235794\n",
      "loss 46.79259691596031\n",
      "loss 49.61616672933101\n",
      "loss 52.47370827019215\n",
      "loss 55.170455439686776\n",
      "loss 58.02267428338528\n",
      "loss 60.78775193035602\n",
      "loss 63.58917641818523\n",
      "loss 66.39136751115322\n",
      "loss 69.0824373692274\n",
      "loss 71.90140293419361\n",
      "loss 74.71799186885357\n",
      "loss 77.53106762826442\n",
      "loss 80.30654771506786\n",
      "loss 83.15947454988957\n",
      "loss 86.0305019211769\n",
      "loss 88.87678203225136\n",
      "loss 91.6241043639183\n",
      "loss 94.44882185578346\n",
      "loss 97.32501066923142\n",
      "loss 100.0292080038786\n",
      "loss 102.8449713474512\n",
      "loss 105.57819059431553\n",
      "loss 108.38548441767692\n",
      "loss 111.23094173669816\n",
      "loss 113.967429895401\n",
      "loss 116.76455688595772\n",
      "loss 119.55624911487102\n",
      "loss 122.3549034255743\n",
      "Epoch:  33\n",
      "Training loss =  2.7815981561450815\n",
      "Validation Loss: 0.6680\tTop 1 Validation Accuracy: 0.8363\t Top 5 Validation Accuracy: 0.9776\n",
      "loss 2.6446927756071092\n",
      "loss 5.284976577162743\n",
      "loss 8.095339990258218\n",
      "loss 10.832994423508644\n",
      "loss 13.556442596316337\n",
      "loss 16.2474996227026\n",
      "loss 19.01880197584629\n",
      "loss 21.81079192698002\n",
      "loss 24.449469222426416\n",
      "loss 27.22885269522667\n",
      "loss 30.04103142976761\n",
      "loss 32.79440194964409\n",
      "loss 35.55037246584892\n",
      "loss 38.3881570482254\n",
      "loss 41.13934505343437\n",
      "loss 43.8610966861248\n",
      "loss 46.64912218332291\n",
      "loss 49.41976867556572\n",
      "loss 54.85873744726181\n",
      "loss 57.57444916844368\n",
      "loss 60.29623928785324\n",
      "loss 63.024432972073555\n",
      "loss 65.77911037325859\n",
      "loss 68.53361598372459\n",
      "loss 71.35543255448341\n",
      "loss 74.15494904816151\n",
      "loss 76.82074314415455\n",
      "loss 79.63194291651249\n",
      "loss 82.33393107712268\n",
      "loss 85.09011062085628\n",
      "loss 87.78961863815785\n",
      "loss 90.54135037243367\n",
      "loss 93.24397870123386\n",
      "loss 96.03168562710285\n",
      "loss 98.8224734967947\n",
      "loss 101.6415221530199\n",
      "loss 104.39828881561756\n",
      "loss 107.18736217677593\n",
      "loss 110.0052450209856\n",
      "loss 112.75312205255031\n",
      "loss 115.54961081683636\n",
      "loss 118.34535070002079\n",
      "loss 121.09138509452343\n",
      "Epoch:  34\n",
      "Training loss =  2.755470171265852\n",
      "Validation Loss: 0.6245\tTop 1 Validation Accuracy: 0.8391\t Top 5 Validation Accuracy: 0.9786\n",
      "loss 2.752614107131958\n",
      "loss 5.427689003944397\n",
      "loss 8.18262684226036\n",
      "loss 10.955315982103349\n",
      "loss 13.638390840291978\n",
      "loss 16.28635582447052\n",
      "loss 18.97245787858963\n",
      "loss 21.706264194846153\n",
      "loss 24.462257711291315\n",
      "loss 27.14703605234623\n",
      "loss 29.890212998986243\n",
      "loss 32.509639634490014\n",
      "loss 35.2623731392622\n",
      "loss 38.00401350796223\n",
      "loss 40.78273885548115\n",
      "loss 43.532392986416816\n",
      "loss 46.177435079216956\n",
      "loss 48.92345539331436\n",
      "loss 51.55295838356018\n",
      "loss 54.32284313440323\n",
      "loss 57.14798154234886\n",
      "loss 59.92144981145859\n",
      "loss 62.64634373784065\n",
      "loss 65.33663010001183\n",
      "loss 68.10715492725372\n",
      "loss 70.80952179908752\n",
      "loss 73.52572958230972\n",
      "loss 76.13672096192836\n",
      "loss 78.91946374714375\n",
      "loss 81.6352774232626\n",
      "loss 84.41340508639813\n",
      "loss 87.18897196948528\n",
      "loss 89.84358811199665\n",
      "loss 92.63995926499366\n",
      "loss 95.39819284319877\n",
      "loss 98.0397593665123\n",
      "loss 100.8748532551527\n",
      "loss 103.71364068806172\n",
      "loss 106.4809011912346\n",
      "loss 109.21215427994728\n",
      "loss 111.94567490160465\n",
      "loss 114.69310360252857\n",
      "loss 117.4445482212305\n",
      "loss 120.17014737486839\n",
      "Epoch:  35\n",
      "Training loss =  2.7326120833265453\n",
      "Validation Loss: 0.6506\tTop 1 Validation Accuracy: 0.8390\t Top 5 Validation Accuracy: 0.9783\n",
      "loss 2.6209061694145204\n",
      "loss 5.36669659614563\n",
      "loss 7.951537035107613\n",
      "loss 10.677013543844224\n",
      "loss 13.355168961286545\n",
      "loss 16.020583232045173\n",
      "loss 18.73264433324337\n",
      "loss 21.395639372467993\n",
      "loss 24.04871784567833\n",
      "loss 26.64989737391472\n",
      "loss 29.426434581279754\n",
      "loss 32.17686768054962\n",
      "loss 34.845004580020905\n",
      "loss 37.59437657594681\n",
      "loss 40.32855303823948\n",
      "loss 43.002666082382206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 45.67476657152176\n",
      "loss 48.418527530431746\n",
      "loss 51.104400247335434\n",
      "loss 53.84021721839905\n",
      "loss 56.60169942140579\n",
      "loss 59.28528546631336\n",
      "loss 62.00195770561695\n",
      "loss 64.74788804590702\n",
      "loss 67.45308248698711\n",
      "loss 70.24745335400104\n",
      "loss 72.99458191633225\n",
      "loss 75.7723473227024\n",
      "loss 78.47130166590213\n",
      "loss 81.21963430583477\n",
      "loss 83.96171811759471\n",
      "loss 86.77840594351292\n",
      "loss 89.5064079350233\n",
      "loss 92.21410917699338\n",
      "loss 94.89904262959958\n",
      "loss 97.64159875452519\n",
      "loss 100.41261889517307\n",
      "loss 105.8934218376875\n",
      "loss 108.64275112450123\n",
      "loss 111.36711986780166\n",
      "loss 114.04922256946564\n",
      "loss 116.7903865134716\n",
      "loss 119.53111822962761\n",
      "Epoch:  36\n",
      "Training loss =  2.7177383076455692\n",
      "Validation Loss: 0.6178\tTop 1 Validation Accuracy: 0.8417\t Top 5 Validation Accuracy: 0.9796\n",
      "loss 2.568167542219162\n",
      "loss 5.214834780693054\n",
      "loss 7.870732927918434\n",
      "loss 13.259012675881387\n",
      "loss 15.798609568476676\n",
      "loss 18.468082496523856\n",
      "loss 21.21590773999691\n",
      "loss 23.835726373791694\n",
      "loss 26.56981711566448\n",
      "loss 29.35826560676098\n",
      "loss 32.100122698545455\n",
      "loss 34.73205119073391\n",
      "loss 37.40422330439091\n",
      "loss 40.06753280580044\n",
      "loss 42.79636973798275\n",
      "loss 45.50651949584484\n",
      "loss 48.21156838178635\n",
      "loss 50.932237033844\n",
      "loss 53.60994663715363\n",
      "loss 56.289834624528886\n",
      "loss 58.937066155076025\n",
      "loss 61.640517786741256\n",
      "loss 64.28077599525452\n",
      "loss 67.00692434191704\n",
      "loss 69.63051022410393\n",
      "loss 72.41733680307865\n",
      "loss 75.11158124864102\n",
      "loss 77.86548776805401\n",
      "loss 80.5511269390583\n",
      "loss 83.25668953239918\n",
      "loss 86.04660829365254\n",
      "loss 88.74239590108395\n",
      "loss 91.36187130153179\n",
      "loss 94.06935179531574\n",
      "loss 96.8055078703165\n",
      "loss 99.56587043821811\n",
      "loss 102.27428489983082\n",
      "loss 105.02320440530777\n",
      "loss 107.71744618773461\n",
      "loss 110.44468788027763\n",
      "loss 113.23634987592698\n",
      "loss 115.96602833986282\n",
      "loss 118.72253388941289\n",
      "Epoch:  37\n",
      "Training loss =  2.6990839911895788\n",
      "Validation Loss: 0.6854\tTop 1 Validation Accuracy: 0.8401\t Top 5 Validation Accuracy: 0.9792\n",
      "loss 2.6064610582590104\n",
      "loss 5.246102756857872\n",
      "loss 7.88892162501812\n",
      "loss 10.545623570084572\n",
      "loss 13.161636336445808\n",
      "loss 18.44225285112858\n",
      "loss 21.201210442185403\n",
      "loss 23.9554741024971\n",
      "loss 26.602737236618996\n",
      "loss 29.205683106780054\n",
      "loss 31.94294459283352\n",
      "loss 34.65219560682774\n",
      "loss 37.36388973414898\n",
      "loss 39.98838256776333\n",
      "loss 42.59322872459889\n",
      "loss 45.242343258261684\n",
      "loss 48.009008513092994\n",
      "loss 50.76565493226051\n",
      "loss 53.5668016576767\n",
      "loss 56.19393310010433\n",
      "loss 58.894381976127626\n",
      "loss 61.64915003657341\n",
      "loss 64.26970120429992\n",
      "loss 66.96344348669052\n",
      "loss 69.64843281030655\n",
      "loss 75.18997776389122\n",
      "loss 78.01494373679161\n",
      "loss 80.74934624850749\n",
      "loss 83.4590336304903\n",
      "loss 86.17472500562668\n",
      "loss 88.94485238075256\n",
      "loss 91.6330471944809\n",
      "loss 94.40357069849968\n",
      "loss 97.20700839161873\n",
      "loss 99.99156279802322\n",
      "loss 102.75266318440437\n",
      "loss 105.37513028621673\n",
      "loss 108.13663524985313\n",
      "loss 110.78868919551373\n",
      "loss 113.4638524478674\n",
      "loss 116.25599497258663\n",
      "loss 119.06423197209836\n",
      "Epoch:  38\n",
      "Training loss =  2.705010474398347\n",
      "Validation Loss: 0.6040\tTop 1 Validation Accuracy: 0.8393\t Top 5 Validation Accuracy: 0.9792\n",
      "loss 2.581408007144928\n",
      "loss 5.271161177158356\n",
      "loss 8.022396197319031\n",
      "loss 10.657077999711037\n",
      "loss 13.263652834296227\n",
      "loss 15.89981500327587\n",
      "loss 18.538740691542625\n",
      "loss 21.104677045345305\n",
      "loss 23.79390354514122\n",
      "loss 26.453632628917696\n",
      "loss 29.156379901766776\n",
      "loss 31.762547341585158\n",
      "loss 34.425883476734164\n",
      "loss 37.07921426534653\n",
      "loss 39.75866551160812\n",
      "loss 42.418532478809354\n",
      "loss 45.137253035306934\n",
      "loss 47.87976161837578\n",
      "loss 50.5748904800415\n",
      "loss 53.260919251441955\n",
      "loss 55.986724153757095\n",
      "loss 58.63624331593513\n",
      "loss 61.371294453144074\n",
      "loss 64.04274590373039\n",
      "loss 66.70138589560986\n",
      "loss 69.39530020773411\n",
      "loss 72.11959809899331\n",
      "loss 77.73184946298599\n",
      "loss 80.37665340125561\n",
      "loss 83.10249307572842\n",
      "loss 85.87968640863896\n",
      "loss 88.55188598692418\n",
      "loss 91.32185230851174\n",
      "loss 94.01771410942078\n",
      "loss 96.65365599155426\n",
      "loss 99.40375458359718\n",
      "loss 102.15681306064128\n",
      "loss 104.89414532840252\n",
      "loss 107.64156071245671\n",
      "loss 110.3818431264162\n",
      "loss 113.08405932843685\n",
      "loss 115.8594281166792\n",
      "loss 118.56176598489284\n",
      "Epoch:  39\n",
      "Training loss =  2.695750171598864\n",
      "Validation Loss: 0.6405\tTop 1 Validation Accuracy: 0.8361\t Top 5 Validation Accuracy: 0.9786\n",
      "loss 2.5824748206138612\n",
      "loss 5.145980680584907\n",
      "loss 7.733799616098404\n",
      "loss 10.407988010644912\n",
      "loss 13.066981421113015\n",
      "loss 15.707942602038383\n",
      "loss 18.375606226325036\n",
      "loss 21.00499577820301\n",
      "loss 23.641398520469664\n",
      "loss 26.267195606231688\n",
      "loss 28.856518958210945\n",
      "loss 31.558813381791115\n",
      "loss 34.269671550393106\n",
      "loss 36.87286480367184\n",
      "loss 39.58016075193882\n",
      "loss 42.18919545471668\n",
      "loss 44.825472764372826\n",
      "loss 47.477526730299\n",
      "loss 50.090347578525545\n",
      "loss 52.745951523780825\n",
      "loss 55.345321800708774\n",
      "loss 58.09219474077225\n",
      "loss 60.82046813845634\n",
      "loss 63.43918815135956\n",
      "loss 66.13589579224586\n",
      "loss 68.76092846035958\n",
      "loss 71.40855307877064\n",
      "loss 74.0681527376175\n",
      "loss 76.7867416381836\n",
      "loss 79.52975002884865\n",
      "loss 82.2512022203207\n",
      "loss 85.08502592265606\n",
      "loss 87.828167039752\n",
      "loss 90.56731150567532\n",
      "loss 93.30244684278965\n",
      "loss 95.83405973374843\n",
      "loss 98.61654462754727\n",
      "loss 101.32379820048808\n",
      "loss 104.02507488667965\n",
      "loss 106.72180597126484\n",
      "loss 109.45683092296123\n",
      "loss 112.1773541766405\n",
      "loss 114.89219344079494\n",
      "loss 117.5741258686781\n",
      "Epoch:  40\n",
      "Training loss =  2.6732600115244645\n",
      "Validation Loss: 0.6560\tTop 1 Validation Accuracy: 0.8384\t Top 5 Validation Accuracy: 0.9792\n",
      "loss 2.533317416906357\n",
      "loss 5.164945243597031\n",
      "loss 7.816815429925919\n",
      "loss 10.440407950282097\n",
      "loss 12.992021954655648\n",
      "loss 15.633141567111016\n",
      "loss 18.248582790493966\n",
      "loss 20.922714851498604\n",
      "loss 23.608537643551827\n",
      "loss 26.18900318682194\n",
      "loss 28.901339451670648\n",
      "loss 31.539650773406027\n",
      "loss 34.15572705864906\n",
      "loss 36.898870960474014\n",
      "loss 39.57063842535019\n",
      "loss 42.27429281115532\n",
      "loss 44.774092776179316\n",
      "loss 47.34380141317845\n",
      "loss 50.06411412656307\n",
      "loss 52.7010739260912\n",
      "loss 55.40384091973305\n",
      "loss 58.12287775635719\n",
      "loss 60.696220635175706\n",
      "loss 63.33585690498352\n",
      "loss 66.16175458788872\n",
      "loss 68.94032827019691\n",
      "loss 71.61909842252732\n",
      "loss 74.3036555737257\n",
      "loss 77.05289280116558\n",
      "loss 79.6913833528757\n",
      "loss 82.4593765538931\n",
      "loss 85.13717757880687\n",
      "loss 87.8391853839159\n",
      "loss 90.44477469861508\n",
      "loss 93.09840966522694\n",
      "loss 95.73428268015385\n",
      "loss 98.39870941340924\n",
      "loss 101.10243543684483\n",
      "loss 103.71113982260228\n",
      "loss 106.4787858492136\n",
      "loss 109.18196606040001\n",
      "loss 111.93118969082832\n",
      "loss 114.6553604221344\n",
      "loss 117.2975800561905\n",
      "Epoch:  41\n",
      "Training loss =  2.6670534560619807\n",
      "Validation Loss: 0.5741\tTop 1 Validation Accuracy: 0.8424\t Top 5 Validation Accuracy: 0.9804\n",
      "loss 2.6148698019981382\n",
      "loss 5.216557717323303\n",
      "loss 7.813908552527428\n",
      "loss 10.402616521716118\n",
      "loss 13.083124168515205\n",
      "loss 15.62931084871292\n",
      "loss 18.287847249507905\n",
      "loss 20.955139566659927\n",
      "loss 23.56908102750778\n",
      "loss 26.153313241004945\n",
      "loss 28.811208614110946\n",
      "loss 31.52307340502739\n",
      "loss 34.188028697371486\n",
      "loss 36.820589767098426\n",
      "loss 39.52759119689465\n",
      "loss 42.22819091975689\n",
      "loss 44.98583007991314\n",
      "loss 47.53624691665173\n",
      "loss 50.11001874089241\n",
      "loss 52.79164380311966\n",
      "loss 55.391359884738925\n",
      "loss 57.992748478651045\n",
      "loss 60.64097102046013\n",
      "loss 63.28020159006119\n",
      "loss 65.85535579025745\n",
      "loss 68.4954706376791\n",
      "loss 71.13392959833145\n",
      "loss 73.83385725736618\n",
      "loss 76.44171284615993\n",
      "loss 79.08136842250823\n",
      "loss 81.78273340404034\n",
      "loss 84.483765899539\n",
      "loss 87.13524305701256\n",
      "loss 89.7888570266962\n",
      "loss 92.4536230379343\n",
      "loss 95.24194314062595\n",
      "loss 98.00349638283252\n",
      "loss 100.67831430852414\n",
      "loss 103.36995240747929\n",
      "loss 106.09038299620151\n",
      "loss 108.81971879899501\n",
      "loss 111.55007563889026\n",
      "loss 114.33213955819606\n",
      "loss 116.9490146201849\n",
      "Epoch:  42\n",
      "Training loss =  2.6604588167276546\n",
      "Validation Loss: 0.6217\tTop 1 Validation Accuracy: 0.8368\t Top 5 Validation Accuracy: 0.9784\n",
      "loss 2.567521405220032\n",
      "loss 5.163074218034744\n",
      "loss 7.753902123570442\n",
      "loss 10.374285772442818\n",
      "loss 13.04398185312748\n",
      "loss 15.690575042366982\n",
      "loss 18.28244801223278\n",
      "loss 20.911420531868934\n",
      "loss 23.51265329837799\n",
      "loss 26.151536724567414\n",
      "loss 28.70322449028492\n",
      "loss 31.318049102425576\n",
      "loss 33.98063780248165\n",
      "loss 36.636595160365104\n",
      "loss 39.309169011116026\n",
      "loss 42.03304495453835\n",
      "loss 44.62067039012909\n",
      "loss 47.249443138241766\n",
      "loss 49.90118499815464\n",
      "loss 52.56099129080772\n",
      "loss 55.27901875019074\n",
      "loss 58.0138546872139\n",
      "loss 60.56118708848953\n",
      "loss 63.199183371663096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 65.84628677546978\n",
      "loss 68.4479652774334\n",
      "loss 71.10536099135875\n",
      "loss 73.8350112324953\n",
      "loss 76.55734017431736\n",
      "loss 79.21884930074215\n",
      "loss 81.96284476578235\n",
      "loss 84.68559818327427\n",
      "loss 87.38090904176235\n",
      "loss 90.04965003430843\n",
      "loss 92.72997390091419\n",
      "loss 95.41157945454121\n",
      "loss 98.14454315304756\n",
      "loss 100.84246354281902\n",
      "loss 103.52413902163505\n",
      "loss 106.22655855596065\n",
      "loss 108.88757456004619\n",
      "loss 111.60711936473847\n",
      "loss 114.33859100341797\n",
      "loss 116.94063013195992\n",
      "Epoch:  43\n",
      "Training loss =  2.6583213233159637\n",
      "Validation Loss: 0.6335\tTop 1 Validation Accuracy: 0.8374\t Top 5 Validation Accuracy: 0.9794\n",
      "loss 2.595043321847916\n",
      "loss 5.2170219898223875\n",
      "loss 7.635602994561196\n",
      "loss 10.265859306454658\n",
      "loss 12.798393484354019\n",
      "loss 15.395794385075568\n",
      "loss 18.09053598344326\n",
      "loss 20.662043261528016\n",
      "loss 23.31669985473156\n",
      "loss 25.975637483000757\n",
      "loss 28.56074731528759\n",
      "loss 31.13931814968586\n",
      "loss 33.745672369003294\n",
      "loss 36.37368000149727\n",
      "loss 38.892500956058505\n",
      "loss 41.57852087020874\n",
      "loss 44.264541058540345\n",
      "loss 46.83862246572971\n",
      "loss 49.35560262322426\n",
      "loss 52.001658399105075\n",
      "loss 54.71116747736931\n",
      "loss 57.32973247289657\n",
      "loss 59.98971872091293\n",
      "loss 62.69534695863724\n",
      "loss 65.35080038487911\n",
      "loss 68.0636580234766\n",
      "loss 70.73166981637478\n",
      "loss 73.38080638349057\n",
      "loss 76.08416623950005\n",
      "loss 78.83581014156341\n",
      "loss 81.54364094495773\n",
      "loss 84.25732749640942\n",
      "loss 87.00338029921055\n",
      "loss 89.67643256485462\n",
      "loss 92.40773663938046\n",
      "loss 95.02673877596855\n",
      "loss 97.77208887457847\n",
      "loss 100.52959699630738\n",
      "loss 103.17673709332944\n",
      "loss 105.92670188486576\n",
      "loss 108.65767557561398\n",
      "loss 111.19105819225311\n",
      "loss 113.93807883024216\n",
      "loss 116.56623104095459\n",
      "Epoch:  44\n",
      "Training loss =  2.6500658623845705\n",
      "Validation Loss: 0.6032\tTop 1 Validation Accuracy: 0.8386\t Top 5 Validation Accuracy: 0.9784\n",
      "loss 2.557653297781944\n",
      "loss 5.109416453242302\n",
      "loss 7.584992015957832\n",
      "loss 10.128592536449432\n",
      "loss 12.697733788490295\n",
      "loss 15.339862949848175\n",
      "loss 17.885613504052163\n",
      "loss 20.479617205262183\n",
      "loss 23.145726423859596\n",
      "loss 25.796791982650756\n",
      "loss 28.48201504409313\n",
      "loss 31.077632278203964\n",
      "loss 33.7132882797718\n",
      "loss 36.36921136081219\n",
      "loss 38.95169972121715\n",
      "loss 41.545497599244115\n",
      "loss 44.18855830848217\n",
      "loss 46.81306877732277\n",
      "loss 49.3779839682579\n",
      "loss 52.0056176763773\n",
      "loss 54.64544990479946\n",
      "loss 57.23030760347843\n",
      "loss 59.841612998843196\n",
      "loss 62.47729036927223\n",
      "loss 65.05940110683441\n",
      "loss 67.63812655329704\n",
      "loss 70.27340556621552\n",
      "loss 72.9851549243927\n",
      "loss 75.43765614271165\n",
      "loss 78.04234375\n",
      "loss 80.65009818375111\n",
      "loss 83.24372185587883\n",
      "loss 85.941926689744\n",
      "loss 88.57148207902908\n",
      "loss 91.29932155311107\n",
      "loss 93.91897525012493\n",
      "loss 96.66110366880893\n",
      "loss 99.35626580059528\n",
      "loss 102.01397734463215\n",
      "loss 104.66205201625824\n",
      "loss 107.34587199926376\n",
      "loss 110.05944874584675\n",
      "loss 112.70614944517612\n",
      "loss 115.42923842728138\n",
      "Epoch:  45\n",
      "Training loss =  2.62388151036988\n",
      "Validation Loss: 0.6716\tTop 1 Validation Accuracy: 0.8382\t Top 5 Validation Accuracy: 0.9791\n",
      "loss 2.576370482444763\n",
      "loss 5.223272188901901\n",
      "loss 7.80414234161377\n",
      "loss 10.365348275899887\n",
      "loss 13.010098626017571\n",
      "loss 15.607024648785591\n",
      "loss 18.25082121014595\n",
      "loss 20.77971554040909\n",
      "loss 23.229177365899087\n",
      "loss 25.813539808392523\n",
      "loss 28.433754189014437\n",
      "loss 31.081092132329943\n",
      "loss 33.66831553459168\n",
      "loss 36.242194301486016\n",
      "loss 38.90050655066967\n",
      "loss 41.456419683098794\n",
      "loss 44.04598095059395\n",
      "loss 46.61152953028679\n",
      "loss 49.1547081309557\n",
      "loss 51.80653825521469\n",
      "loss 54.40916358113289\n",
      "loss 57.02055375933647\n",
      "loss 59.60224869608879\n",
      "loss 62.13282263159752\n",
      "loss 64.88442901849747\n",
      "loss 67.60364778757095\n",
      "loss 70.32877891778946\n",
      "loss 72.9527824807167\n",
      "loss 75.59754121601581\n",
      "loss 78.23772318601608\n",
      "loss 80.9283812713623\n",
      "loss 83.59742208480834\n",
      "loss 86.30121444821357\n",
      "loss 88.98621645092965\n",
      "loss 91.73691439032555\n",
      "loss 94.36815821528435\n",
      "loss 96.96208587229252\n",
      "loss 99.56525146305562\n",
      "loss 102.28973668038844\n",
      "loss 104.9841620272398\n",
      "loss 107.55370989739895\n",
      "loss 110.22305516302586\n",
      "loss 112.84064514338971\n",
      "loss 115.45453221380711\n",
      "Epoch:  46\n",
      "Training loss =  2.624878582818765\n",
      "Validation Loss: 0.6352\tTop 1 Validation Accuracy: 0.8369\t Top 5 Validation Accuracy: 0.9790\n",
      "loss 2.479236027002335\n",
      "loss 5.027039527893066\n",
      "loss 7.555073044300079\n",
      "loss 10.05822192132473\n",
      "loss 12.717308226227761\n",
      "loss 15.205134066939355\n",
      "loss 17.767685728669168\n",
      "loss 20.45168465554714\n",
      "loss 23.151563301682472\n",
      "loss 25.752219167351722\n",
      "loss 28.315545718073846\n",
      "loss 30.898712173104286\n",
      "loss 33.53527495086193\n",
      "loss 36.14084998428822\n",
      "loss 38.90391720473767\n",
      "loss 41.45270773589611\n",
      "loss 44.03475638329983\n",
      "loss 46.63684611201286\n",
      "loss 49.261805931329725\n",
      "loss 51.84054199755192\n",
      "loss 54.41864702284336\n",
      "loss 56.950917887091634\n",
      "loss 59.53505006968975\n",
      "loss 62.15996044933796\n",
      "loss 64.78965685129165\n",
      "loss 67.42780376136302\n",
      "loss 70.02898373663426\n",
      "loss 72.6950337433815\n",
      "loss 75.3330448526144\n",
      "loss 77.97919860184193\n",
      "loss 80.63499017894269\n",
      "loss 83.3167807918787\n",
      "loss 85.97691289961338\n",
      "loss 88.67190822184085\n",
      "loss 91.31749349176884\n",
      "loss 94.01761094391345\n",
      "loss 96.7582786399126\n",
      "loss 102.1778566646576\n",
      "loss 104.78013545334339\n",
      "loss 107.43933050096035\n",
      "loss 110.15885388433934\n",
      "loss 112.85933948576451\n",
      "loss 115.6098692715168\n",
      "Epoch:  47\n",
      "Training loss =  2.6274010100363614\n",
      "Validation Loss: 0.6936\tTop 1 Validation Accuracy: 0.8348\t Top 5 Validation Accuracy: 0.9767\n",
      "loss 2.4537800800800325\n",
      "loss 4.970877879858017\n",
      "loss 7.476131048798561\n",
      "loss 10.025028034448624\n",
      "loss 12.569362540245056\n",
      "loss 15.181447043418885\n",
      "loss 17.76294718801975\n",
      "loss 20.425854652523995\n",
      "loss 22.870658197402953\n",
      "loss 25.50886794090271\n",
      "loss 28.17575252711773\n",
      "loss 30.801521490216256\n",
      "loss 33.36186038374901\n",
      "loss 35.97228121817112\n",
      "loss 38.56422334969044\n",
      "loss 41.168779308795926\n",
      "loss 43.74600069046021\n",
      "loss 46.37741883635521\n",
      "loss 49.00267304301262\n",
      "loss 51.539677815437315\n",
      "loss 54.2256389605999\n",
      "loss 56.790556638240815\n",
      "loss 59.39390424847603\n",
      "loss 62.02748964548111\n",
      "loss 64.74854606866836\n",
      "loss 67.303825725317\n",
      "loss 69.97613220751286\n",
      "loss 72.58932023704052\n",
      "loss 75.18214766681194\n",
      "loss 77.89612653017043\n",
      "loss 80.54254797339439\n",
      "loss 83.1572994339466\n",
      "loss 85.74786773502827\n",
      "loss 88.43168650925159\n",
      "loss 91.10084620893002\n",
      "loss 93.82995302140712\n",
      "loss 96.47695262670517\n",
      "loss 99.18930451154709\n",
      "loss 101.82828238487244\n",
      "loss 104.52208585619927\n",
      "loss 107.196280721426\n",
      "loss 109.82605535507201\n",
      "loss 112.4212083619833\n",
      "loss 115.09550407588482\n",
      "Epoch:  48\n",
      "Training loss =  2.6170911294893386\n",
      "Validation Loss: 0.6345\tTop 1 Validation Accuracy: 0.8349\t Top 5 Validation Accuracy: 0.9770\n",
      "loss 2.5669745022058486\n",
      "loss 5.07947612285614\n",
      "loss 7.6359188580513\n",
      "loss 10.181914457678795\n",
      "loss 12.625355551838874\n",
      "loss 15.215037774443626\n",
      "loss 17.712811461091043\n",
      "loss 20.313619089722632\n",
      "loss 22.870455896258353\n",
      "loss 25.471702525019644\n",
      "loss 28.104831315875053\n",
      "loss 30.7332456189394\n",
      "loss 33.37176265537739\n",
      "loss 35.83378156781197\n",
      "loss 38.44518447637558\n",
      "loss 41.03464443266392\n",
      "loss 43.65859229326248\n",
      "loss 46.20001645207405\n",
      "loss 48.85975286245346\n",
      "loss 51.51573829054833\n",
      "loss 54.05360035419464\n",
      "loss 56.68100484251976\n",
      "loss 59.248928689956664\n",
      "loss 61.857891076803206\n",
      "loss 64.58505509138108\n",
      "loss 67.20690504550934\n",
      "loss 69.90314987182617\n",
      "loss 72.59004169225693\n",
      "loss 75.22815549015999\n",
      "loss 77.87566516757012\n",
      "loss 80.48144345343113\n",
      "loss 83.09982695519925\n",
      "loss 85.74256478250027\n",
      "loss 88.37979554891587\n",
      "loss 91.09795059084892\n",
      "loss 93.72362694382667\n",
      "loss 96.386292578578\n",
      "loss 99.0617987960577\n",
      "loss 101.72231079101563\n",
      "loss 104.46935716629028\n",
      "loss 107.25175225257874\n",
      "loss 109.98342519938946\n",
      "loss 112.66274256408215\n",
      "loss 115.31344723522663\n",
      "Epoch:  49\n",
      "Training loss =  2.621643832232782\n",
      "Validation Loss: 0.5977\tTop 1 Validation Accuracy: 0.8380\t Top 5 Validation Accuracy: 0.9790\n",
      "loss 2.4822038018703463\n",
      "loss 5.116222687959671\n",
      "loss 7.686266834139824\n",
      "loss 10.31001146018505\n",
      "loss 12.852193205356597\n",
      "loss 15.370874031186103\n",
      "loss 17.95900597333908\n",
      "loss 20.555571941733362\n",
      "loss 23.14231534063816\n",
      "loss 25.683910116553307\n",
      "loss 28.267384718060494\n",
      "loss 30.713452193140984\n",
      "loss 33.236534941792485\n",
      "loss 35.75515227794647\n",
      "loss 38.3688767260313\n",
      "loss 40.93541321218014\n",
      "loss 43.54843108594417\n",
      "loss 46.19754744946957\n",
      "loss 48.84726479947567\n",
      "loss 51.47308390378952\n",
      "loss 54.086561123132704\n",
      "loss 56.62318343997001\n",
      "loss 59.16763672053814\n",
      "loss 61.82421396493912\n",
      "loss 64.46943206191062\n",
      "loss 67.11529257774353\n",
      "loss 69.69711569547653\n",
      "loss 72.28944146454334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 74.89277774870396\n",
      "loss 77.54442899405956\n",
      "loss 80.16735058367253\n",
      "loss 82.7943795055151\n",
      "loss 85.4713058435917\n",
      "loss 88.09261323451996\n",
      "loss 90.77033429145813\n",
      "loss 93.39368955433369\n",
      "loss 96.11787779688835\n",
      "loss 98.75661636173726\n",
      "loss 101.31262874424458\n",
      "loss 103.96311559319496\n",
      "loss 106.59576551437378\n",
      "loss 109.28219450235366\n",
      "loss 111.99024011135101\n",
      "loss 114.58450907111168\n",
      "Epoch:  50\n",
      "Training loss =  2.6053315539667787\n",
      "Validation Loss: 0.6838\tTop 1 Validation Accuracy: 0.8380\t Top 5 Validation Accuracy: 0.9783\n",
      "loss 2.5318247121572495\n",
      "loss 4.9665745288133625\n",
      "loss 7.2927101659774785\n",
      "loss 9.755985733270645\n",
      "loss 12.178153026103974\n",
      "loss 14.480857944488525\n",
      "loss 16.83017945766449\n",
      "loss 19.159904502630233\n",
      "loss 21.502991785407065\n",
      "loss 23.79564769089222\n",
      "loss 26.085314978957175\n",
      "loss 30.82130898952484\n",
      "loss 33.19645878791809\n",
      "loss 35.54029342949391\n",
      "loss 37.83542161226273\n",
      "loss 40.16856060087681\n",
      "loss 42.47323167622089\n",
      "loss 44.80263684809208\n",
      "loss 47.072119345068934\n",
      "loss 49.45823809325695\n",
      "loss 51.755268815159795\n",
      "loss 54.080435253977775\n",
      "loss 56.32369023561478\n",
      "loss 58.639938450455666\n",
      "loss 60.91720191657543\n",
      "loss 63.24607944846153\n",
      "loss 65.47532675504685\n",
      "loss 67.7279425305128\n",
      "loss 70.11430659294129\n",
      "loss 72.45333549976348\n",
      "loss 74.68468227505684\n",
      "loss 76.91712580800056\n",
      "loss 79.15500250697136\n",
      "loss 81.41572384417057\n",
      "loss 83.63683833539486\n",
      "loss 85.91295708179474\n",
      "loss 88.19675268888473\n",
      "loss 90.3929465329647\n",
      "loss 92.64420748472213\n",
      "loss 94.86771186113357\n",
      "loss 97.15175060987472\n",
      "loss 99.35901878923178\n",
      "loss 101.580740750134\n",
      "Epoch:  51\n",
      "Training loss =  2.3077591857817406\n",
      "Validation Loss: 0.5210\tTop 1 Validation Accuracy: 0.8579\t Top 5 Validation Accuracy: 0.9842\n",
      "loss 2.2657988595962526\n",
      "loss 4.478619609475135\n",
      "loss 6.700573307871818\n",
      "loss 8.900333623886109\n",
      "loss 11.050955095887184\n",
      "loss 13.289653589725495\n",
      "loss 15.481775314807892\n",
      "loss 17.762061104774475\n",
      "loss 19.99817514240742\n",
      "loss 22.15821655392647\n",
      "loss 24.30017449975014\n",
      "loss 26.555274772942067\n",
      "loss 28.69876501470804\n",
      "loss 30.959889277517796\n",
      "loss 33.223731273710726\n",
      "loss 35.47574632436037\n",
      "loss 37.718518421947955\n",
      "loss 39.955333431065085\n",
      "loss 42.20563863784075\n",
      "loss 44.366646373569964\n",
      "loss 46.532647091448304\n",
      "loss 48.86677866846323\n",
      "loss 51.12996470600367\n",
      "loss 53.324576070010664\n",
      "loss 55.569060313403604\n",
      "loss 57.8267719951272\n",
      "loss 60.061798616945744\n",
      "loss 62.31298011809587\n",
      "loss 64.5750289735198\n",
      "loss 66.85103061169386\n",
      "loss 69.12356800496578\n",
      "loss 71.39102243959904\n",
      "loss 73.64949515789748\n",
      "loss 75.91597174018621\n",
      "loss 78.09064175277949\n",
      "loss 80.10739912956953\n",
      "loss 82.33455180376768\n",
      "loss 84.41292075961829\n",
      "loss 86.63668613284827\n",
      "loss 88.84185531109571\n",
      "loss 91.0386024531722\n",
      "loss 93.23696765452624\n",
      "loss 95.43108497768641\n",
      "loss 97.71616740971804\n",
      "Epoch:  52\n",
      "Training loss =  2.2205693914846325\n",
      "Validation Loss: 0.5242\tTop 1 Validation Accuracy: 0.8579\t Top 5 Validation Accuracy: 0.9844\n",
      "loss 2.2232803636789322\n",
      "loss 4.388614277243614\n",
      "loss 6.62190570294857\n",
      "loss 8.846550284028053\n",
      "loss 11.021878702044487\n",
      "loss 13.282836236357689\n",
      "loss 15.466692510247231\n",
      "loss 17.627444168925287\n",
      "loss 19.763862794041632\n",
      "loss 21.936437826156617\n",
      "loss 24.244546226263047\n",
      "loss 26.455519745349886\n",
      "loss 28.65675420820713\n",
      "loss 30.895127934217452\n",
      "loss 33.11938046514988\n",
      "loss 35.33831787109375\n",
      "loss 37.50783814907074\n",
      "loss 39.76399949729443\n",
      "loss 41.95816546201706\n",
      "loss 44.1904285466671\n",
      "loss 46.37172487974167\n",
      "loss 48.60596552610397\n",
      "loss 52.969061794281004\n",
      "loss 55.06675924479961\n",
      "loss 57.29503392219544\n",
      "loss 59.49267450273037\n",
      "loss 61.77729167521\n",
      "loss 63.9741657435894\n",
      "loss 66.1308526802063\n",
      "loss 68.31516743719578\n",
      "loss 70.46240919351578\n",
      "loss 72.65639822602272\n",
      "loss 74.86557597815991\n",
      "loss 77.13422222197056\n",
      "loss 79.31910585463046\n",
      "loss 81.58389424741269\n",
      "loss 83.80387628376484\n",
      "loss 85.98880298018456\n",
      "loss 88.19134750068187\n",
      "loss 90.31595932304859\n",
      "loss 92.39344520568848\n",
      "loss 94.59552938282489\n",
      "loss 96.74682339251041\n",
      "Epoch:  53\n",
      "Training loss =  2.198705039641979\n",
      "Validation Loss: 0.4853\tTop 1 Validation Accuracy: 0.8629\t Top 5 Validation Accuracy: 0.9859\n",
      "loss 2.1673244804143907\n",
      "loss 4.324733380973339\n",
      "loss 6.455465314090252\n",
      "loss 8.596439283788204\n",
      "loss 10.752322364747524\n",
      "loss 12.979407436549664\n",
      "loss 15.10088245242834\n",
      "loss 17.27759086817503\n",
      "loss 19.398353980481623\n",
      "loss 21.563977785110474\n",
      "loss 23.69818475961685\n",
      "loss 25.87846653521061\n",
      "loss 28.052934495210646\n",
      "loss 30.202643477916716\n",
      "loss 32.392499846816065\n",
      "loss 34.51917340815067\n",
      "loss 36.58611060619354\n",
      "loss 38.78591161668301\n",
      "loss 41.03851276159286\n",
      "loss 43.22021728187799\n",
      "loss 45.409848956763746\n",
      "loss 47.532414905130864\n",
      "loss 49.718904040157796\n",
      "loss 51.94970171004534\n",
      "loss 54.172136967480185\n",
      "loss 56.32256329208612\n",
      "loss 58.498117565214635\n",
      "loss 60.608133246004584\n",
      "loss 62.86179745703936\n",
      "loss 65.04292612224818\n",
      "loss 67.23822713911534\n",
      "loss 69.35157957673073\n",
      "loss 71.52733069837093\n",
      "loss 73.68813814401626\n",
      "loss 75.95114302814007\n",
      "loss 78.11852890431881\n",
      "loss 80.34088107526303\n",
      "loss 82.51799607634544\n",
      "loss 84.63740440160036\n",
      "loss 86.74183765858412\n",
      "loss 88.90690098017454\n",
      "loss 91.04564406424761\n",
      "loss 93.24648819357157\n",
      "loss 95.4535576620698\n",
      "Epoch:  54\n",
      "Training loss =  2.1694232796601347\n",
      "Validation Loss: 0.5547\tTop 1 Validation Accuracy: 0.8577\t Top 5 Validation Accuracy: 0.9844\n",
      "loss 2.1535994324088095\n",
      "loss 4.219665857851505\n",
      "loss 6.320551434457302\n",
      "loss 8.49672935038805\n",
      "loss 10.665179962217808\n",
      "loss 12.864660902321338\n",
      "loss 15.072840823829175\n",
      "loss 17.236337942183017\n",
      "loss 19.358391495645048\n",
      "loss 21.444104131162167\n",
      "loss 23.667538625895975\n",
      "loss 25.790507052242756\n",
      "loss 27.80652085274458\n",
      "loss 30.035654015243054\n",
      "loss 32.25286826819181\n",
      "loss 34.442794080674645\n",
      "loss 36.52597690671682\n",
      "loss 38.715074821412564\n",
      "loss 40.882361581623556\n",
      "loss 43.05685759335756\n",
      "loss 45.17204578310251\n",
      "loss 47.32229192703962\n",
      "loss 49.53424062877893\n",
      "loss 51.72762038499117\n",
      "loss 53.92043941289187\n",
      "loss 56.14436601549387\n",
      "loss 58.32315845221281\n",
      "loss 60.34675761401653\n",
      "loss 62.559041184782984\n",
      "loss 64.73073798805476\n",
      "loss 66.88744298309088\n",
      "loss 69.01783352166414\n",
      "loss 71.18620887309312\n",
      "loss 73.3617924323678\n",
      "loss 75.45689271897078\n",
      "loss 77.66798794955015\n",
      "loss 79.78686475306749\n",
      "loss 81.89964435666799\n",
      "loss 84.05979525357485\n",
      "loss 86.19756294697522\n",
      "loss 88.35607568770646\n",
      "loss 90.46553732007742\n",
      "loss 92.54419431298972\n",
      "loss 94.75430552452802\n",
      "Epoch:  55\n",
      "Training loss =  2.1536062399684806\n",
      "Validation Loss: 0.4906\tTop 1 Validation Accuracy: 0.8607\t Top 5 Validation Accuracy: 0.9856\n",
      "loss 2.1522232496738436\n",
      "loss 4.332022882699967\n",
      "loss 6.489716584682465\n",
      "loss 8.68378867983818\n",
      "loss 10.872355905771256\n",
      "loss 13.011054297089578\n",
      "loss 15.173706671595573\n",
      "loss 17.25231399357319\n",
      "loss 19.462041490674018\n",
      "loss 21.450591343939305\n",
      "loss 23.527835250794887\n",
      "loss 25.680596693456174\n",
      "loss 27.785741861760616\n",
      "loss 29.97372488230467\n",
      "loss 32.16803692728281\n",
      "loss 34.312771328389644\n",
      "loss 36.417542748749256\n",
      "loss 38.55612864226103\n",
      "loss 40.69160393089056\n",
      "loss 42.827569674849514\n",
      "loss 44.97650967478752\n",
      "loss 47.100027309060096\n",
      "loss 49.2573779541254\n",
      "loss 51.453835085630416\n",
      "loss 53.617612630724906\n",
      "loss 55.73349978744984\n",
      "loss 57.88960012257099\n",
      "loss 60.075224215388296\n",
      "loss 62.166032882928846\n",
      "loss 64.38845427155495\n",
      "loss 66.52743172287941\n",
      "loss 68.70847739756107\n",
      "loss 70.92853170454502\n",
      "loss 73.01833409786224\n",
      "loss 75.10857621490955\n",
      "loss 77.24668615400792\n",
      "loss 79.32308902561664\n",
      "loss 81.46828391432763\n",
      "loss 83.55660078585147\n",
      "loss 85.72201665818692\n",
      "loss 87.7507660767436\n",
      "loss 89.84978657871484\n",
      "loss 91.95921620160341\n",
      "loss 94.07055112034082\n",
      "Epoch:  56\n",
      "Training loss =  2.138498269850843\n",
      "Validation Loss: 0.5036\tTop 1 Validation Accuracy: 0.8605\t Top 5 Validation Accuracy: 0.9854\n",
      "loss 2.117503364086151\n",
      "loss 4.217519105672836\n",
      "loss 6.382751327753067\n",
      "loss 8.460425849556923\n",
      "loss 10.566030854582786\n",
      "loss 12.658898810148239\n",
      "loss 14.710622135996818\n",
      "loss 16.825748067498207\n",
      "loss 19.009150916337965\n",
      "loss 21.16246206730604\n",
      "loss 23.163411243259908\n",
      "loss 25.296465809047223\n",
      "loss 27.487565399110316\n",
      "loss 29.50079652518034\n",
      "loss 31.58173637241125\n",
      "loss 33.71669525980949\n",
      "loss 35.85500184059143\n",
      "loss 38.03780598223209\n",
      "loss 40.168325898051265\n",
      "loss 42.30169780552387\n",
      "loss 44.3983219987154\n",
      "loss 46.50451976299286\n",
      "loss 48.66181815981865\n",
      "loss 50.76526727497578\n",
      "loss 52.925120251774786\n",
      "loss 55.059142597317695\n",
      "loss 57.187216208577155\n",
      "loss 59.34370385468006\n",
      "loss 61.465113377273084\n",
      "loss 63.67015520364046\n",
      "loss 65.74118992865085\n",
      "loss 67.94822106838227\n",
      "loss 70.0860691422224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 72.16702068626881\n",
      "loss 74.35210218846798\n",
      "loss 76.57045969963073\n",
      "loss 78.73428349256515\n",
      "loss 80.8826809823513\n",
      "loss 83.01938785672188\n",
      "loss 85.18008964538575\n",
      "loss 87.31121177136897\n",
      "loss 89.49775310575961\n",
      "loss 91.70685376763343\n",
      "loss 93.85515528082847\n",
      "Epoch:  57\n",
      "Training loss =  2.133955674088883\n",
      "Validation Loss: 0.5025\tTop 1 Validation Accuracy: 0.8597\t Top 5 Validation Accuracy: 0.9851\n",
      "loss 2.0816297924518583\n",
      "loss 4.162449315786362\n",
      "loss 6.290139870643616\n",
      "loss 8.450630774199963\n",
      "loss 10.501224434673786\n",
      "loss 12.608449474275112\n",
      "loss 14.70483366459608\n",
      "loss 16.88568266004324\n",
      "loss 19.016090667545797\n",
      "loss 21.106310916244983\n",
      "loss 23.207689813673497\n",
      "loss 25.373729245960714\n",
      "loss 27.448150524199008\n",
      "loss 29.521905079185963\n",
      "loss 31.6037876072526\n",
      "loss 33.76920799165964\n",
      "loss 35.86381867259741\n",
      "loss 37.95163370341063\n",
      "loss 40.07324938327074\n",
      "loss 42.265086535513404\n",
      "loss 44.36877993375063\n",
      "loss 46.44513929933309\n",
      "loss 48.60999481111765\n",
      "loss 50.77315601319074\n",
      "loss 52.94485437899828\n",
      "loss 55.04061619609595\n",
      "loss 57.15084863096476\n",
      "loss 59.24305591017008\n",
      "loss 61.35417539566755\n",
      "loss 63.44598195284605\n",
      "loss 65.56316592246294\n",
      "loss 67.62324190288783\n",
      "loss 69.78218371599912\n",
      "loss 71.9981737780571\n",
      "loss 74.13112699985504\n",
      "loss 76.31083425700665\n",
      "loss 78.45965349376202\n",
      "loss 80.56916289806367\n",
      "loss 82.71192179083825\n",
      "loss 84.85885021924973\n",
      "loss 86.94012796044349\n",
      "loss 88.99408821463585\n",
      "loss 91.02560365080834\n",
      "loss 93.1207743382454\n",
      "Epoch:  58\n",
      "Training loss =  2.117149248521516\n",
      "Validation Loss: 0.5364\tTop 1 Validation Accuracy: 0.8602\t Top 5 Validation Accuracy: 0.9854\n",
      "loss 2.1783755415678026\n",
      "loss 4.2621239069104195\n",
      "loss 6.419896571338176\n",
      "loss 8.497229507863521\n",
      "loss 10.532149294316769\n",
      "loss 12.678843528926372\n",
      "loss 14.763229193389416\n",
      "loss 16.90160866647959\n",
      "loss 19.034180455505847\n",
      "loss 20.996247434318065\n",
      "loss 23.12963641256094\n",
      "loss 25.213699539005756\n",
      "loss 27.32979952186346\n",
      "loss 29.430411994159222\n",
      "loss 31.456280029118062\n",
      "loss 33.557068379223345\n",
      "loss 35.665498073399064\n",
      "loss 37.841756949722765\n",
      "loss 40.00147944897413\n",
      "loss 42.16285780161619\n",
      "loss 44.254533503949645\n",
      "loss 46.34718538850546\n",
      "loss 48.4084484025836\n",
      "loss 50.49270927101374\n",
      "loss 52.56056897312403\n",
      "loss 54.653702878654\n",
      "loss 56.74616476625204\n",
      "loss 61.07935031205416\n",
      "loss 63.21352664142847\n",
      "loss 65.31169237405062\n",
      "loss 67.35750086635352\n",
      "loss 69.50803766459227\n",
      "loss 71.66371150821448\n",
      "loss 73.88206751734019\n",
      "loss 75.96631646901369\n",
      "loss 78.13341924160719\n",
      "loss 80.22211614638567\n",
      "loss 82.41598374873399\n",
      "loss 84.51662760466337\n",
      "loss 86.54240459740161\n",
      "loss 88.7239479470253\n",
      "loss 90.89840413689613\n",
      "loss 92.99513000369072\n",
      "Epoch:  59\n",
      "Training loss =  2.113791178391707\n",
      "Validation Loss: 0.4977\tTop 1 Validation Accuracy: 0.8618\t Top 5 Validation Accuracy: 0.9858\n",
      "loss 2.043704068660736\n",
      "loss 4.124391576349735\n",
      "loss 6.214635101854801\n",
      "loss 8.277766276299953\n",
      "loss 10.382833640873432\n",
      "loss 12.459664489328862\n",
      "loss 14.49677843183279\n",
      "loss 16.559915802180768\n",
      "loss 18.62214254796505\n",
      "loss 20.722528287768363\n",
      "loss 22.88980889648199\n",
      "loss 24.98399833470583\n",
      "loss 27.13167934268713\n",
      "loss 29.251438185870647\n",
      "loss 31.262168351113797\n",
      "loss 33.31519044995308\n",
      "loss 35.41078442245722\n",
      "loss 37.49112541168928\n",
      "loss 39.628396411240104\n",
      "loss 41.70484351247549\n",
      "loss 43.766317267715934\n",
      "loss 45.87549105554819\n",
      "loss 48.00887151569128\n",
      "loss 50.08462603479624\n",
      "loss 52.1602158126235\n",
      "loss 54.29825400620699\n",
      "loss 56.37242231398821\n",
      "loss 58.423259680569174\n",
      "loss 60.60310163348913\n",
      "loss 62.72468705147505\n",
      "loss 64.82753342092037\n",
      "loss 66.88505215764046\n",
      "loss 68.84974662542344\n",
      "loss 70.96779749691487\n",
      "loss 73.09405205965042\n",
      "loss 75.16651134192944\n",
      "loss 77.26286029577255\n",
      "loss 79.35776005983352\n",
      "loss 81.46923471152782\n",
      "loss 83.55207042038441\n",
      "loss 85.6817556387186\n",
      "loss 87.75239693760872\n",
      "loss 89.82292711734772\n",
      "loss 91.92016683757305\n",
      "Epoch:  60\n",
      "Training loss =  2.0891289767694654\n",
      "Validation Loss: 0.4990\tTop 1 Validation Accuracy: 0.8616\t Top 5 Validation Accuracy: 0.9858\n",
      "loss 2.108375570178032\n",
      "loss 4.211928994059563\n",
      "loss 6.278226490020752\n",
      "loss 8.294770959615708\n",
      "loss 10.395853168964386\n",
      "loss 12.486723039746284\n",
      "loss 14.523183763027191\n",
      "loss 16.65811098396778\n",
      "loss 18.764649401903153\n",
      "loss 20.755040667057038\n",
      "loss 22.88165894806385\n",
      "loss 25.017216096520425\n",
      "loss 27.084998400211333\n",
      "loss 29.12428854882717\n",
      "loss 31.268834122419356\n",
      "loss 33.242712241709235\n",
      "loss 35.330606006383896\n",
      "loss 37.46734238564968\n",
      "loss 39.62082017481327\n",
      "loss 41.67933679461479\n",
      "loss 43.79504891395569\n",
      "loss 45.82956456720829\n",
      "loss 47.900728528499606\n",
      "loss 49.98561188042164\n",
      "loss 52.01598444283009\n",
      "loss 54.12628274589777\n",
      "loss 56.17644710034132\n",
      "loss 58.27043628782034\n",
      "loss 60.36911278039217\n",
      "loss 62.454145507514475\n",
      "loss 64.48365090578794\n",
      "loss 66.52180272907019\n",
      "loss 68.60226296931505\n",
      "loss 70.75361768871545\n",
      "loss 72.88533841639757\n",
      "loss 74.92077227503061\n",
      "loss 76.97349555701017\n",
      "loss 79.06821486443282\n",
      "loss 81.22901219367981\n",
      "loss 83.39765345156192\n",
      "loss 85.5192924195528\n",
      "loss 87.58348515629768\n",
      "loss 89.67810065090656\n",
      "loss 91.77352990508079\n",
      "Epoch:  61\n",
      "Training loss =  2.087416852595486\n",
      "Validation Loss: 0.4996\tTop 1 Validation Accuracy: 0.8623\t Top 5 Validation Accuracy: 0.9858\n",
      "loss 2.0591568982601167\n",
      "loss 4.129454107284546\n",
      "loss 6.2501857733726505\n",
      "loss 8.316074015498161\n",
      "loss 10.450032356381417\n",
      "loss 12.44126848101616\n",
      "loss 14.509126085042954\n",
      "loss 16.567752931714057\n",
      "loss 18.63496538460255\n",
      "loss 20.751965859234332\n",
      "loss 22.888079865276815\n",
      "loss 24.990765710175037\n",
      "loss 27.111528589725495\n",
      "loss 29.212801147699356\n",
      "loss 31.268202743530274\n",
      "loss 33.3047790402174\n",
      "loss 35.38527205109596\n",
      "loss 37.402434108257296\n",
      "loss 39.42428243935108\n",
      "loss 41.56433389127255\n",
      "loss 43.67635095417499\n",
      "loss 45.769782415628434\n",
      "loss 47.818604463934896\n",
      "loss 49.89900463432073\n",
      "loss 51.95444410771132\n",
      "loss 54.11383273214102\n",
      "loss 56.20859053641558\n",
      "loss 58.358768910467624\n",
      "loss 60.44177467226982\n",
      "loss 62.60777302622795\n",
      "loss 64.72985712468625\n",
      "loss 66.70751057207585\n",
      "loss 68.79307021260261\n",
      "loss 70.85738526552916\n",
      "loss 72.93275821208954\n",
      "loss 75.01604909777642\n",
      "loss 77.07287988126278\n",
      "loss 79.12776743292808\n",
      "loss 81.20758357822895\n",
      "loss 83.25543283700944\n",
      "loss 85.31866407155991\n",
      "loss 87.40183103829622\n",
      "loss 89.4797193095088\n",
      "loss 91.61154929488897\n",
      "Epoch:  62\n",
      "Training loss =  2.082735488159833\n",
      "Validation Loss: 0.4959\tTop 1 Validation Accuracy: 0.8612\t Top 5 Validation Accuracy: 0.9856\n",
      "loss 2.0505911552906038\n",
      "loss 4.137363010346889\n",
      "loss 6.247198131978512\n",
      "loss 8.209645321667194\n",
      "loss 10.265615136623383\n",
      "loss 14.48130772382021\n",
      "loss 16.5903113886714\n",
      "loss 18.635499255657194\n",
      "loss 20.72998601406813\n",
      "loss 22.826246679127216\n",
      "loss 24.944683094918727\n",
      "loss 26.951958055496217\n",
      "loss 28.98692656874657\n",
      "loss 31.033913919627665\n",
      "loss 33.151238642036915\n",
      "loss 35.218231143057345\n",
      "loss 37.35871239513159\n",
      "loss 41.50053468644619\n",
      "loss 43.59791102230549\n",
      "loss 45.60635441124439\n",
      "loss 47.76495186448097\n",
      "loss 49.80543596029282\n",
      "loss 51.99679470539093\n",
      "loss 54.04658080935478\n",
      "loss 56.17625229597092\n",
      "loss 58.24709179699421\n",
      "loss 60.28621581226587\n",
      "loss 62.42303004533053\n",
      "loss 64.56604189842939\n",
      "loss 66.61592468947173\n",
      "loss 68.717394836545\n",
      "loss 70.73945427298545\n",
      "loss 72.82078604966402\n",
      "loss 74.8408611190319\n",
      "loss 76.97872882187366\n",
      "loss 79.1495662021637\n",
      "loss 81.22834594726562\n",
      "loss 83.31390879452229\n",
      "loss 85.41488571941852\n",
      "loss 87.52520682394504\n",
      "loss 89.63895742475987\n",
      "loss 91.79110663086176\n",
      "Epoch:  63\n",
      "Training loss =  2.085702216093101\n",
      "Validation Loss: 0.5241\tTop 1 Validation Accuracy: 0.8586\t Top 5 Validation Accuracy: 0.9848\n",
      "loss 2.036518303155899\n",
      "loss 4.102150444090366\n",
      "loss 6.184822380840778\n",
      "loss 8.308880309164524\n",
      "loss 10.295450004041195\n",
      "loss 12.38293673545122\n",
      "loss 14.342651068866253\n",
      "loss 16.374526459872722\n",
      "loss 18.512433632314206\n",
      "loss 20.557193681299687\n",
      "loss 22.632506923973562\n",
      "loss 24.749473955333233\n",
      "loss 26.87226668328047\n",
      "loss 29.014197637438773\n",
      "loss 31.081886526942252\n",
      "loss 33.1918167001009\n",
      "loss 35.242297433018685\n",
      "loss 37.33262599349022\n",
      "loss 39.38231684923172\n",
      "loss 41.40046809434891\n",
      "loss 43.51570025384426\n",
      "loss 45.56725324988365\n",
      "loss 47.59430648714304\n",
      "loss 49.70950785070658\n",
      "loss 51.77333812206984\n",
      "loss 53.89647969633341\n",
      "loss 56.07919072657823\n",
      "loss 58.19196549624205\n",
      "loss 60.308827632963656\n",
      "loss 62.400545856654645\n",
      "loss 64.48794646710158\n",
      "loss 66.63097749561071\n",
      "loss 68.73583776593209\n",
      "loss 70.823990573287\n",
      "loss 72.87032568871975\n",
      "loss 74.97691075742244\n",
      "loss 77.10093746364117\n",
      "loss 79.19303933411837\n",
      "loss 81.20135366827249\n",
      "loss 83.26552202910185\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 85.32443727940321\n",
      "loss 87.45013596564532\n",
      "loss 89.46627559095622\n",
      "loss 91.59544802635908\n",
      "Epoch:  64\n",
      "Training loss =  2.0824783185787195\n",
      "Validation Loss: 0.5107\tTop 1 Validation Accuracy: 0.8596\t Top 5 Validation Accuracy: 0.9849\n",
      "loss 2.0791758561134337\n",
      "loss 4.104924710988999\n",
      "loss 6.175849694013595\n",
      "loss 8.250524733662605\n",
      "loss 10.296245069503785\n",
      "loss 12.33644621372223\n",
      "loss 14.400772047638894\n",
      "loss 16.506592687368393\n",
      "loss 18.585217739343644\n",
      "loss 20.64926367998123\n",
      "loss 22.723182640075684\n",
      "loss 24.799771457612515\n",
      "loss 26.8979298633337\n",
      "loss 28.913319225907326\n",
      "loss 30.94907417714596\n",
      "loss 33.092138018012044\n",
      "loss 35.07216495901346\n",
      "loss 37.13771108835935\n",
      "loss 39.17718958646059\n",
      "loss 41.236301268339155\n",
      "loss 43.30632717430591\n",
      "loss 45.392240525484084\n",
      "loss 47.388951781392095\n",
      "loss 49.37896714150906\n",
      "loss 51.490625607967374\n",
      "loss 55.528390445411205\n",
      "loss 57.63586748629808\n",
      "loss 59.681113460958\n",
      "loss 61.80423618167639\n",
      "loss 63.83223245024681\n",
      "loss 65.88007139444352\n",
      "loss 68.02822243690491\n",
      "loss 70.10408503830433\n",
      "loss 72.11619615912437\n",
      "loss 74.07473861396312\n",
      "loss 76.19294643938541\n",
      "loss 78.23396810293198\n",
      "loss 80.3502534893155\n",
      "loss 82.46657180279493\n",
      "loss 84.53935839146375\n",
      "loss 86.61859425276518\n",
      "loss 88.70493902474642\n",
      "loss 90.80732235103845\n",
      "Epoch:  65\n",
      "Training loss =  2.063688173645503\n",
      "Validation Loss: 0.4999\tTop 1 Validation Accuracy: 0.8616\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 2.083482049703598\n",
      "loss 6.041756153702736\n",
      "loss 8.052514218091964\n",
      "loss 10.086810583770275\n",
      "loss 12.09817732602358\n",
      "loss 14.140024357140064\n",
      "loss 16.186519559323788\n",
      "loss 18.299736607968807\n",
      "loss 20.36201742976904\n",
      "loss 22.33950746625662\n",
      "loss 24.326542321145535\n",
      "loss 26.42250279098749\n",
      "loss 28.473143015801906\n",
      "loss 30.552388322949408\n",
      "loss 32.624699909687045\n",
      "loss 34.73669485211372\n",
      "loss 36.78829177886248\n",
      "loss 38.910621735751626\n",
      "loss 40.98003023117781\n",
      "loss 43.07274030894041\n",
      "loss 45.142195789814\n",
      "loss 47.18530438452959\n",
      "loss 49.1424077001214\n",
      "loss 51.1812609949708\n",
      "loss 53.294573326408866\n",
      "loss 55.34431510061026\n",
      "loss 57.30503877311945\n",
      "loss 59.35300024300814\n",
      "loss 61.45531343191862\n",
      "loss 63.51588189393282\n",
      "loss 65.61904866605997\n",
      "loss 67.65081349760294\n",
      "loss 69.6525718292594\n",
      "loss 71.73511454373597\n",
      "loss 73.78442478448153\n",
      "loss 75.81393769472838\n",
      "loss 77.86733414381743\n",
      "loss 79.93640461295843\n",
      "loss 81.95727351993322\n",
      "loss 83.94877992242574\n",
      "loss 86.04969358086586\n",
      "loss 88.14645266532898\n",
      "loss 90.2669964325428\n",
      "Epoch:  66\n",
      "Training loss =  2.0507168617842573\n",
      "Validation Loss: 0.5085\tTop 1 Validation Accuracy: 0.8613\t Top 5 Validation Accuracy: 0.9855\n",
      "loss 2.0897494152188303\n",
      "loss 4.173168804347515\n",
      "loss 6.192316743433476\n",
      "loss 8.306698295176028\n",
      "loss 10.318301006555558\n",
      "loss 12.292660787403584\n",
      "loss 14.350163386762143\n",
      "loss 16.38419961363077\n",
      "loss 18.32834732145071\n",
      "loss 20.35388125181198\n",
      "loss 22.399994499087335\n",
      "loss 24.485866973400118\n",
      "loss 26.5562036716938\n",
      "loss 28.57397909373045\n",
      "loss 30.578565661013126\n",
      "loss 32.702016691863534\n",
      "loss 34.81129427999258\n",
      "loss 36.872026847600935\n",
      "loss 39.00652962476015\n",
      "loss 41.068390668332576\n",
      "loss 43.132730522453784\n",
      "loss 45.21265045851469\n",
      "loss 47.34408591896295\n",
      "loss 49.388720557689666\n",
      "loss 51.44331853747368\n",
      "loss 53.534870461821555\n",
      "loss 55.56900525987148\n",
      "loss 57.6793342679739\n",
      "loss 59.76752068549395\n",
      "loss 61.85616910666227\n",
      "loss 63.834862678945065\n",
      "loss 65.93399847477674\n",
      "loss 67.93855853945017\n",
      "loss 70.05892327696085\n",
      "loss 72.09454353302718\n",
      "loss 74.17556570380926\n",
      "loss 76.18058026641607\n",
      "loss 78.25639979511499\n",
      "loss 80.31996270507574\n",
      "loss 82.43654483526944\n",
      "loss 84.52401910573244\n",
      "loss 86.69355670839548\n",
      "loss 88.81569319993258\n",
      "loss 90.89303379267454\n",
      "Epoch:  67\n",
      "Training loss =  2.0657689693149943\n",
      "Validation Loss: 0.5045\tTop 1 Validation Accuracy: 0.8613\t Top 5 Validation Accuracy: 0.9859\n",
      "loss 1.9932267528772354\n",
      "loss 4.073827580809593\n",
      "loss 6.088557673692703\n",
      "loss 8.078475326895713\n",
      "loss 10.150568940639495\n",
      "loss 12.151335560679435\n",
      "loss 14.265099441409111\n",
      "loss 16.227230922579764\n",
      "loss 18.259005547761916\n",
      "loss 20.316003108620645\n",
      "loss 22.412941603660585\n",
      "loss 24.48579695999622\n",
      "loss 26.569927324056625\n",
      "loss 28.598584312796593\n",
      "loss 30.703576222360134\n",
      "loss 32.772718367874624\n",
      "loss 34.85444394260645\n",
      "loss 36.89888419687748\n",
      "loss 38.90767035603523\n",
      "loss 40.99901691675186\n",
      "loss 43.03219399571419\n",
      "loss 45.106326842606066\n",
      "loss 47.116753525435925\n",
      "loss 49.17066513568163\n",
      "loss 51.279630601108074\n",
      "loss 53.34229458302259\n",
      "loss 55.33384873479605\n",
      "loss 57.324970040023324\n",
      "loss 59.44771898835897\n",
      "loss 61.54742434412241\n",
      "loss 63.65352381318807\n",
      "loss 65.68299890607595\n",
      "loss 67.7322905793786\n",
      "loss 69.79580132424832\n",
      "loss 71.80500831604004\n",
      "loss 73.84022482931614\n",
      "loss 75.88512166142463\n",
      "loss 77.89392592549324\n",
      "loss 79.91112571835518\n",
      "loss 81.94092682361602\n",
      "loss 84.02560700923205\n",
      "loss 86.11113885998726\n",
      "loss 88.17123688519001\n",
      "loss 90.2242575353384\n",
      "Epoch:  68\n",
      "Training loss =  2.0506221901333035\n",
      "Validation Loss: 0.5006\tTop 1 Validation Accuracy: 0.8623\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 2.076579216122627\n",
      "loss 4.062213994860649\n",
      "loss 6.081981136202812\n",
      "loss 8.135794268548489\n",
      "loss 10.144439279139043\n",
      "loss 12.19705853074789\n",
      "loss 14.239568659067153\n",
      "loss 16.2651786339283\n",
      "loss 18.265377237200738\n",
      "loss 20.284066156446933\n",
      "loss 22.252137318849563\n",
      "loss 24.353439910411836\n",
      "loss 26.390780177116394\n",
      "loss 28.409682098031045\n",
      "loss 30.449122917950152\n",
      "loss 32.48181380718947\n",
      "loss 34.54272945433855\n",
      "loss 36.58489113658666\n",
      "loss 38.55455484896898\n",
      "loss 40.555199152827264\n",
      "loss 42.55414576858282\n",
      "loss 44.6537465339899\n",
      "loss 46.714857467114925\n",
      "loss 48.749205041825775\n",
      "loss 50.83518498927355\n",
      "loss 52.874034426510335\n",
      "loss 54.897507377266884\n",
      "loss 56.95400016963482\n",
      "loss 59.01688735872507\n",
      "loss 61.06562522560358\n",
      "loss 63.10777043610811\n",
      "loss 65.17840062946081\n",
      "loss 67.27057230204343\n",
      "loss 69.34849880844355\n",
      "loss 71.34427134901286\n",
      "loss 73.40925246536732\n",
      "loss 75.45080490261316\n",
      "loss 77.4825803205371\n",
      "loss 79.43118396431208\n",
      "loss 81.46329965382814\n",
      "loss 83.5405654641986\n",
      "loss 85.5731515571475\n",
      "loss 87.68137726575137\n",
      "loss 89.74896305292845\n",
      "Epoch:  69\n",
      "Training loss =  2.0405201369158092\n",
      "Validation Loss: 0.5109\tTop 1 Validation Accuracy: 0.8619\t Top 5 Validation Accuracy: 0.9855\n",
      "loss 1.9647166001796723\n",
      "loss 3.917040587067604\n",
      "loss 5.950419835746288\n",
      "loss 7.953804472982884\n",
      "loss 10.003995588123798\n",
      "loss 11.997273212373257\n",
      "loss 13.967827942073345\n",
      "loss 15.991277594864368\n",
      "loss 17.99882487207651\n",
      "loss 20.05888643115759\n",
      "loss 22.08360130339861\n",
      "loss 24.15154479563236\n",
      "loss 26.192103501558304\n",
      "loss 28.140171890258788\n",
      "loss 30.165631613731385\n",
      "loss 32.16356895983219\n",
      "loss 34.23207964301109\n",
      "loss 36.258165503442285\n",
      "loss 38.336849136650564\n",
      "loss 40.37193902105093\n",
      "loss 42.390638503730294\n",
      "loss 44.390622955262664\n",
      "loss 46.44327618300915\n",
      "loss 48.527035152316095\n",
      "loss 50.57640542447567\n",
      "loss 52.55423175036907\n",
      "loss 54.52279012143612\n",
      "loss 56.52180314183235\n",
      "loss 58.55467340409756\n",
      "loss 60.590040078759195\n",
      "loss 62.70707209289074\n",
      "loss 64.65363916814327\n",
      "loss 66.64420171380043\n",
      "loss 68.7427317005396\n",
      "loss 70.85388359487057\n",
      "loss 72.94088038563729\n",
      "loss 74.90897019922733\n",
      "loss 76.92262519150972\n",
      "loss 78.92116874337196\n",
      "loss 80.92221770197153\n",
      "loss 82.96429101616144\n",
      "loss 85.0414155241847\n",
      "loss 87.0511964520812\n",
      "loss 89.1012420913577\n",
      "Epoch:  70\n",
      "Training loss =  2.024713555232364\n",
      "Validation Loss: 0.5245\tTop 1 Validation Accuracy: 0.8631\t Top 5 Validation Accuracy: 0.9859\n",
      "loss 2.0326159697771073\n",
      "loss 4.113680863976478\n",
      "loss 6.126687697172165\n",
      "loss 8.06156542301178\n",
      "loss 10.13001303076744\n",
      "loss 12.16684970676899\n",
      "loss 14.16276868402958\n",
      "loss 16.16778865724802\n",
      "loss 18.112868368923664\n",
      "loss 20.160197574198246\n",
      "loss 22.209003804624082\n",
      "loss 24.175108143389224\n",
      "loss 26.21907988101244\n",
      "loss 28.237486384809017\n",
      "loss 30.317574982345104\n",
      "loss 32.27127990812063\n",
      "loss 34.28678746432066\n",
      "loss 36.31611004263163\n",
      "loss 38.382459629476074\n",
      "loss 40.35716849774122\n",
      "loss 42.468539232611654\n",
      "loss 44.507274990677836\n",
      "loss 46.617933497726916\n",
      "loss 48.61147641152144\n",
      "loss 50.61130037426948\n",
      "loss 52.69172626107931\n",
      "loss 54.73704236209392\n",
      "loss 56.77649004846811\n",
      "loss 58.83693892508745\n",
      "loss 60.93478465825319\n",
      "loss 63.013230664730074\n",
      "loss 64.98642944097519\n",
      "loss 66.98457567751407\n",
      "loss 69.06324722409248\n",
      "loss 71.07140595257282\n",
      "loss 73.13543489694595\n",
      "loss 75.17204059302807\n",
      "loss 77.28605462312699\n",
      "loss 79.29009720206261\n",
      "loss 81.414292126894\n",
      "loss 83.45181644141674\n",
      "loss 85.52777375042439\n",
      "loss 87.60126805901527\n",
      "loss 89.61390004187822\n",
      "Epoch:  71\n",
      "Training loss =  2.0372098381730552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.5002\tTop 1 Validation Accuracy: 0.8607\t Top 5 Validation Accuracy: 0.9858\n",
      "loss 2.095767865777016\n",
      "loss 4.126472179889679\n",
      "loss 6.125804146528244\n",
      "loss 8.13993242830038\n",
      "loss 10.130729975402355\n",
      "loss 12.227287692129613\n",
      "loss 14.303942469656468\n",
      "loss 16.37314706802368\n",
      "loss 18.4465569627285\n",
      "loss 20.430512119233608\n",
      "loss 22.466506894528866\n",
      "loss 24.42263119727373\n",
      "loss 26.37400117337704\n",
      "loss 28.42511241674423\n",
      "loss 30.427957895696164\n",
      "loss 32.39198701620102\n",
      "loss 34.38386349022389\n",
      "loss 36.4543747612834\n",
      "loss 38.53115450590849\n",
      "loss 40.48748629331589\n",
      "loss 42.5132854527235\n",
      "loss 44.61406316041946\n",
      "loss 46.69320258080959\n",
      "loss 48.7419962733984\n",
      "loss 50.75443403065205\n",
      "loss 52.82569184958935\n",
      "loss 54.895095732808116\n",
      "loss 56.94219834774733\n",
      "loss 58.997459692656996\n",
      "loss 61.043147601783275\n",
      "loss 63.1021313175559\n",
      "loss 65.13344305485487\n",
      "loss 67.11504951387644\n",
      "loss 69.09296298772097\n",
      "loss 71.14136843591929\n",
      "loss 73.1933372721076\n",
      "loss 75.11670614391565\n",
      "loss 77.14846357107163\n",
      "loss 79.19682240128517\n",
      "loss 81.2231931707263\n",
      "loss 83.3114174053073\n",
      "loss 85.40369173556566\n",
      "loss 87.41497536838055\n",
      "loss 89.54529202461242\n",
      "Epoch:  72\n",
      "Training loss =  2.034963646041213\n",
      "Validation Loss: 0.5041\tTop 1 Validation Accuracy: 0.8619\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 2.1310827744007113\n",
      "loss 4.116543811559677\n",
      "loss 6.096001926660538\n",
      "loss 10.08765502423048\n",
      "loss 12.059152652323245\n",
      "loss 14.02489948272705\n",
      "loss 16.00495287656784\n",
      "loss 18.037207612395285\n",
      "loss 19.94708874642849\n",
      "loss 22.00123847961426\n",
      "loss 23.95385485112667\n",
      "loss 25.861644864082336\n",
      "loss 27.821143892407417\n",
      "loss 29.869836322665215\n",
      "loss 31.93033691227436\n",
      "loss 33.971086561083794\n",
      "loss 35.980296186208726\n",
      "loss 37.96538989961147\n",
      "loss 40.04507956624031\n",
      "loss 42.06959168195724\n",
      "loss 44.02983845531941\n",
      "loss 46.08234380245209\n",
      "loss 48.16070931732654\n",
      "loss 50.12298176169396\n",
      "loss 52.17137349516153\n",
      "loss 54.118121925294396\n",
      "loss 56.07435984104872\n",
      "loss 58.11613062530756\n",
      "loss 60.15216794759035\n",
      "loss 62.08227903574705\n",
      "loss 64.14305344104767\n",
      "loss 66.19399515390396\n",
      "loss 68.1920681989193\n",
      "loss 70.27541543781757\n",
      "loss 72.26671991556883\n",
      "loss 74.32468823820352\n",
      "loss 76.32733768314124\n",
      "loss 78.37279733330011\n",
      "loss 80.41297683537006\n",
      "loss 82.43540515214205\n",
      "loss 84.49399616867304\n",
      "loss 86.48463025182485\n",
      "loss 88.54858938544989\n",
      "Epoch:  73\n",
      "Training loss =  2.0125787413693934\n",
      "Validation Loss: 0.5112\tTop 1 Validation Accuracy: 0.8605\t Top 5 Validation Accuracy: 0.9853\n",
      "loss 1.9215056371688843\n",
      "loss 3.957417075932026\n",
      "loss 5.994151889383793\n",
      "loss 7.957280486822128\n",
      "loss 9.953724565804004\n",
      "loss 11.967085546553134\n",
      "loss 13.915053431689739\n",
      "loss 15.896411128342152\n",
      "loss 17.989101548194885\n",
      "loss 19.995183464586734\n",
      "loss 21.946535496711732\n",
      "loss 24.00307334959507\n",
      "loss 26.093619865775107\n",
      "loss 28.20289095878601\n",
      "loss 30.145920872092248\n",
      "loss 32.09863003045321\n",
      "loss 34.12123071700334\n",
      "loss 36.20087813824415\n",
      "loss 38.224230624735355\n",
      "loss 40.16033697217703\n",
      "loss 42.12153384923935\n",
      "loss 44.16984285712242\n",
      "loss 46.20259052097797\n",
      "loss 48.173484315872194\n",
      "loss 50.193463257551194\n",
      "loss 52.29947967410087\n",
      "loss 54.244001013040545\n",
      "loss 56.2610596549511\n",
      "loss 58.24294677495956\n",
      "loss 60.21452836036682\n",
      "loss 62.270506302714345\n",
      "loss 64.26670455038547\n",
      "loss 66.21426296114922\n",
      "loss 68.16495597541332\n",
      "loss 70.17135808497667\n",
      "loss 72.17690733879805\n",
      "loss 74.10308415561914\n",
      "loss 76.11604450762272\n",
      "loss 78.10641648054123\n",
      "loss 80.14455595135689\n",
      "loss 82.21411144256592\n",
      "loss 84.22829289197922\n",
      "loss 86.21801419436932\n",
      "loss 88.31291914701461\n",
      "Epoch:  74\n",
      "Training loss =  2.008079008236343\n",
      "Validation Loss: 0.4963\tTop 1 Validation Accuracy: 0.8617\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 1.9665464925765992\n",
      "loss 3.969607086181641\n",
      "loss 5.966462416052818\n",
      "loss 7.8992167615890505\n",
      "loss 9.931111332774162\n",
      "loss 11.940099914968014\n",
      "loss 13.946049846112729\n",
      "loss 15.942193929255009\n",
      "loss 17.99299240618944\n",
      "loss 19.979771794378756\n",
      "loss 22.003608897030354\n",
      "loss 23.91485384374857\n",
      "loss 25.966097630560398\n",
      "loss 27.993150863945484\n",
      "loss 29.913162410259247\n",
      "loss 31.949692738354205\n",
      "loss 33.97295790284872\n",
      "loss 35.97599686354399\n",
      "loss 37.98306785017252\n",
      "loss 40.001786514520646\n",
      "loss 42.045612442493436\n",
      "loss 44.029488510489465\n",
      "loss 46.15628012955189\n",
      "loss 48.15628636866808\n",
      "loss 50.204522754251954\n",
      "loss 52.25855468481779\n",
      "loss 54.20940652012825\n",
      "loss 56.181379377245904\n",
      "loss 58.22144434988499\n",
      "loss 60.21669555604458\n",
      "loss 62.247451735138895\n",
      "loss 64.29717184603214\n",
      "loss 66.22132865279913\n",
      "loss 68.20531120806932\n",
      "loss 70.15769190520048\n",
      "loss 72.1454792034626\n",
      "loss 74.2142574250698\n",
      "loss 76.28484117925167\n",
      "loss 78.2649462878704\n",
      "loss 80.20851130455732\n",
      "loss 82.24092249542474\n",
      "loss 84.27509893387555\n",
      "loss 86.21939431279898\n",
      "loss 88.15991898655892\n",
      "Epoch:  75\n",
      "Training loss =  2.0036100059620363\n",
      "Validation Loss: 0.5083\tTop 1 Validation Accuracy: 0.8605\t Top 5 Validation Accuracy: 0.9851\n",
      "loss 1.9670261132717133\n",
      "loss 3.923469992876053\n",
      "loss 5.973327523469925\n",
      "loss 7.9302249339222906\n",
      "loss 9.920140114724637\n",
      "loss 11.905645929872989\n",
      "loss 13.864674371182918\n",
      "loss 15.848929318487643\n",
      "loss 17.88330752879381\n",
      "loss 19.811334923803805\n",
      "loss 21.803255911767483\n",
      "loss 23.712681929171087\n",
      "loss 25.623872357308866\n",
      "loss 27.593192124068736\n",
      "loss 29.445020860731603\n",
      "loss 31.405212167799473\n",
      "loss 33.40912017136812\n",
      "loss 35.32551583141088\n",
      "loss 37.28133576899767\n",
      "loss 39.31332831770182\n",
      "loss 41.252633899748325\n",
      "loss 43.20412105232477\n",
      "loss 45.16368232548237\n",
      "loss 47.06915490150452\n",
      "loss 48.99559213966131\n",
      "loss 51.081029555499555\n",
      "loss 53.003274910748004\n",
      "loss 54.907505097091196\n",
      "loss 56.790391027629376\n",
      "loss 58.778150021731854\n",
      "loss 60.754398798644544\n",
      "loss 62.743031761944295\n",
      "loss 64.68064345747233\n",
      "loss 66.62087823241949\n",
      "loss 68.59814924627543\n",
      "loss 70.57747619897127\n",
      "loss 72.53824393421411\n",
      "loss 74.56498911887407\n",
      "loss 76.48838798165322\n",
      "loss 78.40347212016583\n",
      "loss 80.33767668902874\n",
      "loss 82.320825420022\n",
      "loss 84.32091304570436\n",
      "loss 86.35265493243932\n",
      "Epoch:  76\n",
      "Training loss =  1.9621853529066617\n",
      "Validation Loss: 0.5607\tTop 1 Validation Accuracy: 0.8618\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 1.9869186389446258\n",
      "loss 3.8674139630794526\n",
      "loss 5.834575984477997\n",
      "loss 7.852111998200416\n",
      "loss 9.80485970467329\n",
      "loss 11.763930059969425\n",
      "loss 13.703849883973598\n",
      "loss 15.68523779630661\n",
      "loss 17.596359712481497\n",
      "loss 19.515748512148857\n",
      "loss 21.442277883291244\n",
      "loss 23.40902157664299\n",
      "loss 25.393484056293964\n",
      "loss 27.39656883865595\n",
      "loss 29.355582490861416\n",
      "loss 31.381552817821504\n",
      "loss 33.410787149071695\n",
      "loss 35.365455107092856\n",
      "loss 37.357746968269346\n",
      "loss 39.26571429371834\n",
      "loss 41.233938450217245\n",
      "loss 43.1050769725442\n",
      "loss 44.9471059897542\n",
      "loss 46.87475240111351\n",
      "loss 48.90588422298431\n",
      "loss 50.80363654255867\n",
      "loss 52.71430563390255\n",
      "loss 54.687579673528674\n",
      "loss 56.593412201106545\n",
      "loss 58.47802003145218\n",
      "loss 60.49944841146469\n",
      "loss 62.38315706074238\n",
      "loss 64.31354355067015\n",
      "loss 66.2592429330945\n",
      "loss 68.28833410352469\n",
      "loss 70.21305913209915\n",
      "loss 72.07426498472691\n",
      "loss 73.99607240468264\n",
      "loss 76.00890876621008\n",
      "loss 77.98438803166151\n",
      "loss 79.9834723815322\n",
      "loss 81.97706532269716\n",
      "loss 83.91983716279269\n",
      "loss 85.88964364618063\n",
      "Epoch:  77\n",
      "Training loss =  1.9529958455340632\n",
      "Validation Loss: 0.5116\tTop 1 Validation Accuracy: 0.8624\t Top 5 Validation Accuracy: 0.9859\n",
      "loss 1.9837147906422614\n",
      "loss 3.9586085456609728\n",
      "loss 5.919762986302376\n",
      "loss 7.777312033772469\n",
      "loss 9.789802449941636\n",
      "loss 11.764999942779541\n",
      "loss 13.65066767513752\n",
      "loss 15.615747761130333\n",
      "loss 17.43910666048527\n",
      "loss 19.39071224272251\n",
      "loss 21.318029187619686\n",
      "loss 23.20692534804344\n",
      "loss 25.127030635476114\n",
      "loss 27.03654676347971\n",
      "loss 28.999827781021594\n",
      "loss 30.967335655987263\n",
      "loss 32.92746302306652\n",
      "loss 34.83719924867153\n",
      "loss 36.77134671151638\n",
      "loss 38.66943400025368\n",
      "loss 40.58278998523951\n",
      "loss 42.55881657809019\n",
      "loss 44.52855462521315\n",
      "loss 46.53151046484709\n",
      "loss 48.50436031609774\n",
      "loss 50.46756959617138\n",
      "loss 52.34277963280678\n",
      "loss 54.30343117773533\n",
      "loss 56.235023396909234\n",
      "loss 58.14551705151796\n",
      "loss 60.04033758640289\n",
      "loss 61.97105034887791\n",
      "loss 63.964519501924514\n",
      "loss 65.87890752077102\n",
      "loss 67.85044562220574\n",
      "loss 69.80990117251874\n",
      "loss 71.75969417154789\n",
      "loss 73.69235633045434\n",
      "loss 75.65873258471488\n",
      "loss 77.6036630141735\n",
      "loss 79.49638416290283\n",
      "loss 81.38845622897148\n",
      "loss 83.39089010506868\n",
      "loss 85.37952888906003\n",
      "Epoch:  78\n",
      "Training loss =  1.941543680471009\n",
      "Validation Loss: 0.5002\tTop 1 Validation Accuracy: 0.8618\t Top 5 Validation Accuracy: 0.9860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.9518341615796089\n",
      "loss 3.9058261105418204\n",
      "loss 5.903440857827664\n",
      "loss 9.704771953821183\n",
      "loss 11.67225672185421\n",
      "loss 13.582811566591262\n",
      "loss 15.506399427950383\n",
      "loss 17.44726731926203\n",
      "loss 19.431580584943294\n",
      "loss 21.39850492268801\n",
      "loss 23.291193956136702\n",
      "loss 25.140936901271342\n",
      "loss 27.00599185556173\n",
      "loss 28.965528941452504\n",
      "loss 30.97271196395159\n",
      "loss 32.99688594967127\n",
      "loss 34.949406735002995\n",
      "loss 36.90733546048403\n",
      "loss 38.835529101192954\n",
      "loss 40.72902007848025\n",
      "loss 42.772336254417894\n",
      "loss 44.6693596842885\n",
      "loss 46.663158159554\n",
      "loss 48.62170021235943\n",
      "loss 50.57834047436714\n",
      "loss 52.49942805975675\n",
      "loss 54.436909476220606\n",
      "loss 56.46648710757494\n",
      "loss 58.35402847975492\n",
      "loss 60.330556033551694\n",
      "loss 62.264842525720596\n",
      "loss 64.30543071866036\n",
      "loss 66.26828738689423\n",
      "loss 68.22432744204998\n",
      "loss 70.11314434170723\n",
      "loss 72.11522194743156\n",
      "loss 74.09759034574031\n",
      "loss 78.0615808814764\n",
      "loss 80.01355361819267\n",
      "loss 81.92741203188896\n",
      "loss 83.87302545249462\n",
      "loss 85.89038869559765\n",
      "Epoch:  79\n",
      "Training loss =  1.9532879954352658\n",
      "Validation Loss: 0.5015\tTop 1 Validation Accuracy: 0.8620\t Top 5 Validation Accuracy: 0.9861\n",
      "loss 1.9370565688610077\n",
      "loss 3.791454327404499\n",
      "loss 5.7441813135147095\n",
      "loss 7.6864876660704615\n",
      "loss 11.522257322371006\n",
      "loss 13.48763813585043\n",
      "loss 15.411590798795224\n",
      "loss 17.38266765534878\n",
      "loss 19.350886026024817\n",
      "loss 21.27373568087816\n",
      "loss 23.141431979238988\n",
      "loss 25.067391060888767\n",
      "loss 27.017733667194843\n",
      "loss 28.948215300142763\n",
      "loss 30.949316875636576\n",
      "loss 32.922341300547124\n",
      "loss 34.91185555696487\n",
      "loss 36.82519280344248\n",
      "loss 38.79064761430025\n",
      "loss 40.78131536215544\n",
      "loss 42.68980059117079\n",
      "loss 44.600837879776954\n",
      "loss 46.55942927598953\n",
      "loss 48.52347750544548\n",
      "loss 50.49090716600418\n",
      "loss 52.40377879559994\n",
      "loss 54.29616245687008\n",
      "loss 56.155648590922354\n",
      "loss 58.09101900070905\n",
      "loss 60.09410633176565\n",
      "loss 61.98007474929094\n",
      "loss 63.9267291906476\n",
      "loss 65.86685098499059\n",
      "loss 67.80717548340559\n",
      "loss 69.77573357015848\n",
      "loss 71.64268174856902\n",
      "loss 73.58318220734596\n",
      "loss 75.53223841249942\n",
      "loss 77.51840933918953\n",
      "loss 79.47912379980087\n",
      "loss 81.42151116549968\n",
      "loss 83.4180703252554\n",
      "loss 85.34307790786028\n",
      "Epoch:  80\n",
      "Training loss =  1.9399224709319833\n",
      "Validation Loss: 0.5129\tTop 1 Validation Accuracy: 0.8636\t Top 5 Validation Accuracy: 0.9864\n",
      "loss 1.9399164426326752\n",
      "loss 3.9623484885692597\n",
      "loss 5.942968646287918\n",
      "loss 7.935192689597606\n",
      "loss 9.91747213691473\n",
      "loss 11.921734562814235\n",
      "loss 13.853969295918942\n",
      "loss 15.78415535479784\n",
      "loss 17.671396367549896\n",
      "loss 19.620124351382255\n",
      "loss 21.527288426160812\n",
      "loss 23.502878769636155\n",
      "loss 25.46072772204876\n",
      "loss 27.387973391413688\n",
      "loss 29.304492375254632\n",
      "loss 31.227224631011484\n",
      "loss 33.256637400090696\n",
      "loss 35.18824610561133\n",
      "loss 37.08984020680189\n",
      "loss 39.042763356864455\n",
      "loss 40.95803983777761\n",
      "loss 42.9264780035615\n",
      "loss 44.90986059695482\n",
      "loss 46.85722890794277\n",
      "loss 48.78338227897883\n",
      "loss 50.726850929558275\n",
      "loss 52.567201616764066\n",
      "loss 54.6269854670763\n",
      "loss 56.57766937822103\n",
      "loss 58.50783394664526\n",
      "loss 60.46960526376963\n",
      "loss 62.39256669521332\n",
      "loss 64.30593705445528\n",
      "loss 66.31128373235464\n",
      "loss 68.31991775587201\n",
      "loss 70.24897005274892\n",
      "loss 72.26008530333638\n",
      "loss 74.1797062022984\n",
      "loss 76.17240968093276\n",
      "loss 78.09986332461239\n",
      "loss 80.05712572827935\n",
      "loss 81.95244728758931\n",
      "loss 83.97741263464093\n",
      "loss 85.88603584691882\n",
      "Epoch:  81\n",
      "Training loss =  1.9525337238438563\n",
      "Validation Loss: 0.5107\tTop 1 Validation Accuracy: 0.8622\t Top 5 Validation Accuracy: 0.9859\n",
      "loss 1.8847300571203232\n",
      "loss 3.8158855551481246\n",
      "loss 5.722176508307457\n",
      "loss 7.657356708049774\n",
      "loss 9.66391911804676\n",
      "loss 11.617560320496558\n",
      "loss 13.465311809778214\n",
      "loss 15.42295749783516\n",
      "loss 17.35501814752817\n",
      "loss 19.280737297236918\n",
      "loss 21.16273389548063\n",
      "loss 23.143052668273448\n",
      "loss 25.00665500253439\n",
      "loss 26.898158116638662\n",
      "loss 28.798078111708165\n",
      "loss 30.696680972278116\n",
      "loss 32.654303695261476\n",
      "loss 34.575070398449895\n",
      "loss 36.43831673830748\n",
      "loss 38.40060088008642\n",
      "loss 40.29277823716402\n",
      "loss 42.19400825709104\n",
      "loss 44.130918916165825\n",
      "loss 46.11340649694204\n",
      "loss 48.0077450671792\n",
      "loss 49.95165664047003\n",
      "loss 51.9341463252902\n",
      "loss 53.892997247576716\n",
      "loss 55.85485712826252\n",
      "loss 57.819062727093694\n",
      "loss 59.69660305202007\n",
      "loss 61.66762826025486\n",
      "loss 63.52910388916731\n",
      "loss 65.5576872447133\n",
      "loss 67.45566918045283\n",
      "loss 69.42494117319583\n",
      "loss 71.36000340789556\n",
      "loss 73.22108785510063\n",
      "loss 75.14635683298111\n",
      "loss 77.13397947579622\n",
      "loss 79.00019582003355\n",
      "loss 80.90497312158347\n",
      "loss 82.84307094573974\n",
      "loss 84.64039281845093\n",
      "Epoch:  82\n",
      "Training loss =  1.924271764033077\n",
      "Validation Loss: 0.5002\tTop 1 Validation Accuracy: 0.8628\t Top 5 Validation Accuracy: 0.9860\n",
      "loss 1.9963964962959289\n",
      "loss 3.928604558110237\n",
      "loss 7.69462553858757\n",
      "loss 9.56331987708807\n",
      "loss 11.464874999523163\n",
      "loss 13.42847959637642\n",
      "loss 15.340167804956437\n",
      "loss 17.29657784551382\n",
      "loss 19.24324424892664\n",
      "loss 21.22762175053358\n",
      "loss 23.188337649405003\n",
      "loss 25.11956320732832\n",
      "loss 27.098050982654094\n",
      "loss 29.035372712910174\n",
      "loss 32.95578945845366\n",
      "loss 34.850080293715\n",
      "loss 36.8504309129715\n",
      "loss 38.74435497879982\n",
      "loss 40.65490448772907\n",
      "loss 42.59635126411915\n",
      "loss 44.61355457901955\n",
      "loss 46.468950177431104\n",
      "loss 48.42148988962173\n",
      "loss 50.40586533486843\n",
      "loss 52.341682788133625\n",
      "loss 54.25943066060543\n",
      "loss 56.221860139369966\n",
      "loss 58.24167554855347\n",
      "loss 60.07327329427004\n",
      "loss 62.01992384403944\n",
      "loss 63.97295561850071\n",
      "loss 65.89771893978119\n",
      "loss 67.87493787586689\n",
      "loss 69.82324992597103\n",
      "loss 71.80699246108531\n",
      "loss 73.73288927137851\n",
      "loss 75.67786944538355\n",
      "loss 77.58881243407727\n",
      "loss 79.53738784372807\n",
      "loss 81.4898789948225\n",
      "loss 83.42664749324322\n",
      "loss 85.29851190567017\n",
      "Epoch:  83\n",
      "Training loss =  1.9389370615251256\n",
      "Validation Loss: 0.4983\tTop 1 Validation Accuracy: 0.8625\t Top 5 Validation Accuracy: 0.9860\n",
      "loss 1.9747492688894273\n",
      "loss 3.933890216052532\n",
      "loss 5.860258032679558\n",
      "loss 7.745474334657192\n",
      "loss 9.663752572834492\n",
      "loss 11.577404380738734\n",
      "loss 13.532054224610329\n",
      "loss 15.409977529942989\n",
      "loss 17.32628397613764\n",
      "loss 19.329530412256716\n",
      "loss 21.29849266111851\n",
      "loss 23.23911630809307\n",
      "loss 25.20051836311817\n",
      "loss 27.18550440609455\n",
      "loss 29.198692517876626\n",
      "loss 31.095374001264574\n",
      "loss 33.064734570384026\n",
      "loss 34.94346101939678\n",
      "loss 36.86032103359699\n",
      "loss 38.77240048617124\n",
      "loss 40.569509534537794\n",
      "loss 42.44942778915167\n",
      "loss 44.35747258752585\n",
      "loss 46.35575209349394\n",
      "loss 48.23902680933475\n",
      "loss 50.12028870522976\n",
      "loss 52.03714286386967\n",
      "loss 53.90689577817917\n",
      "loss 55.76441293805838\n",
      "loss 57.7403301551938\n",
      "loss 59.70139985591173\n",
      "loss 61.643633401691915\n",
      "loss 63.50784255772829\n",
      "loss 65.34721974998712\n",
      "loss 67.345875813663\n",
      "loss 69.3896693405509\n",
      "loss 71.3523264709115\n",
      "loss 73.35322176843881\n",
      "loss 75.27104468017816\n",
      "loss 77.26651303708553\n",
      "loss 79.23404618442059\n",
      "loss 81.18671542882919\n",
      "loss 83.12413761913777\n",
      "loss 85.07459237992764\n",
      "Epoch:  84\n",
      "Training loss =  1.9336496651413897\n",
      "Validation Loss: 0.5035\tTop 1 Validation Accuracy: 0.8615\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 1.9729361134767531\n",
      "loss 3.8616662138700484\n",
      "loss 5.807098981738091\n",
      "loss 7.657599401473999\n",
      "loss 9.60550012767315\n",
      "loss 11.603067319989204\n",
      "loss 13.515622539520264\n",
      "loss 15.479015530347825\n",
      "loss 17.43970013678074\n",
      "loss 19.37826026558876\n",
      "loss 21.273215147256852\n",
      "loss 23.1955115288496\n",
      "loss 25.166577752232552\n",
      "loss 27.084256444871425\n",
      "loss 29.064160678088665\n",
      "loss 31.00847043812275\n",
      "loss 32.913972318172455\n",
      "loss 34.881474891901014\n",
      "loss 36.77685178339481\n",
      "loss 38.75023287564516\n",
      "loss 40.63322346478701\n",
      "loss 42.63537130028009\n",
      "loss 44.62306621104479\n",
      "loss 46.601185478270054\n",
      "loss 48.54587762027979\n",
      "loss 50.523502027094366\n",
      "loss 52.46674588739872\n",
      "loss 54.35687133133411\n",
      "loss 56.331664171218875\n",
      "loss 58.20448495984078\n",
      "loss 60.09860110580921\n",
      "loss 62.06986491501331\n",
      "loss 64.05007904529572\n",
      "loss 65.97041505992412\n",
      "loss 67.90011627912521\n",
      "loss 69.80164964258671\n",
      "loss 71.70530094087124\n",
      "loss 73.67748565018177\n",
      "loss 75.63767605245113\n",
      "loss 77.62654367148876\n",
      "loss 79.54807341217995\n",
      "loss 81.4844399318099\n",
      "loss 83.44439450353384\n",
      "loss 85.36982493489981\n",
      "Epoch:  85\n",
      "Training loss =  1.9406454006165152\n",
      "Validation Loss: 0.4925\tTop 1 Validation Accuracy: 0.8638\t Top 5 Validation Accuracy: 0.9864\n",
      "loss 1.8862955942749977\n",
      "loss 3.860027876198292\n",
      "loss 5.755382080972194\n",
      "loss 7.654396160542965\n",
      "loss 9.562809582948685\n",
      "loss 11.596685991883279\n",
      "loss 13.516517642438412\n",
      "loss 15.474302259981632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 17.412165788710116\n",
      "loss 19.416290913522243\n",
      "loss 21.334533436894418\n",
      "loss 23.264053706228733\n",
      "loss 25.193097709417344\n",
      "loss 27.073549694418908\n",
      "loss 29.00557696759701\n",
      "loss 30.871059638261794\n",
      "loss 32.917599982619286\n",
      "loss 34.87044303715229\n",
      "loss 36.77032060772181\n",
      "loss 38.61984503924847\n",
      "loss 40.58141119480133\n",
      "loss 42.53091444194317\n",
      "loss 44.52244258463383\n",
      "loss 46.505649779438976\n",
      "loss 48.490619044899944\n",
      "loss 50.46014345228672\n",
      "loss 52.43087983667851\n",
      "loss 54.36246073663235\n",
      "loss 56.28350226283074\n",
      "loss 58.17338200390339\n",
      "loss 60.16745224773884\n",
      "loss 62.137966104745864\n",
      "loss 64.01314995169639\n",
      "loss 66.02797286987305\n",
      "loss 67.91126156896352\n",
      "loss 69.84169372290373\n",
      "loss 71.72156620830297\n",
      "loss 73.64633279711008\n",
      "loss 75.56532552093267\n",
      "loss 77.47807502627373\n",
      "loss 79.35863120526075\n",
      "loss 81.23835003107786\n",
      "loss 83.16568188279867\n",
      "loss 85.05010217666626\n",
      "Epoch:  86\n",
      "Training loss =  1.9333147757075357\n",
      "Validation Loss: 0.5044\tTop 1 Validation Accuracy: 0.8624\t Top 5 Validation Accuracy: 0.9857\n",
      "loss 1.912740495800972\n",
      "loss 3.876611860394478\n",
      "loss 5.823956053853035\n",
      "loss 7.699594016373157\n",
      "loss 9.665375090241433\n",
      "loss 11.636993801891803\n",
      "loss 13.569878406226636\n",
      "loss 15.505034405291081\n",
      "loss 17.475601432323455\n",
      "loss 19.407447613477707\n",
      "loss 21.356875556707383\n",
      "loss 23.318196794092657\n",
      "loss 25.303116226196288\n",
      "loss 27.22195048779249\n",
      "loss 29.221986413896083\n",
      "loss 31.102941290438174\n",
      "loss 33.04719264745712\n",
      "loss 34.99709105491638\n",
      "loss 36.955645452737805\n",
      "loss 38.849540475606915\n",
      "loss 40.706849947571754\n",
      "loss 42.610477725863454\n",
      "loss 44.54151181101799\n",
      "loss 46.45288511544466\n",
      "loss 48.47468000322581\n",
      "loss 50.43478612929582\n",
      "loss 52.358280125856396\n",
      "loss 54.20262218773365\n",
      "loss 56.11757422089577\n",
      "loss 58.01454236775637\n",
      "loss 59.92229825049639\n",
      "loss 61.84158043980599\n",
      "loss 63.78977078944445\n",
      "loss 65.73612354785205\n",
      "loss 67.68537136167288\n",
      "loss 69.6472350731492\n",
      "loss 71.5956703826785\n",
      "loss 73.49300157696008\n",
      "loss 75.45133635938167\n",
      "loss 77.34273602157832\n",
      "loss 79.27747429400682\n",
      "loss 81.20398429125548\n",
      "loss 83.12430836677551\n",
      "loss 85.02766584634782\n",
      "Epoch:  87\n",
      "Training loss =  1.9328802702318075\n",
      "Validation Loss: 0.5012\tTop 1 Validation Accuracy: 0.8618\t Top 5 Validation Accuracy: 0.9860\n",
      "loss 1.9102531406283378\n",
      "loss 3.807577123939991\n",
      "loss 5.720249467194081\n",
      "loss 7.687554210722446\n",
      "loss 11.575304465591907\n",
      "loss 13.554432847797871\n",
      "loss 15.48168658286333\n",
      "loss 17.418602492809296\n",
      "loss 19.351558916270733\n",
      "loss 21.205599887371065\n",
      "loss 23.042470315396784\n",
      "loss 24.988570970594882\n",
      "loss 27.014210920631886\n",
      "loss 28.92826315432787\n",
      "loss 30.88394167482853\n",
      "loss 32.8868513917923\n",
      "loss 34.77847282677889\n",
      "loss 36.672816166877745\n",
      "loss 38.59988011419773\n",
      "loss 40.46535371184349\n",
      "loss 42.41580454260111\n",
      "loss 44.346137315928935\n",
      "loss 46.32201793551445\n",
      "loss 48.26687199890613\n",
      "loss 50.27641906201839\n",
      "loss 52.191978898048404\n",
      "loss 54.072322072982786\n",
      "loss 55.99405174434185\n",
      "loss 57.93308586657047\n",
      "loss 59.89075017511845\n",
      "loss 61.87718884408474\n",
      "loss 63.81305332690477\n",
      "loss 65.71955170720815\n",
      "loss 67.66317501634359\n",
      "loss 69.43100465625524\n",
      "loss 71.39266749978066\n",
      "loss 73.28964770078659\n",
      "loss 75.20928341865539\n",
      "loss 77.06417817294597\n",
      "loss 78.927887160182\n",
      "loss 80.74994929879904\n",
      "loss 82.61751711755991\n",
      "loss 84.55061129689217\n",
      "Epoch:  88\n",
      "Training loss =  1.922354607601393\n",
      "Validation Loss: 0.5066\tTop 1 Validation Accuracy: 0.8635\t Top 5 Validation Accuracy: 0.9862\n",
      "loss 1.9223184037208556\n",
      "loss 3.867938593626022\n",
      "loss 5.673533712029457\n",
      "loss 7.5966579955816265\n",
      "loss 9.50615576952696\n",
      "loss 11.480366025269031\n",
      "loss 13.33954724907875\n",
      "loss 15.20368384540081\n",
      "loss 16.999624207019806\n",
      "loss 18.958915448486806\n",
      "loss 20.982360967695712\n",
      "loss 22.903649082481863\n",
      "loss 24.842938682436944\n",
      "loss 26.736833285093308\n",
      "loss 28.626200322806834\n",
      "loss 30.549845194518568\n",
      "loss 32.42584263831377\n",
      "loss 34.36946865051985\n",
      "loss 36.342563896775246\n",
      "loss 38.29374305039644\n",
      "loss 40.19902771353722\n",
      "loss 42.17910682141781\n",
      "loss 44.12489794194698\n",
      "loss 46.03721759676933\n",
      "loss 47.95316310584545\n",
      "loss 49.91332570970059\n",
      "loss 51.757938714027404\n",
      "loss 53.70435873985291\n",
      "loss 55.62942206680775\n",
      "loss 57.55193247616291\n",
      "loss 59.53562527954578\n",
      "loss 61.32915587186813\n",
      "loss 63.19024303197861\n",
      "loss 65.17929389238357\n",
      "loss 67.01501656472684\n",
      "loss 68.94442363977433\n",
      "loss 70.84585898578167\n",
      "loss 72.78354441821575\n",
      "loss 74.71950693964958\n",
      "loss 76.63440933972598\n",
      "loss 78.48771691232919\n",
      "loss 80.44024045169354\n",
      "loss 82.39392400860787\n",
      "loss 84.35406580269337\n",
      "Epoch:  89\n",
      "Training loss =  1.9169695583493194\n",
      "Validation Loss: 0.4921\tTop 1 Validation Accuracy: 0.8641\t Top 5 Validation Accuracy: 0.9865\n",
      "loss 1.91055524289608\n",
      "loss 3.8023099264502527\n",
      "loss 5.7012169942259785\n",
      "loss 7.616394791305066\n",
      "loss 9.536824222803116\n",
      "loss 11.487585624158383\n",
      "loss 13.381248265206814\n",
      "loss 15.33369752496481\n",
      "loss 17.2512964117527\n",
      "loss 19.210992721021174\n",
      "loss 21.075894860327242\n",
      "loss 22.998543902635575\n",
      "loss 24.97343161702156\n",
      "loss 26.861010955274104\n",
      "loss 28.744869568943976\n",
      "loss 30.688402233719827\n",
      "loss 32.637436902821065\n",
      "loss 34.54304739922285\n",
      "loss 36.43816745996475\n",
      "loss 38.43720852136612\n",
      "loss 40.320889005362986\n",
      "loss 42.27795636504889\n",
      "loss 44.18279543966055\n",
      "loss 46.06494261771441\n",
      "loss 48.05098351031542\n",
      "loss 49.987804165780545\n",
      "loss 51.970745955705645\n",
      "loss 53.87256069779396\n",
      "loss 55.790551546812054\n",
      "loss 57.76446710407734\n",
      "loss 59.63401304125786\n",
      "loss 61.543043761849404\n",
      "loss 63.55932655751705\n",
      "loss 65.40082986354828\n",
      "loss 67.34855916798115\n",
      "loss 69.32250292539597\n",
      "loss 71.25624557793141\n",
      "loss 73.19128717780113\n",
      "loss 75.13622063696384\n",
      "loss 77.08432314455509\n",
      "loss 79.03010017186404\n",
      "loss 80.98286837905646\n",
      "loss 82.97968238204717\n",
      "loss 84.90113244920968\n",
      "Epoch:  90\n",
      "Training loss =  1.929896980081753\n",
      "Validation Loss: 0.4974\tTop 1 Validation Accuracy: 0.8643\t Top 5 Validation Accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "train(model, train_dataloader, validation_dataloader, beta, cutmix_prob, criterion, optimizer, lr_scheduler, modelpath, writer, device=device, epochs = num_Epochs)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load saved model from checkpoint  #####\n",
    "model, optimizer, lr_scheduler, train_loss, v_loss, v_acc, epoch = load(modelpath, model, optimizer, lr_scheduler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### Plot Loss Curves ####\n",
    "plot_loss(epoch, train_loss, v_loss, title = 'NewData_Apro_AA_Cutmix_ResNeXt101_32x8d')\n",
    "plot_acc(epoch, v_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loss,top1_t_acc, top5_t_acc =test_classify(model, train_dataloader, criterion, device)\n",
    "print('Training Loss: {:.4f}\\tTop 1 Training Accuracy: {:.4f}\\t Top 5 Training Accuracy: {:.4f}'.format(t_loss, top1_t_acc, top5_t_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.4974\tTop 1 Validation Accuracy: 0.8643\n",
      "Accuracy:defaultdict(<class 'int'>, {'Top 1 Accuracy': 86.42702768829263, 'Top 5 Accuracy': 98.64413000040778, 'Top 10 Accuracy': 99.23745055662032, 'Top 20 Accuracy': 99.53037284725904, 'Top 30 Accuracy': 99.63979393494543, 'Top 50 Accuracy': 99.74173904769673, 'Top 100 Accuracy': 99.83009147874785})\t\n"
     ]
    }
   ],
   "source": [
    "v_loss, top1_acc, accuracy_dict= eval_classify(model, validation_dataloader, criterion, device)\n",
    "print('Validation Loss: {:.4f}\\tTop 1 Validation Accuracy: {:.4f}\\nAccuracy:{}\\t'.format(v_loss, top1_acc, accuracy_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tune using Label Smoothing Cross entropy loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import train, test_classify, eval_classify\n",
    "from label_smoothing import LabelSmoothingCrossEntropy\n",
    "\n",
    "## Loss Function\n",
    "criterion = LabelSmoothingCrossEntropy()\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer_ls = torch.optim.SGD(model.parameters(), lr=0.00005, weight_decay=1e-4, momentum=0.9)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 25, gamma = 0.1)\n",
    "\n",
    "# Epochs\n",
    "num_Epochs = 5\n",
    "\n",
    "\n",
    "#### Load saved model from checkpoint  #####\n",
    "model, optimizer, lr_scheduler, train_loss, v_loss, v_acc, epoch = load(modelpath, model, optimizer, lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.799258632659912\n",
      "loss 3.60006702542305\n",
      "loss 5.399953892230988\n",
      "loss 7.204748657941818\n",
      "loss 9.007300528287887\n",
      "loss 10.806632572412491\n",
      "loss 12.603097579479218\n",
      "loss 14.39781732559204\n",
      "loss 16.19654513478279\n",
      "loss 17.99356863498688\n",
      "loss 19.789346923828123\n",
      "loss 21.59025391459465\n",
      "loss 23.396574788093567\n",
      "loss 25.195338928699492\n",
      "loss 26.998297612667084\n",
      "loss 28.80042635798454\n",
      "loss 30.599844102859496\n",
      "loss 32.39181084752083\n",
      "loss 34.2046596467495\n",
      "loss 36.005369755029676\n",
      "loss 37.79795155882835\n",
      "loss 39.594953135252\n",
      "loss 41.39225044846535\n",
      "loss 43.19655650019646\n",
      "loss 44.99705384492874\n",
      "loss 46.794407538175584\n",
      "loss 48.59371188044548\n",
      "loss 50.39833599328995\n",
      "loss 52.20157232761383\n",
      "loss 54.00468543410301\n",
      "loss 55.81589411020279\n",
      "loss 57.62008223176002\n",
      "loss 59.42370941519737\n",
      "loss 61.22548110842705\n",
      "loss 63.026123200654986\n",
      "loss 64.82493898034096\n",
      "loss 66.6223119199276\n",
      "loss 68.42108634710311\n",
      "loss 70.22112751603126\n",
      "loss 72.0297906255722\n",
      "loss 73.82918095707893\n",
      "loss 75.62575507640838\n",
      "loss 77.43271194458008\n",
      "Epoch:  1\n",
      "training loss =  1.801245826695886\n",
      "Validation Loss: 2.0055\tTop 1 Validation Accuracy: 0.8656\t Top 5 Validation Accuracy: 0.9861\n",
      "loss 1.7954199779033662\n",
      "loss 3.6031749701499938\n",
      "loss 5.398544360399246\n",
      "loss 7.198637503385544\n",
      "loss 8.991766204833985\n",
      "loss 10.78835555434227\n",
      "loss 12.59611046552658\n",
      "loss 14.390687347650529\n",
      "loss 16.188155348300935\n",
      "loss 17.987997859716415\n",
      "loss 19.78848484158516\n",
      "loss 21.584546654224397\n",
      "loss 23.385337167978285\n",
      "loss 25.185375435352327\n",
      "loss 26.983265603780747\n",
      "loss 28.789669234752655\n",
      "loss 30.589902493953705\n",
      "loss 32.39281796693802\n",
      "loss 34.19730221867562\n",
      "loss 35.9974736726284\n",
      "loss 37.79449425935745\n",
      "loss 39.59059638500214\n",
      "loss 41.38752746224404\n",
      "loss 43.18423464417457\n",
      "loss 44.981985912323\n",
      "loss 46.78491427183151\n",
      "loss 50.38423103332519\n",
      "loss 52.186610885858535\n",
      "loss 53.98716833114624\n",
      "loss 55.795496326684955\n",
      "loss 57.596545400619505\n",
      "loss 59.39915489673614\n",
      "loss 61.20270802617073\n",
      "loss 63.00411075592041\n",
      "loss 64.80510812401772\n",
      "loss 66.60029111504555\n",
      "loss 68.40549178481102\n",
      "loss 70.2108258318901\n",
      "loss 72.00983852386474\n",
      "loss 73.80868723750115\n",
      "loss 75.60950239181518\n",
      "loss 77.41134765148163\n",
      "loss 79.21024690270424\n",
      "Epoch:  2\n",
      "training loss =  1.8004486866953777\n",
      "Validation Loss: 2.0080\tTop 1 Validation Accuracy: 0.8654\t Top 5 Validation Accuracy: 0.9860\n",
      "loss 1.7932920920848847\n",
      "loss 3.5904789459705353\n",
      "loss 5.387429492473602\n",
      "loss 7.185570160150528\n",
      "loss 8.985184378623963\n",
      "loss 10.783913023471833\n",
      "loss 12.585493234395981\n",
      "loss 14.396376192569733\n",
      "loss 16.195309578180314\n",
      "loss 17.997933424711228\n",
      "loss 19.803120657205582\n",
      "loss 21.606336532831193\n",
      "loss 23.402630096673967\n",
      "loss 25.200379663705824\n",
      "loss 26.99051851987839\n",
      "loss 28.78164484500885\n",
      "loss 30.57859605193138\n",
      "loss 32.37756456136704\n",
      "loss 34.16748844504357\n",
      "loss 35.97442684650421\n",
      "loss 37.77740218043327\n",
      "loss 39.572650197744366\n",
      "loss 41.37313486933708\n",
      "loss 43.16408080458641\n",
      "loss 44.963072860240935\n",
      "loss 48.56304774403572\n",
      "loss 50.363669154644015\n",
      "loss 52.1685209107399\n",
      "loss 53.965188920497894\n",
      "loss 55.76582592844963\n",
      "loss 57.559042165279386\n",
      "loss 59.358271532058716\n",
      "loss 61.15278349280357\n",
      "loss 62.95666095018387\n",
      "loss 64.75467428207398\n",
      "loss 66.55207234978676\n",
      "loss 68.35774327516556\n",
      "loss 70.15440239310264\n",
      "loss 71.95901595711707\n",
      "loss 73.7606671833992\n",
      "loss 75.56214768052101\n",
      "loss 77.36645560383796\n",
      "loss 79.17043572902679\n",
      "Epoch:  3\n",
      "training loss =  1.7994749426761316\n",
      "Validation Loss: 2.0086\tTop 1 Validation Accuracy: 0.8651\t Top 5 Validation Accuracy: 0.9858\n",
      "loss 1.8027195501327515\n",
      "loss 3.594579073190689\n",
      "loss 5.389045677185059\n",
      "loss 7.184722926616669\n",
      "loss 8.980200406312942\n",
      "loss 10.780930358171464\n",
      "loss 12.576717985868454\n",
      "loss 14.377289532423019\n",
      "loss 16.174214572906493\n",
      "loss 17.971169891357423\n",
      "loss 19.764911136627198\n",
      "loss 21.561078337430953\n",
      "loss 23.355521726608277\n",
      "loss 25.157167414426805\n",
      "loss 26.955900492668153\n",
      "loss 28.752913411855697\n",
      "loss 30.551035828590393\n",
      "loss 32.34885260462761\n",
      "loss 34.14839170217514\n",
      "loss 35.95938224196434\n",
      "loss 37.75274309039116\n",
      "loss 39.5437148296833\n",
      "loss 41.337275758981704\n",
      "loss 43.137151799201966\n",
      "loss 44.93164494752884\n",
      "loss 46.72670475840569\n",
      "loss 48.526191190481185\n",
      "loss 50.32486099243164\n",
      "loss 52.118346382379535\n",
      "loss 53.92089441537857\n",
      "loss 55.71582863807678\n",
      "loss 57.51226990699768\n",
      "loss 59.31624377131462\n",
      "loss 61.11597642660141\n",
      "loss 62.9114431142807\n",
      "loss 64.70389941215515\n",
      "loss 66.50479125857353\n",
      "loss 68.30648244738579\n",
      "loss 70.1088684630394\n",
      "loss 71.9142679655552\n",
      "loss 75.51527443647385\n",
      "loss 77.316910841465\n",
      "loss 79.11751543402671\n",
      "Epoch:  4\n",
      "training loss =  1.7984120957578518\n",
      "Validation Loss: 2.0146\tTop 1 Validation Accuracy: 0.8643\t Top 5 Validation Accuracy: 0.9860\n",
      "loss 1.8071446406841278\n",
      "loss 3.607451078891754\n",
      "loss 5.4062072277069095\n",
      "loss 7.198349238634109\n",
      "loss 8.990433660745621\n",
      "loss 10.79342500925064\n",
      "loss 12.594569330215455\n",
      "loss 14.391809095144271\n",
      "loss 16.18555338025093\n",
      "loss 17.9758101940155\n",
      "loss 19.771616994142533\n",
      "loss 21.563398331403732\n",
      "loss 23.36846363902092\n",
      "loss 25.172029538154604\n",
      "loss 26.965646340847016\n",
      "loss 28.756681736707687\n",
      "loss 30.55164481520653\n",
      "loss 32.34948764204979\n",
      "loss 35.93334540367127\n",
      "loss 37.72584262132645\n",
      "loss 39.522227127552036\n",
      "loss 41.31901995182037\n",
      "loss 43.1207632124424\n",
      "loss 44.91211483359337\n",
      "loss 46.70857769489288\n",
      "loss 48.502258294820784\n",
      "loss 50.29787267923355\n",
      "loss 52.09227717757225\n",
      "loss 53.90336701631546\n",
      "loss 55.70460254430771\n",
      "loss 57.49830893158913\n",
      "loss 59.29338090419769\n",
      "loss 61.09764245390892\n",
      "loss 62.888858730793\n",
      "loss 64.69251074790955\n",
      "loss 66.48604446291924\n",
      "loss 68.27989117383957\n",
      "loss 70.07708085894585\n",
      "loss 71.87887891411782\n",
      "loss 73.68339859247207\n",
      "loss 75.48360925197602\n",
      "loss 77.29053336381912\n",
      "loss 79.0853313946724\n",
      "Epoch:  5\n",
      "training loss =  1.797478212167205\n",
      "Validation Loss: 2.0048\tTop 1 Validation Accuracy: 0.8648\t Top 5 Validation Accuracy: 0.9858\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "train(model, train_dataloader, validation_dataloader, criterion, optimizer_ls, lr_scheduler, modelpath, writer, device, epochs= num_Epochs)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Nov  7 19:33:48 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 440.33.01    Driver Version: 440.33.01    CUDA Version: 10.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  TITAN RTX           Off  | 00000000:04:00.0 Off |                  N/A |\n",
      "| 42%   65C    P2   239W / 280W |  23873MiB / 24220MiB |     85%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  TITAN RTX           Off  | 00000000:05:00.0 Off |                  N/A |\n",
      "| 79%   85C    P2   128W / 280W |  23946MiB / 24220MiB |     95%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  TITAN RTX           Off  | 00000000:08:00.0 Off |                  N/A |\n",
      "| 93%   86C    P2   119W / 280W |  23800MiB / 24220MiB |     88%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  TITAN RTX           Off  | 00000000:09:00.0 Off |                  N/A |\n",
      "| 88%   85C    P2   106W / 280W |  23800MiB / 24220MiB |     85%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   4  TITAN RTX           Off  | 00000000:84:00.0 Off |                  N/A |\n",
      "| 66%   82C    P2   113W / 280W |  23800MiB / 24220MiB |     82%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   5  TITAN RTX           Off  | 00000000:85:00.0 Off |                  N/A |\n",
      "| 45%   61C    P2    76W / 280W |   1164MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   6  TITAN RTX           Off  | 00000000:88:00.0 Off |                  N/A |\n",
      "| 41%   25C    P8     4W / 280W |  18941MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   7  TITAN RTX           Off  | 00000000:89:00.0 Off |                  N/A |\n",
      "| 41%   31C    P8    12W / 280W |  14170MiB / 24220MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|    0     21832      C   .../anaconda3/envs/abhishek_env/bin/python   719MiB |\n",
      "|    0     29595      C   .../anaconda3/envs/shayeree_env/bin/python 23129MiB |\n",
      "|    1     41338      C   python                                     23923MiB |\n",
      "|    2     41338      C   python                                     23777MiB |\n",
      "|    3     41338      C   python                                     23777MiB |\n",
      "|    4     41338      C   python                                     23777MiB |\n",
      "|    5     33660      C   ...t/anaconda3/envs/mmlab_myenv/bin/python  1153MiB |\n",
      "|    6     21832      C   .../anaconda3/envs/abhishek_env/bin/python 16407MiB |\n",
      "|    7     21832      C   .../anaconda3/envs/abhishek_env/bin/python 14149MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
